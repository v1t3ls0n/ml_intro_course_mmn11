{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture run_output\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from logger.config import logger\n",
    "from perceptron.data_preprocessing import load_mnist, preprocess_data\n",
    "# IMPORTANT: This must be the updated version that records iteration-level losses in `loss_history`.\n",
    "from perceptron.multi_class_perceptron_unified import MultiClassPerceptron  \n",
    "from analysis.evaluation_functions import (\n",
    "    evaluate_model,\n",
    "    analyze_confusion_matrix,\n",
    "    plot_confusion_matrix,\n",
    "    plot_confusion_matrix_annotated,\n",
    "    plot_class_metrics,\n",
    "    # We'll import plot_history so we can visualize the real iteration-based train vs. test curves\n",
    "    plot_history\n",
    ")\n",
    "\n",
    "def aggregate_iteration_losses(mcp, mode_name=\"pocket\"):\n",
    "    \"\"\"\n",
    "    Aggregates iteration-level train/test losses across all digits \n",
    "    into a single 'train_curve' and 'test_curve' by averaging.\n",
    "\n",
    "    We assume 'mcp.loss_history[i][\"train\"]' is a list of training errors \n",
    "    for digit i at each iteration, and 'mcp.loss_history[i][\"test\"]' similarly.\n",
    "    Different digits may converge early => we pad with last value.\n",
    "\n",
    "    Returns: (train_curve, test_curve) as lists of length = max iteration across digits.\n",
    "    If no test data was provided, test_curve may be empty or zero.\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes = mcp.num_classes\n",
    "    # Find the maximum length across all classes for \"train\" or \"test\"\n",
    "    max_len = 0\n",
    "    for i in range(num_classes):\n",
    "        length_i = len(mcp.loss_history[i][\"train\"])\n",
    "        if length_i > max_len:\n",
    "            max_len = length_i\n",
    "\n",
    "    # We'll store \"train\" and \"test\" in a 2D list for each iteration\n",
    "    all_train = []\n",
    "    all_test  = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        # train array for class i\n",
    "        t_arr = mcp.loss_history[i][\"train\"]\n",
    "        # test array for class i\n",
    "        te_arr = mcp.loss_history[i][\"test\"]\n",
    "\n",
    "        # If the digit converged early, pad with the final value\n",
    "        if len(t_arr) < max_len:\n",
    "            t_arr = t_arr + [t_arr[-1]]*(max_len - len(t_arr))\n",
    "        if len(te_arr) < max_len and len(te_arr) > 0:\n",
    "            te_arr = te_arr + [te_arr[-1]]*(max_len - len(te_arr))\n",
    "\n",
    "        all_train.append(t_arr)\n",
    "        # If class i has no test data (or not used), fill with 0 or last known\n",
    "        if len(te_arr) == 0:\n",
    "            # Means we never recorded test error\n",
    "            te_arr = [0]*max_len\n",
    "        all_test.append(te_arr)\n",
    "\n",
    "    # Convert to np.array for easy mean\n",
    "    all_train = np.array(all_train)  # shape (num_classes, max_len)\n",
    "    all_test  = np.array(all_test)   # shape (num_classes, max_len)\n",
    "\n",
    "    # Average across classes\n",
    "    train_curve = np.mean(all_train, axis=0).tolist()\n",
    "    test_curve  = np.mean(all_test, axis=0).tolist()\n",
    "\n",
    "    return train_curve, test_curve\n",
    "\n",
    "def run_perceptron(mode_name, use_pocket, X_train, y_train, X_test, y_test, parent_dir):\n",
    "    \"\"\"\n",
    "    Runs the MultiClassPerceptron in either pocket or clean mode, \n",
    "    saves logs and figures to a subfolder named <mode_name>_<timestamp> under 'parent_dir'.\n",
    "    Also plots a real iteration-based train vs. test curve.\n",
    "\n",
    "    Args:\n",
    "      mode_name (str): \"pocket\" or \"clean\" (label used for folder name & logs).\n",
    "      use_pocket (bool): True => pocket algorithm, False => clean.\n",
    "      X_train, y_train, X_test, y_test: MNIST data/labels, preprocessed.\n",
    "      parent_dir (str): top-level directory for outputs (e.g. \"../outputs/ComparePocketClean\").\n",
    "\n",
    "    Returns:\n",
    "      run_dir (str): path to the subfolder containing logs & figures.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Create subfolder with timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(parent_dir, f\"{mode_name}_{timestamp}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    # 2. Filenames\n",
    "    log_filename = os.path.join(run_dir, \"log.txt\")\n",
    "    cm_path         = os.path.join(run_dir, \"conf_mat.png\")\n",
    "    cm_annot_path   = os.path.join(run_dir, \"conf_mat_annot.png\")\n",
    "    class_metrics_path   = os.path.join(run_dir, \"class_metrics.png\")\n",
    "    train_vs_test_path   = os.path.join(run_dir, \"train_vs_test_curve.png\")\n",
    "\n",
    "    # 3. Configure logger for this run\n",
    "    logger.handlers = []\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    fh = logging.FileHandler(log_filename, mode=\"w\")\n",
    "    fh.setLevel(logging.INFO)\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    # 4. Start\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"=== Starting {mode_name.upper()} run ===\")\n",
    "    logger.info(f\"Run directory: {run_dir}\")\n",
    "    logger.info(f\"Log file: {log_filename}\")\n",
    "\n",
    "    # 5. Train perceptron\n",
    "    # IMPORTANT: pass X_test,y_test so we can track iteration-based test error\n",
    "    logger.info(f\"Training MultiClassPerceptron with pocket={use_pocket}\")\n",
    "    mcp = MultiClassPerceptron(use_pocket=use_pocket)\n",
    "    # If we want iteration-based test error, pass them in fit:\n",
    "    mcp.fit(X_train, y_train, X_val=None, y_val=None, X_test=X_test, y_test=y_test)\n",
    "\n",
    "    # 6. Evaluate\n",
    "    logger.info(f\"Evaluating on test set ({mode_name} mode)\")\n",
    "    cm, acc = evaluate_model(mcp, X_test, y_test)\n",
    "    analyze_confusion_matrix(cm)\n",
    "\n",
    "    # If you have iteration/final error stats in your updated perceptron, log them:\n",
    "    if hasattr(mcp, \"converged_iterations\"):\n",
    "        logger.info(\"=== Additional training stats per digit ===\")\n",
    "        for cls in range(mcp.num_classes):\n",
    "            iters = mcp.converged_iterations.get(cls, \"N/A\")\n",
    "            ftrain_err = mcp.final_train_error.get(cls, \"N/A\")\n",
    "            fval_err   = mcp.final_val_error.get(cls, \"N/A\")\n",
    "            ftest_err  = mcp.final_test_error.get(cls, \"N/A\")\n",
    "            logger.info(\n",
    "                f\"Digit {cls}: iters={iters}, \"\n",
    "                f\"final_train_err={ftrain_err}, val_err={fval_err}, test_err={ftest_err}\"\n",
    "            )\n",
    "\n",
    "    # 7. Plot confusion matrix\n",
    "    plot_confusion_matrix(\n",
    "        cm,\n",
    "        title=f\"Confusion Matrix ({mode_name})\",\n",
    "        save_path=cm_path\n",
    "    )\n",
    "    logger.info(f\"Basic confusion matrix saved to {cm_path}\")\n",
    "\n",
    "    # 7b. Annotated confusion matrix\n",
    "    plot_confusion_matrix_annotated(\n",
    "        cm,\n",
    "        title=f\"Confusion Matrix Annotated ({mode_name})\",\n",
    "        save_path=cm_annot_path\n",
    "    )\n",
    "    logger.info(f\"Annotated confusion matrix saved to {cm_annot_path}\")\n",
    "\n",
    "    # 8. Class metrics (TPR)\n",
    "    tpr_values = []\n",
    "    for cls in range(cm.shape[0]):\n",
    "        TP = cm[cls, cls]\n",
    "        FN = np.sum(cm[cls, :]) - TP\n",
    "        tpr = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        tpr_values.append(tpr)\n",
    "\n",
    "    plot_class_metrics(\n",
    "        values=tpr_values,\n",
    "        metric_name=f\"TPR ({mode_name})\",\n",
    "        classes=range(cm.shape[0]),\n",
    "        save_path=class_metrics_path\n",
    "    )\n",
    "    logger.info(f\"Class metrics plot (TPR) saved to {class_metrics_path}\")\n",
    "    logger.info(f\"Overall Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "    # 9. REAL iteration-based train vs. test curves across classes\n",
    "    train_curve, test_curve = aggregate_iteration_losses(mcp, mode_name=mode_name)\n",
    "\n",
    "    # We'll use the existing plot_history from evaluation_functions (assuming it has signature: plot_history(train_values, val_values=None, ...):\n",
    "    from analysis.evaluation_functions import plot_history\n",
    "    plot_history(\n",
    "        train_values=train_curve,\n",
    "        val_values=test_curve,\n",
    "        title=f\"Real Train vs Test Curves ({mode_name})\",\n",
    "        ylabel=\"#Misclassifications (Avg across classes)\",\n",
    "        save_path=train_vs_test_path\n",
    "    )\n",
    "    logger.info(f\"Iteration-based train vs. test curve saved to {train_vs_test_path}\")\n",
    "\n",
    "    # 10. End\n",
    "    duration = time.time() - start_time\n",
    "    logger.info(f\"{mode_name.upper()} run complete. Total runtime: {duration:.2f} sec.\")\n",
    "\n",
    "    return run_dir\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Main single cell logic\n",
    "# -------------------------------------------------------------------\n",
    "top_parent_dir = \"../outputs/ComparePocketClean\"\n",
    "os.makedirs(top_parent_dir, exist_ok=True)\n",
    "\n",
    "start_global = time.time()\n",
    "logger.info(\"=== Setting up environment for Pocket vs Clean comparison with REAL iteration-based curves ===\")\n",
    "\n",
    "# Load & preprocess MNIST\n",
    "logger.info(\"Loading MNIST once for both runs\")\n",
    "X, y = load_mnist()\n",
    "X = preprocess_data(X)\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "# Run in Pocket mode\n",
    "pocket_dir = run_perceptron(\n",
    "    mode_name=\"pocket\",\n",
    "    use_pocket=True,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test,\n",
    "    parent_dir=top_parent_dir\n",
    ")\n",
    "\n",
    "# Run in Clean mode\n",
    "clean_dir = run_perceptron(\n",
    "    mode_name=\"clean\",\n",
    "    use_pocket=False,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test,\n",
    "    parent_dir=top_parent_dir\n",
    ")\n",
    "\n",
    "end_global = time.time()\n",
    "logger.info(f\"Both runs complete. Total time: {end_global - start_global:.2f} seconds.\")\n",
    "\n",
    "# Append captured cell output to a single file in the top_parent_dir\n",
    "compare_path = os.path.join(top_parent_dir, \"Compare_output.txt\")\n",
    "with open(compare_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"--- Combined cell output from Pocket vs Clean runs (with iteration-based curves) ---\\n\\n\")\n",
    "    f.write(run_output.stdout)\n",
    "\n",
    "print(f\"Comparison done. Pocket logs/plots in: {pocket_dir}\")\n",
    "print(f\"Clean logs/plots in: {clean_dir}\")\n",
    "print(f\"All cell output appended to: {compare_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
