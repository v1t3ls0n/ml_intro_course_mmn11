{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/v1t3ls0n/ml_intro_course_mmn11/blob/main/notebooks/%D7%9E%D7%91%D7%95%D7%90%20%D7%9C%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%97%D7%99%D7%A9%D7%95%D7%91%D7%99%D7%AA%20-%20%D7%9E%D7%9E%D7%9F%2011%20-%20%D7%92%D7%99%D7%90%20%D7%95%D7%99%D7%98%D7%9C%D7%96%D7%95%D7%9F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ztlf-kRcUjIz"
   },
   "source": [
    "# Maman 11 By Guy Vitelson\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXiZOQmO5Tak"
   },
   "source": [
    "##**If you run this within Google Collab, Dont Worry!**\n",
    "all the missing python files/directories/modules will be automatically feteched from my github repository\n",
    "\n",
    "**My GitHub Profile** : https://github.com/v1t3ls0n\n",
    "\n",
    "**The Repository:** https://github.com/v1t3ls0n/ml_intro_course_mmn11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKs0r5ROHxuY"
   },
   "source": [
    "# Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWMyo4jnR-mx"
   },
   "source": [
    "## MNIST Digit Classification Using Perceptron Learning Algorithm (PLA)\n",
    "\n",
    "**Objective:**  \n",
    "This notebook compares the performance of two variants of the Perceptron Learning Algorithm (PLA) on the MNIST digit classification task:\n",
    "- **Clean PLA:** Standard perceptron without enhancements.\n",
    "- **Pocket PLA:** Enhanced perceptron that stores the best-performing weights during training (using the Pocket algorithm).\n",
    "\n",
    "**Dataset:**  \n",
    "- MNIST dataset consisting of 60,000 training samples and 10,000 test samples.\n",
    "- The images are normalized to the range [0, 1] and a bias term is added, resulting in input samples with 785 features.\n",
    "\n",
    "**Evaluation Metrics:**  \n",
    "- **Confusion Matrices:** Provides a detailed view of how well each digit is classified.\n",
    "- **Overall Accuracy (ACC):** Defined as \\(\\text{ACC} = \\frac{TP + TN}{TP + TN + FP + FN}\\).\n",
    "- **Sensitivity (True Positive Rate, TPR):** For each digit, calculated as \\(\\text{TPR} = \\frac{TP}{TP + FN}\\), showing the model’s ability to correctly identify the digit.\n",
    "- **Selectivity (Specificity, TNR):** For each digit, calculated as \\(\\text{TNR} = \\frac{TN}{TN + FP}\\), showing the model’s ability to correctly identify negatives.\n",
    "- **Training and Testing Error Curves:** Visualized as a function of iteration for detailed analysis of learning dynamics.\n",
    "- **Runtime:** The time taken to train the models.\n",
    "\n",
    "**Goals:**  \n",
    "- Evaluate and compare the model accuracy and robustness between Clean PLA and Pocket PLA.\n",
    "- Analyze and visualize the performance through confusion matrices, error curves, and summary plots (accuracy, sensitivity, selectivity, and runtime vs. the number of iterations).\n",
    "- Provide a comprehensive discussion on how training iterations affect the decision boundaries and the overall performance, particularly in the one-vs-all classification setup.\n",
    "\n",
    "This notebook integrates detailed quantitative evaluation with comprehensive visualizations to thoroughly analyze the multi-class Perceptron performance on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqLIUat1dEr2"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmauUwgLR-mx"
   },
   "source": [
    "## External Code Imports (pip packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xUkaAHQFR-mx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import seaborn as sns # type: ignore\n",
    "import time\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlMUAQraR-my"
   },
   "source": [
    "## Fetch Missing Files For Google Colab Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "msKnbktXR-my"
   },
   "outputs": [],
   "source": [
    "\n",
    "# %%capture run_output\n",
    "# %matplotlib inline\n",
    "\n",
    "if sys.platform != 'win32': # check if we are running on google collab\n",
    "  repo_url = \"https://github.com/v1t3ls0n/ml_intro_course_mmn11\"\n",
    "  repo_name = \"ml_intro_course_mmn11\"\n",
    "  from tqdm.notebook import tqdm # type: ignore\n",
    "\n",
    "\n",
    "  # Clone the repository if it doesn't exist\n",
    "  if not os.path.exists(repo_name):\n",
    "    os.system(f\"git clone {repo_url}\")\n",
    "\n",
    "  # Construct the path to the repository directory\n",
    "  repo_path = os.path.join(os.getcwd(), repo_name)\n",
    "\n",
    "  # Add the repository directory to the Python path\n",
    "  if repo_path not in sys.path:\n",
    "    sys.path.insert(0, repo_path)\n",
    "\n",
    "  # --- Extract 'core' and 'notebooks' directories ---\n",
    "  def extract_directories(source_dir, destination_dir, dir_names):\n",
    "      for dir_name in dir_names:\n",
    "          source_path = os.path.join(source_dir, dir_name)\n",
    "          destination_path = os.path.join(destination_dir, dir_name)\n",
    "          if os.path.exists(source_path):\n",
    "              shutil.copytree(source_path, destination_path, dirs_exist_ok=True)\n",
    "\n",
    "  destination_path = \".\"\n",
    "  # Extract the directories\n",
    "  extract_directories(repo_path, destination_path, [\"core\"])\n",
    "  project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "  sys.path.insert(0, project_root)\n",
    "  if os.path.exists(\"ml_intro_course_mmn11\"):\n",
    "    shutil.rmtree(\"ml_intro_course_mmn11\")\n",
    "  if os.path.exists(\"sample_data\"):\n",
    "    shutil.rmtree(\"sample_data\")\n",
    "else:\n",
    "  from tqdm import tqdm  # type: ignore\n",
    "  current_dir = os.getcwd()  # Current working directory\n",
    "  project_root = os.path.abspath(os.path.join(current_dir, '..'))  # Root directory of the project\n",
    "  sys.path.insert(0, project_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYrRE0dcR-my"
   },
   "source": [
    "## Internal Code Imports (original code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VTaL_MsqeE00"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ========== Internal Code Imports ==========\n",
    "\n",
    "#Logger\n",
    "from core.logger.config import logger\n",
    "\n",
    "# Data Preprocessing\n",
    "from core.data.mnist_loader import load_mnist\n",
    "from core.data.data_preprocessing import preprocess_data\n",
    "\n",
    "# Models\n",
    "from core.models.perceptron.multi_class_perceptron import MultiClassPerceptron\n",
    "from core.models.logistic_regression.softmax_lregression import SoftmaxRegression\n",
    "from core.models.linear_regression.linear_regression import  LinearRegression\n",
    "\n",
    "# Performance & Plotting\n",
    "from core.analysis.evaluation_functions import (\n",
    "    evaluate_model,\n",
    "    aggregate_iteration_losses,\n",
    "    aggregate_iteration_losses_softmax\n",
    ")\n",
    "\n",
    "from core.analysis.plotting import (\n",
    "    plot_confusion_matrix_annotated,\n",
    "    plot_error_curves,\n",
    "    plot_accuracy_vs_max_iter,\n",
    "    plot_runtime_vs_max_iter,\n",
    "    plot_performance_summary_extended,\n",
    "    plot_train_curves_three_models,\n",
    "    plot_metric_vs_learning_rate,\n",
    "    plot_accuracy_vs_max_iter_4models,\n",
    "    plot_runtime_vs_max_iter_4models,\n",
    "    plot_accuracy_vs_runtime,\n",
    "    plot_performance_summary_extended_by_runtime,\n",
    "    plot_performance_summary_4models_by_runtime,\n",
    "    plot_accuracy_vs_runtime_4models\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"MyGlobalLogger\") # configured in core/logger/config.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keDSGERzwvrB"
   },
   "source": [
    "# Choose Run Parameters **(Significant Effect On Model's Runtime!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tNJHO7mQrANq",
    "outputId": "6b4dd274-1d3f-4489-9ebd-ebb7c4299b04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 21:25:45,596 - INFO - === Perceptron Run Parameters ===\n",
      "2025-03-18 21:25:45,597 - INFO - max_iter_values = [20, 50, 100, 1000]\n",
      "2025-03-18 21:25:45,597 - INFO - === Regression Run Parameters ===\n",
      "2025-03-18 21:25:45,598 - INFO - LR=0.1/Iter=100 -> learning_rate=0.1, max_iter=100\n",
      "2025-03-18 21:25:45,598 - INFO - LR=0.1/Iter=1000 -> learning_rate=0.1, max_iter=1000\n",
      "2025-03-18 21:25:45,599 - INFO - LR=0.1/Iter=10000 -> learning_rate=0.1, max_iter=10000\n",
      "2025-03-18 21:25:45,597 - INFO - max_iter_values = [20, 50, 100, 1000]\n",
      "2025-03-18 21:25:45,597 - INFO - === Regression Run Parameters ===\n",
      "2025-03-18 21:25:45,598 - INFO - LR=0.1/Iter=100 -> learning_rate=0.1, max_iter=100\n",
      "2025-03-18 21:25:45,598 - INFO - LR=0.1/Iter=1000 -> learning_rate=0.1, max_iter=1000\n",
      "2025-03-18 21:25:45,599 - INFO - LR=0.1/Iter=10000 -> learning_rate=0.1, max_iter=10000\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# SEPARATE RUN PARAMETERS FOR PERCEPTRONS vs. REGRESSIONS\n",
    "#######################################################################\n",
    "\n",
    "# Perceptrons (Clean & Pocket) iteration-based run\n",
    "perceptron_max_iter_values = [20,50,100,1000]  # for Clean PLA & Pocket PLA\n",
    "# Logging the run parameters\n",
    "logger.info(f\"=== Perceptron Run Parameters ===\")\n",
    "logger.info(f\"max_iter_values = {perceptron_max_iter_values}\")\n",
    "\n",
    "\n",
    "# Regression (Softmax & Linear) run parameters.\n",
    "learning_rates = [0.1]\n",
    "iteration_counts = [100,1000,10000]\n",
    "regression_run_configs = [\n",
    "    {\n",
    "        \"label\": f\"LR={lr}/Iter={it}\",\n",
    "        \"learning_rate\": lr,\n",
    "        \"max_iter\": it\n",
    "    }\n",
    "    for lr in learning_rates\n",
    "    for it in iteration_counts\n",
    "]\n",
    "\n",
    "logger.info(f\"=== Regression Run Parameters ===\")\n",
    "for cfg in regression_run_configs:\n",
    "    logger.info(f\"{cfg['label']} -> learning_rate={cfg['learning_rate']}, max_iter={cfg['max_iter']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e73BoKY7cmJU"
   },
   "source": [
    "# Load and Preprocess the MNIST Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "osGLi3Hic5qW",
    "outputId": "1f51e239-f74c-418a-e3ed-801afd689fa7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 21:25:47,833 - INFO - Raw MNIST data shapes: X_raw: (70000, 784), y_raw: (70000,)\n",
      "2025-03-18 21:25:48,000 - INFO - Preprocessed shape: (70000, 785)\n",
      "2025-03-18 21:25:48,002 - INFO - Train set: X_train: (60000, 785), y_train: (60000,)\n",
      "2025-03-18 21:25:48,002 - INFO - Test set: X_test: (10000, 785), y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We'll load the MNIST dataset using our custom loader (`mnist_loader`) and then apply preprocessing (`data_preprocessing`).\n",
    "The preprocessing step normalizes each image to the range [0, 1] and adds a bias term, resulting in input samples with 785 features.\n",
    "This setup ensures that the training set contains 60,000 samples and the test set 10,000 samples, preparing the data for the subsequent classification tasks.\n",
    "'''\n",
    "\n",
    "# New section\n",
    "# Load raw MNIST data (X: images, y: labels)\n",
    "X_raw, y_raw = load_mnist()\n",
    "\n",
    "\n",
    "logger.info(\"Raw MNIST data shapes: X_raw: %s, y_raw: %s\", X_raw.shape, y_raw.shape)\n",
    "\n",
    "# Preprocess (normalize & add bias = True)\n",
    "X = preprocess_data(X_raw, add_bias=True, normalize=True)\n",
    "logger.info(\"Preprocessed shape: %s\", X.shape)\n",
    "\n",
    "# Split into train/test manually or with 60k/10k as the task suggests\n",
    "X_train, y_train = X[:60000], y_raw[:60000]\n",
    "X_test,  y_test  = X[60000:], y_raw[60000:]\n",
    "\n",
    "logger.info(\"Train set: X_train: %s, y_train: %s\", X_train.shape, y_train.shape)\n",
    "logger.info(\"Test set: X_test: %s, y_test: %s\", X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-O4hrMBCejtr"
   },
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e637ccdffee5428897190964713e4df0",
      "d8ddbe8dea3d4f7f99361f9eb81f2ab0",
      "7eb0fc2ebb6449e2888386391ca55d70",
      "c88135114994459b8babe54b307dd75b",
      "c2b28605e3b44cc7846a45c039351958",
      "ae3831170c414a1eaf29434c4aaec771",
      "3b66d8c07f8540998106f3e1d30fcf0c",
      "f52eb9ff71ec48feb8b09d0d80228c58",
      "fba5d1c8e57c44adb12faeb482e60424",
      "99a9e2e08aa046a49b6332e2b054416b",
      "e371b2dc55164dcfab419c03cd2e1e25"
     ]
    },
    "id": "Sik1JDX6Hxub",
    "outputId": "a5b516a7-c6a3-460e-b3df-d0cbeaea748c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 21:25:48,008 - INFO - === TRAINING REGRESSION MODELS (Softmax & Linear) ===\n",
      "Train Regressions:   0%|          | 0/3 [00:00<?, ?it/s]2025-03-18 21:25:48,010 - INFO - --- Softmax LR=0.1/Iter=100 ---\n",
      "Train Regressions:   0%|          | 0/3 [00:00<?, ?it/s]2025-03-18 21:25:48,010 - INFO - --- Softmax LR=0.1/Iter=100 ---\n",
      "2025-03-18 21:25:48,054 - INFO - Iter 1/100, Loss: 2.4006, Avg Adaptive LR: 14.067368\n",
      "2025-03-18 21:25:48,480 - INFO - Iter 11/100, Loss: 0.4338, Avg Adaptive LR: 3.044223\n",
      "2025-03-18 21:25:48,888 - INFO - Iter 21/100, Loss: 0.3772, Avg Adaptive LR: 3.040986\n",
      "2025-03-18 21:25:49,299 - INFO - Iter 31/100, Loss: 0.3520, Avg Adaptive LR: 3.039560\n",
      "2025-03-18 21:25:49,720 - INFO - Iter 41/100, Loss: 0.3361, Avg Adaptive LR: 3.038652\n",
      "2025-03-18 21:25:50,131 - INFO - Iter 51/100, Loss: 0.3248, Avg Adaptive LR: 3.038002\n",
      "2025-03-18 21:25:50,543 - INFO - Iter 61/100, Loss: 0.3162, Avg Adaptive LR: 3.037505\n",
      "2025-03-18 21:25:50,944 - INFO - Iter 71/100, Loss: 0.3093, Avg Adaptive LR: 3.037109\n",
      "2025-03-18 21:25:51,356 - INFO - Iter 81/100, Loss: 0.3037, Avg Adaptive LR: 3.036782\n",
      "2025-03-18 21:25:51,756 - INFO - Iter 91/100, Loss: 0.2989, Avg Adaptive LR: 3.036506\n",
      "2025-03-18 21:25:52,119 - INFO - SoftmaxRegression training completed in 4.11 seconds.\n",
      "2025-03-18 21:25:52,120 - INFO - --- Linear Regression LR=0.1/Iter=100 ---\n",
      "2025-03-18 21:25:55,318 - INFO - Iter 100/100, Loss: 0.7147, Gradient Norm: 16.0981, Avg Adaptive LR: 1.3971574443060855\n",
      "2025-03-18 21:25:55,318 - INFO - LinearRegressionClassifier training completed in 3.20 seconds.\n",
      "Train Regressions:  33%|███▎      | 1/3 [00:07<00:14,  7.31s/it]2025-03-18 21:25:55,319 - INFO - --- Softmax LR=0.1/Iter=1000 ---\n",
      "2025-03-18 21:25:55,358 - INFO - Iter 1/1000, Loss: 2.3143, Avg Adaptive LR: 13.534203\n",
      "2025-03-18 21:25:55,760 - INFO - Iter 11/1000, Loss: 0.4351, Avg Adaptive LR: 3.351356\n",
      "2025-03-18 21:25:56,174 - INFO - Iter 21/1000, Loss: 0.3740, Avg Adaptive LR: 3.346282\n",
      "2025-03-18 21:25:56,583 - INFO - Iter 31/1000, Loss: 0.3483, Avg Adaptive LR: 3.344529\n",
      "2025-03-18 21:25:56,986 - INFO - Iter 41/1000, Loss: 0.3324, Avg Adaptive LR: 3.343431\n",
      "2025-03-18 21:25:57,392 - INFO - Iter 51/1000, Loss: 0.3212, Avg Adaptive LR: 3.342651\n",
      "2025-03-18 21:25:57,794 - INFO - Iter 61/1000, Loss: 0.3128, Avg Adaptive LR: 3.342059\n",
      "2025-03-18 21:25:58,195 - INFO - Iter 71/1000, Loss: 0.3060, Avg Adaptive LR: 3.341587\n",
      "2025-03-18 21:25:58,608 - INFO - Iter 81/1000, Loss: 0.3005, Avg Adaptive LR: 3.341200\n",
      "2025-03-18 21:25:59,008 - INFO - Iter 91/1000, Loss: 0.2959, Avg Adaptive LR: 3.340875\n",
      "2025-03-18 21:25:59,419 - INFO - Iter 101/1000, Loss: 0.2919, Avg Adaptive LR: 3.340596\n",
      "2025-03-18 21:25:59,829 - INFO - Iter 111/1000, Loss: 0.2885, Avg Adaptive LR: 3.340353\n",
      "2025-03-18 21:26:00,235 - INFO - Iter 121/1000, Loss: 0.2854, Avg Adaptive LR: 3.340139\n",
      "2025-03-18 21:26:00,641 - INFO - Iter 131/1000, Loss: 0.2827, Avg Adaptive LR: 3.339949\n",
      "2025-03-18 21:26:01,042 - INFO - Iter 141/1000, Loss: 0.2803, Avg Adaptive LR: 3.339778\n",
      "2025-03-18 21:26:01,445 - INFO - Iter 151/1000, Loss: 0.2781, Avg Adaptive LR: 3.339624\n",
      "2025-03-18 21:26:01,847 - INFO - Iter 161/1000, Loss: 0.2761, Avg Adaptive LR: 3.339483\n",
      "2025-03-18 21:26:02,256 - INFO - Iter 171/1000, Loss: 0.2743, Avg Adaptive LR: 3.339354\n",
      "2025-03-18 21:26:02,656 - INFO - Iter 181/1000, Loss: 0.2726, Avg Adaptive LR: 3.339235\n",
      "2025-03-18 21:26:03,071 - INFO - Iter 191/1000, Loss: 0.2710, Avg Adaptive LR: 3.339125\n",
      "2025-03-18 21:26:03,478 - INFO - Iter 201/1000, Loss: 0.2696, Avg Adaptive LR: 3.339023\n",
      "2025-03-18 21:26:03,885 - INFO - Iter 211/1000, Loss: 0.2682, Avg Adaptive LR: 3.338928\n",
      "2025-03-18 21:26:04,291 - INFO - Iter 221/1000, Loss: 0.2670, Avg Adaptive LR: 3.338839\n",
      "2025-03-18 21:26:04,688 - INFO - Iter 231/1000, Loss: 0.2658, Avg Adaptive LR: 3.338755\n",
      "2025-03-18 21:26:05,085 - INFO - Iter 241/1000, Loss: 0.2647, Avg Adaptive LR: 3.338676\n",
      "2025-03-18 21:26:05,497 - INFO - Iter 251/1000, Loss: 0.2636, Avg Adaptive LR: 3.338601\n",
      "2025-03-18 21:26:05,893 - INFO - Iter 261/1000, Loss: 0.2626, Avg Adaptive LR: 3.338531\n",
      "2025-03-18 21:26:06,297 - INFO - Iter 271/1000, Loss: 0.2617, Avg Adaptive LR: 3.338464\n",
      "2025-03-18 21:26:06,694 - INFO - Iter 281/1000, Loss: 0.2608, Avg Adaptive LR: 3.338400\n",
      "2025-03-18 21:26:07,094 - INFO - Iter 291/1000, Loss: 0.2599, Avg Adaptive LR: 3.338339\n",
      "2025-03-18 21:26:07,499 - INFO - Iter 301/1000, Loss: 0.2591, Avg Adaptive LR: 3.338282\n",
      "2025-03-18 21:26:07,898 - INFO - Iter 311/1000, Loss: 0.2583, Avg Adaptive LR: 3.338226\n",
      "2025-03-18 21:26:08,303 - INFO - Iter 321/1000, Loss: 0.2576, Avg Adaptive LR: 3.338173\n",
      "2025-03-18 21:26:08,701 - INFO - Iter 331/1000, Loss: 0.2569, Avg Adaptive LR: 3.338122\n",
      "2025-03-18 21:26:09,100 - INFO - Iter 341/1000, Loss: 0.2562, Avg Adaptive LR: 3.338074\n",
      "2025-03-18 21:26:09,502 - INFO - Iter 351/1000, Loss: 0.2555, Avg Adaptive LR: 3.338027\n",
      "2025-03-18 21:26:09,896 - INFO - Iter 361/1000, Loss: 0.2549, Avg Adaptive LR: 3.337982\n",
      "2025-03-18 21:26:10,308 - INFO - Iter 371/1000, Loss: 0.2543, Avg Adaptive LR: 3.337938\n",
      "2025-03-18 21:26:10,705 - INFO - Iter 381/1000, Loss: 0.2537, Avg Adaptive LR: 3.337896\n",
      "2025-03-18 21:26:11,107 - INFO - Iter 391/1000, Loss: 0.2531, Avg Adaptive LR: 3.337856\n",
      "2025-03-18 21:26:11,515 - INFO - Iter 401/1000, Loss: 0.2525, Avg Adaptive LR: 3.337817\n",
      "2025-03-18 21:26:11,915 - INFO - Iter 411/1000, Loss: 0.2520, Avg Adaptive LR: 3.337779\n",
      "2025-03-18 21:26:12,329 - INFO - Iter 421/1000, Loss: 0.2515, Avg Adaptive LR: 3.337742\n",
      "2025-03-18 21:26:12,727 - INFO - Iter 431/1000, Loss: 0.2510, Avg Adaptive LR: 3.337707\n",
      "2025-03-18 21:26:13,124 - INFO - Iter 441/1000, Loss: 0.2505, Avg Adaptive LR: 3.337672\n",
      "2025-03-18 21:26:13,544 - INFO - Iter 451/1000, Loss: 0.2500, Avg Adaptive LR: 3.337639\n",
      "2025-03-18 21:26:13,956 - INFO - Iter 461/1000, Loss: 0.2496, Avg Adaptive LR: 3.337606\n",
      "2025-03-18 21:26:14,359 - INFO - Iter 471/1000, Loss: 0.2491, Avg Adaptive LR: 3.337575\n",
      "2025-03-18 21:26:14,761 - INFO - Iter 481/1000, Loss: 0.2487, Avg Adaptive LR: 3.337544\n",
      "2025-03-18 21:26:15,158 - INFO - Iter 491/1000, Loss: 0.2483, Avg Adaptive LR: 3.337514\n",
      "2025-03-18 21:26:15,562 - INFO - Iter 501/1000, Loss: 0.2479, Avg Adaptive LR: 3.337485\n",
      "2025-03-18 21:26:15,963 - INFO - Iter 511/1000, Loss: 0.2475, Avg Adaptive LR: 3.337457\n",
      "2025-03-18 21:26:16,368 - INFO - Iter 521/1000, Loss: 0.2471, Avg Adaptive LR: 3.337429\n",
      "2025-03-18 21:26:16,762 - INFO - Iter 531/1000, Loss: 0.2467, Avg Adaptive LR: 3.337402\n",
      "2025-03-18 21:26:17,155 - INFO - Iter 541/1000, Loss: 0.2463, Avg Adaptive LR: 3.337376\n",
      "2025-03-18 21:26:17,562 - INFO - Iter 551/1000, Loss: 0.2459, Avg Adaptive LR: 3.337350\n",
      "2025-03-18 21:26:17,956 - INFO - Iter 561/1000, Loss: 0.2456, Avg Adaptive LR: 3.337325\n",
      "2025-03-18 21:26:18,373 - INFO - Iter 571/1000, Loss: 0.2452, Avg Adaptive LR: 3.337300\n",
      "2025-03-18 21:26:18,771 - INFO - Iter 581/1000, Loss: 0.2449, Avg Adaptive LR: 3.337277\n",
      "2025-03-18 21:26:19,176 - INFO - Iter 591/1000, Loss: 0.2446, Avg Adaptive LR: 3.337253\n",
      "2025-03-18 21:26:19,582 - INFO - Iter 601/1000, Loss: 0.2443, Avg Adaptive LR: 3.337230\n",
      "2025-03-18 21:26:19,983 - INFO - Iter 611/1000, Loss: 0.2439, Avg Adaptive LR: 3.337208\n",
      "2025-03-18 21:26:20,388 - INFO - Iter 621/1000, Loss: 0.2436, Avg Adaptive LR: 3.337186\n",
      "2025-03-18 21:26:20,791 - INFO - Iter 631/1000, Loss: 0.2433, Avg Adaptive LR: 3.337164\n",
      "2025-03-18 21:26:21,189 - INFO - Iter 641/1000, Loss: 0.2430, Avg Adaptive LR: 3.337143\n",
      "2025-03-18 21:26:21,600 - INFO - Iter 651/1000, Loss: 0.2427, Avg Adaptive LR: 3.337123\n",
      "2025-03-18 21:26:21,999 - INFO - Iter 661/1000, Loss: 0.2425, Avg Adaptive LR: 3.337103\n",
      "2025-03-18 21:26:22,404 - INFO - Iter 671/1000, Loss: 0.2422, Avg Adaptive LR: 3.337083\n",
      "2025-03-18 21:26:22,801 - INFO - Iter 681/1000, Loss: 0.2419, Avg Adaptive LR: 3.337063\n",
      "2025-03-18 21:26:23,199 - INFO - Iter 691/1000, Loss: 0.2416, Avg Adaptive LR: 3.337044\n",
      "2025-03-18 21:26:23,619 - INFO - Iter 701/1000, Loss: 0.2414, Avg Adaptive LR: 3.337025\n",
      "2025-03-18 21:26:24,024 - INFO - Iter 711/1000, Loss: 0.2411, Avg Adaptive LR: 3.337007\n",
      "2025-03-18 21:26:24,440 - INFO - Iter 721/1000, Loss: 0.2409, Avg Adaptive LR: 3.336989\n",
      "2025-03-18 21:26:24,842 - INFO - Iter 731/1000, Loss: 0.2406, Avg Adaptive LR: 3.336971\n",
      "2025-03-18 21:26:25,251 - INFO - Iter 741/1000, Loss: 0.2404, Avg Adaptive LR: 3.336954\n",
      "2025-03-18 21:26:25,647 - INFO - Iter 751/1000, Loss: 0.2401, Avg Adaptive LR: 3.336937\n",
      "2025-03-18 21:26:26,042 - INFO - Iter 761/1000, Loss: 0.2399, Avg Adaptive LR: 3.336920\n",
      "2025-03-18 21:26:26,444 - INFO - Iter 771/1000, Loss: 0.2396, Avg Adaptive LR: 3.336903\n",
      "2025-03-18 21:26:26,848 - INFO - Iter 781/1000, Loss: 0.2394, Avg Adaptive LR: 3.336887\n",
      "2025-03-18 21:26:27,254 - INFO - Iter 791/1000, Loss: 0.2392, Avg Adaptive LR: 3.336871\n",
      "2025-03-18 21:26:27,655 - INFO - Iter 801/1000, Loss: 0.2390, Avg Adaptive LR: 3.336855\n",
      "2025-03-18 21:26:28,056 - INFO - Iter 811/1000, Loss: 0.2387, Avg Adaptive LR: 3.336840\n",
      "2025-03-18 21:26:28,466 - INFO - Iter 821/1000, Loss: 0.2385, Avg Adaptive LR: 3.336825\n",
      "2025-03-18 21:26:28,862 - INFO - Iter 831/1000, Loss: 0.2383, Avg Adaptive LR: 3.336810\n",
      "2025-03-18 21:26:29,272 - INFO - Iter 841/1000, Loss: 0.2381, Avg Adaptive LR: 3.336795\n",
      "2025-03-18 21:26:29,675 - INFO - Iter 851/1000, Loss: 0.2379, Avg Adaptive LR: 3.336780\n",
      "2025-03-18 21:26:30,080 - INFO - Iter 861/1000, Loss: 0.2377, Avg Adaptive LR: 3.336766\n",
      "2025-03-18 21:26:30,486 - INFO - Iter 871/1000, Loss: 0.2375, Avg Adaptive LR: 3.336752\n",
      "2025-03-18 21:26:30,899 - INFO - Iter 881/1000, Loss: 0.2373, Avg Adaptive LR: 3.336738\n",
      "2025-03-18 21:26:31,304 - INFO - Iter 891/1000, Loss: 0.2371, Avg Adaptive LR: 3.336724\n",
      "2025-03-18 21:26:31,696 - INFO - Iter 901/1000, Loss: 0.2369, Avg Adaptive LR: 3.336711\n",
      "2025-03-18 21:26:32,092 - INFO - Iter 911/1000, Loss: 0.2367, Avg Adaptive LR: 3.336698\n",
      "2025-03-18 21:26:32,496 - INFO - Iter 921/1000, Loss: 0.2366, Avg Adaptive LR: 3.336684\n",
      "2025-03-18 21:26:32,892 - INFO - Iter 931/1000, Loss: 0.2364, Avg Adaptive LR: 3.336672\n",
      "2025-03-18 21:26:33,295 - INFO - Iter 941/1000, Loss: 0.2362, Avg Adaptive LR: 3.336659\n",
      "2025-03-18 21:26:33,695 - INFO - Iter 951/1000, Loss: 0.2360, Avg Adaptive LR: 3.336646\n",
      "2025-03-18 21:26:34,098 - INFO - Iter 961/1000, Loss: 0.2358, Avg Adaptive LR: 3.336634\n",
      "2025-03-18 21:26:34,519 - INFO - Iter 971/1000, Loss: 0.2357, Avg Adaptive LR: 3.336622\n",
      "2025-03-18 21:26:34,916 - INFO - Iter 981/1000, Loss: 0.2355, Avg Adaptive LR: 3.336609\n",
      "2025-03-18 21:26:35,330 - INFO - Iter 991/1000, Loss: 0.2353, Avg Adaptive LR: 3.336598\n",
      "2025-03-18 21:26:35,690 - INFO - SoftmaxRegression training completed in 40.37 seconds.\n",
      "2025-03-18 21:26:35,691 - INFO - --- Linear Regression LR=0.1/Iter=1000 ---\n",
      "2025-03-18 21:26:38,822 - INFO - Iter 100/1000, Loss: 0.7348, Gradient Norm: 16.3445, Avg Adaptive LR: 1.3971218005437425\n",
      "2025-03-18 21:26:41,998 - INFO - Iter 200/1000, Loss: 0.4130, Gradient Norm: 11.9788, Avg Adaptive LR: 0.9913774491410133\n",
      "2025-03-18 21:26:45,342 - INFO - Iter 300/1000, Loss: 0.2897, Gradient Norm: 9.8017, Avg Adaptive LR: 0.8108005591183135\n",
      "2025-03-18 21:26:48,670 - INFO - Iter 400/1000, Loss: 0.2214, Gradient Norm: 8.3568, Avg Adaptive LR: 0.702854553981635\n",
      "2025-03-18 21:26:51,986 - INFO - Iter 500/1000, Loss: 0.1827, Gradient Norm: 7.4142, Avg Adaptive LR: 0.6291135396210678\n",
      "2025-03-18 21:26:55,332 - INFO - Iter 600/1000, Loss: 0.1514, Gradient Norm: 6.5548, Avg Adaptive LR: 0.5745581695686357\n",
      "2025-03-18 21:26:58,660 - INFO - Iter 700/1000, Loss: 0.1321, Gradient Norm: 5.9598, Avg Adaptive LR: 0.5321583299184253\n",
      "2025-03-18 21:27:01,980 - INFO - Iter 800/1000, Loss: 0.1166, Gradient Norm: 5.4377, Avg Adaptive LR: 0.49793763751408765\n",
      "2025-03-18 21:27:05,340 - INFO - Iter 900/1000, Loss: 0.1051, Gradient Norm: 5.0141, Avg Adaptive LR: 0.46959128033932185\n",
      "2025-03-18 21:27:08,637 - INFO - Iter 1000/1000, Loss: 0.0960, Gradient Norm: 4.6565, Avg Adaptive LR: 0.44559773839234845\n",
      "2025-03-18 21:27:08,637 - INFO - LinearRegressionClassifier training completed in 32.95 seconds.\n",
      "Train Regressions:  67%|██████▋   | 2/3 [01:20<00:46, 46.14s/it]2025-03-18 21:27:08,638 - INFO - --- Softmax LR=0.1/Iter=10000 ---\n",
      "2025-03-18 21:27:08,680 - INFO - Iter 1/10000, Loss: 2.3145, Avg Adaptive LR: 15.220312\n",
      "2025-03-18 21:27:09,079 - INFO - Iter 11/10000, Loss: 0.4765, Avg Adaptive LR: 3.326323\n",
      "2025-03-18 21:27:09,483 - INFO - Iter 21/10000, Loss: 0.3710, Avg Adaptive LR: 3.305129\n",
      "2025-03-18 21:27:09,890 - INFO - Iter 31/10000, Loss: 0.3469, Avg Adaptive LR: 3.303520\n",
      "2025-03-18 21:27:10,298 - INFO - Iter 41/10000, Loss: 0.3316, Avg Adaptive LR: 3.302486\n",
      "2025-03-18 21:27:10,692 - INFO - Iter 51/10000, Loss: 0.3207, Avg Adaptive LR: 3.301743\n",
      "2025-03-18 21:27:11,086 - INFO - Iter 61/10000, Loss: 0.3124, Avg Adaptive LR: 3.301174\n",
      "2025-03-18 21:27:11,489 - INFO - Iter 71/10000, Loss: 0.3057, Avg Adaptive LR: 3.300720\n",
      "2025-03-18 21:27:11,888 - INFO - Iter 81/10000, Loss: 0.3002, Avg Adaptive LR: 3.300346\n",
      "2025-03-18 21:27:12,292 - INFO - Iter 91/10000, Loss: 0.2957, Avg Adaptive LR: 3.300031\n",
      "2025-03-18 21:27:12,691 - INFO - Iter 101/10000, Loss: 0.2917, Avg Adaptive LR: 3.299761\n",
      "2025-03-18 21:27:13,098 - INFO - Iter 111/10000, Loss: 0.2883, Avg Adaptive LR: 3.299526\n",
      "2025-03-18 21:27:13,501 - INFO - Iter 121/10000, Loss: 0.2853, Avg Adaptive LR: 3.299318\n",
      "2025-03-18 21:27:13,903 - INFO - Iter 131/10000, Loss: 0.2826, Avg Adaptive LR: 3.299134\n",
      "2025-03-18 21:27:14,314 - INFO - Iter 141/10000, Loss: 0.2802, Avg Adaptive LR: 3.298968\n",
      "2025-03-18 21:27:14,710 - INFO - Iter 151/10000, Loss: 0.2780, Avg Adaptive LR: 3.298817\n",
      "2025-03-18 21:27:15,141 - INFO - Iter 161/10000, Loss: 0.2760, Avg Adaptive LR: 3.298680\n",
      "2025-03-18 21:27:15,548 - INFO - Iter 171/10000, Loss: 0.2742, Avg Adaptive LR: 3.298554\n",
      "2025-03-18 21:27:15,952 - INFO - Iter 181/10000, Loss: 0.2725, Avg Adaptive LR: 3.298438\n",
      "2025-03-18 21:27:16,367 - INFO - Iter 191/10000, Loss: 0.2709, Avg Adaptive LR: 3.298331\n",
      "2025-03-18 21:27:16,784 - INFO - Iter 201/10000, Loss: 0.2695, Avg Adaptive LR: 3.298231\n",
      "2025-03-18 21:27:17,192 - INFO - Iter 211/10000, Loss: 0.2681, Avg Adaptive LR: 3.298138\n",
      "2025-03-18 21:27:17,602 - INFO - Iter 221/10000, Loss: 0.2669, Avg Adaptive LR: 3.298050\n",
      "2025-03-18 21:27:18,008 - INFO - Iter 231/10000, Loss: 0.2657, Avg Adaptive LR: 3.297968\n",
      "2025-03-18 21:27:18,416 - INFO - Iter 241/10000, Loss: 0.2645, Avg Adaptive LR: 3.297891\n",
      "2025-03-18 21:27:18,820 - INFO - Iter 251/10000, Loss: 0.2635, Avg Adaptive LR: 3.297818\n",
      "2025-03-18 21:27:19,228 - INFO - Iter 261/10000, Loss: 0.2625, Avg Adaptive LR: 3.297749\n",
      "2025-03-18 21:27:19,640 - INFO - Iter 271/10000, Loss: 0.2615, Avg Adaptive LR: 3.297683\n",
      "2025-03-18 21:27:20,046 - INFO - Iter 281/10000, Loss: 0.2606, Avg Adaptive LR: 3.297620\n",
      "2025-03-18 21:27:20,459 - INFO - Iter 291/10000, Loss: 0.2598, Avg Adaptive LR: 3.297561\n",
      "2025-03-18 21:27:20,876 - INFO - Iter 301/10000, Loss: 0.2589, Avg Adaptive LR: 3.297504\n",
      "2025-03-18 21:27:21,291 - INFO - Iter 311/10000, Loss: 0.2582, Avg Adaptive LR: 3.297450\n",
      "2025-03-18 21:27:21,700 - INFO - Iter 321/10000, Loss: 0.2574, Avg Adaptive LR: 3.297398\n",
      "2025-03-18 21:27:22,113 - INFO - Iter 331/10000, Loss: 0.2567, Avg Adaptive LR: 3.297348\n",
      "2025-03-18 21:27:22,525 - INFO - Iter 341/10000, Loss: 0.2560, Avg Adaptive LR: 3.297300\n",
      "2025-03-18 21:27:22,922 - INFO - Iter 351/10000, Loss: 0.2553, Avg Adaptive LR: 3.297254\n",
      "2025-03-18 21:27:23,351 - INFO - Iter 361/10000, Loss: 0.2547, Avg Adaptive LR: 3.297210\n",
      "2025-03-18 21:27:23,754 - INFO - Iter 371/10000, Loss: 0.2541, Avg Adaptive LR: 3.297167\n",
      "2025-03-18 21:27:24,155 - INFO - Iter 381/10000, Loss: 0.2535, Avg Adaptive LR: 3.297126\n",
      "2025-03-18 21:27:24,573 - INFO - Iter 391/10000, Loss: 0.2529, Avg Adaptive LR: 3.297086\n",
      "2025-03-18 21:27:24,978 - INFO - Iter 401/10000, Loss: 0.2523, Avg Adaptive LR: 3.297048\n",
      "2025-03-18 21:27:25,404 - INFO - Iter 411/10000, Loss: 0.2518, Avg Adaptive LR: 3.297011\n",
      "2025-03-18 21:27:25,803 - INFO - Iter 421/10000, Loss: 0.2513, Avg Adaptive LR: 3.296975\n",
      "2025-03-18 21:27:26,204 - INFO - Iter 431/10000, Loss: 0.2508, Avg Adaptive LR: 3.296940\n",
      "2025-03-18 21:27:26,625 - INFO - Iter 441/10000, Loss: 0.2503, Avg Adaptive LR: 3.296906\n",
      "2025-03-18 21:27:27,034 - INFO - Iter 451/10000, Loss: 0.2498, Avg Adaptive LR: 3.296873\n",
      "2025-03-18 21:27:27,446 - INFO - Iter 461/10000, Loss: 0.2493, Avg Adaptive LR: 3.296842\n",
      "2025-03-18 21:27:27,842 - INFO - Iter 471/10000, Loss: 0.2489, Avg Adaptive LR: 3.296811\n",
      "2025-03-18 21:27:28,247 - INFO - Iter 481/10000, Loss: 0.2485, Avg Adaptive LR: 3.296781\n",
      "2025-03-18 21:27:28,643 - INFO - Iter 491/10000, Loss: 0.2480, Avg Adaptive LR: 3.296751\n",
      "2025-03-18 21:27:29,044 - INFO - Iter 501/10000, Loss: 0.2476, Avg Adaptive LR: 3.296723\n",
      "2025-03-18 21:27:29,449 - INFO - Iter 511/10000, Loss: 0.2472, Avg Adaptive LR: 3.296695\n",
      "2025-03-18 21:27:29,851 - INFO - Iter 521/10000, Loss: 0.2468, Avg Adaptive LR: 3.296668\n",
      "2025-03-18 21:27:30,256 - INFO - Iter 531/10000, Loss: 0.2465, Avg Adaptive LR: 3.296642\n",
      "2025-03-18 21:27:30,661 - INFO - Iter 541/10000, Loss: 0.2461, Avg Adaptive LR: 3.296616\n",
      "2025-03-18 21:27:31,056 - INFO - Iter 551/10000, Loss: 0.2457, Avg Adaptive LR: 3.296591\n",
      "2025-03-18 21:27:31,463 - INFO - Iter 561/10000, Loss: 0.2454, Avg Adaptive LR: 3.296567\n",
      "2025-03-18 21:27:31,858 - INFO - Iter 571/10000, Loss: 0.2450, Avg Adaptive LR: 3.296543\n",
      "2025-03-18 21:27:32,263 - INFO - Iter 581/10000, Loss: 0.2447, Avg Adaptive LR: 3.296519\n",
      "2025-03-18 21:27:32,667 - INFO - Iter 591/10000, Loss: 0.2444, Avg Adaptive LR: 3.296497\n",
      "2025-03-18 21:27:33,069 - INFO - Iter 601/10000, Loss: 0.2440, Avg Adaptive LR: 3.296474\n",
      "2025-03-18 21:27:33,479 - INFO - Iter 611/10000, Loss: 0.2437, Avg Adaptive LR: 3.296452\n",
      "2025-03-18 21:27:33,921 - INFO - Iter 621/10000, Loss: 0.2434, Avg Adaptive LR: 3.296431\n",
      "2025-03-18 21:27:34,327 - INFO - Iter 631/10000, Loss: 0.2431, Avg Adaptive LR: 3.296410\n",
      "2025-03-18 21:27:34,742 - INFO - Iter 641/10000, Loss: 0.2428, Avg Adaptive LR: 3.296390\n",
      "2025-03-18 21:27:35,144 - INFO - Iter 651/10000, Loss: 0.2425, Avg Adaptive LR: 3.296370\n",
      "2025-03-18 21:27:35,554 - INFO - Iter 661/10000, Loss: 0.2422, Avg Adaptive LR: 3.296350\n",
      "2025-03-18 21:27:35,952 - INFO - Iter 671/10000, Loss: 0.2420, Avg Adaptive LR: 3.296331\n",
      "2025-03-18 21:27:36,365 - INFO - Iter 681/10000, Loss: 0.2417, Avg Adaptive LR: 3.296312\n",
      "2025-03-18 21:27:36,767 - INFO - Iter 691/10000, Loss: 0.2414, Avg Adaptive LR: 3.296293\n",
      "2025-03-18 21:27:37,172 - INFO - Iter 701/10000, Loss: 0.2411, Avg Adaptive LR: 3.296275\n",
      "2025-03-18 21:27:37,584 - INFO - Iter 711/10000, Loss: 0.2409, Avg Adaptive LR: 3.296257\n",
      "2025-03-18 21:27:37,979 - INFO - Iter 721/10000, Loss: 0.2406, Avg Adaptive LR: 3.296239\n",
      "2025-03-18 21:27:38,384 - INFO - Iter 731/10000, Loss: 0.2404, Avg Adaptive LR: 3.296222\n",
      "2025-03-18 21:27:38,782 - INFO - Iter 741/10000, Loss: 0.2401, Avg Adaptive LR: 3.296205\n",
      "2025-03-18 21:27:39,176 - INFO - Iter 751/10000, Loss: 0.2399, Avg Adaptive LR: 3.296189\n",
      "2025-03-18 21:27:39,580 - INFO - Iter 761/10000, Loss: 0.2397, Avg Adaptive LR: 3.296172\n",
      "2025-03-18 21:27:39,978 - INFO - Iter 771/10000, Loss: 0.2394, Avg Adaptive LR: 3.296156\n",
      "2025-03-18 21:27:40,387 - INFO - Iter 781/10000, Loss: 0.2392, Avg Adaptive LR: 3.296140\n",
      "2025-03-18 21:27:40,788 - INFO - Iter 791/10000, Loss: 0.2390, Avg Adaptive LR: 3.296125\n",
      "2025-03-18 21:27:41,197 - INFO - Iter 801/10000, Loss: 0.2387, Avg Adaptive LR: 3.296109\n",
      "2025-03-18 21:27:41,603 - INFO - Iter 811/10000, Loss: 0.2385, Avg Adaptive LR: 3.296094\n",
      "2025-03-18 21:27:42,005 - INFO - Iter 821/10000, Loss: 0.2383, Avg Adaptive LR: 3.296079\n",
      "2025-03-18 21:27:42,408 - INFO - Iter 831/10000, Loss: 0.2381, Avg Adaptive LR: 3.296065\n",
      "2025-03-18 21:27:42,808 - INFO - Iter 841/10000, Loss: 0.2379, Avg Adaptive LR: 3.296051\n",
      "2025-03-18 21:27:43,205 - INFO - Iter 851/10000, Loss: 0.2377, Avg Adaptive LR: 3.296036\n",
      "2025-03-18 21:27:43,611 - INFO - Iter 861/10000, Loss: 0.2375, Avg Adaptive LR: 3.296022\n",
      "2025-03-18 21:27:44,017 - INFO - Iter 871/10000, Loss: 0.2373, Avg Adaptive LR: 3.296009\n",
      "2025-03-18 21:27:44,427 - INFO - Iter 881/10000, Loss: 0.2371, Avg Adaptive LR: 3.295995\n",
      "2025-03-18 21:27:44,828 - INFO - Iter 891/10000, Loss: 0.2369, Avg Adaptive LR: 3.295982\n",
      "2025-03-18 21:27:45,250 - INFO - Iter 901/10000, Loss: 0.2367, Avg Adaptive LR: 3.295969\n",
      "2025-03-18 21:27:45,666 - INFO - Iter 911/10000, Loss: 0.2365, Avg Adaptive LR: 3.295956\n",
      "2025-03-18 21:27:46,063 - INFO - Iter 921/10000, Loss: 0.2363, Avg Adaptive LR: 3.295943\n",
      "2025-03-18 21:27:46,470 - INFO - Iter 931/10000, Loss: 0.2362, Avg Adaptive LR: 3.295930\n",
      "2025-03-18 21:27:46,865 - INFO - Iter 941/10000, Loss: 0.2360, Avg Adaptive LR: 3.295918\n",
      "2025-03-18 21:27:47,269 - INFO - Iter 951/10000, Loss: 0.2358, Avg Adaptive LR: 3.295906\n",
      "2025-03-18 21:27:47,675 - INFO - Iter 961/10000, Loss: 0.2356, Avg Adaptive LR: 3.295894\n",
      "2025-03-18 21:27:48,080 - INFO - Iter 971/10000, Loss: 0.2355, Avg Adaptive LR: 3.295882\n",
      "2025-03-18 21:27:48,495 - INFO - Iter 981/10000, Loss: 0.2353, Avg Adaptive LR: 3.295870\n",
      "2025-03-18 21:27:48,890 - INFO - Iter 991/10000, Loss: 0.2351, Avg Adaptive LR: 3.295858\n",
      "2025-03-18 21:27:49,296 - INFO - Iter 1001/10000, Loss: 0.2350, Avg Adaptive LR: 3.295847\n",
      "2025-03-18 21:27:49,699 - INFO - Iter 1011/10000, Loss: 0.2348, Avg Adaptive LR: 3.295836\n",
      "2025-03-18 21:27:50,103 - INFO - Iter 1021/10000, Loss: 0.2346, Avg Adaptive LR: 3.295825\n",
      "2025-03-18 21:27:50,516 - INFO - Iter 1031/10000, Loss: 0.2345, Avg Adaptive LR: 3.295814\n",
      "2025-03-18 21:27:50,916 - INFO - Iter 1041/10000, Loss: 0.2343, Avg Adaptive LR: 3.295803\n",
      "2025-03-18 21:27:51,311 - INFO - Iter 1051/10000, Loss: 0.2342, Avg Adaptive LR: 3.295792\n",
      "2025-03-18 21:27:51,704 - INFO - Iter 1061/10000, Loss: 0.2340, Avg Adaptive LR: 3.295781\n",
      "2025-03-18 21:27:52,103 - INFO - Iter 1071/10000, Loss: 0.2339, Avg Adaptive LR: 3.295771\n",
      "2025-03-18 21:27:52,516 - INFO - Iter 1081/10000, Loss: 0.2337, Avg Adaptive LR: 3.295760\n",
      "2025-03-18 21:27:52,918 - INFO - Iter 1091/10000, Loss: 0.2336, Avg Adaptive LR: 3.295750\n",
      "2025-03-18 21:27:53,320 - INFO - Iter 1101/10000, Loss: 0.2334, Avg Adaptive LR: 3.295740\n",
      "2025-03-18 21:27:53,715 - INFO - Iter 1111/10000, Loss: 0.2333, Avg Adaptive LR: 3.295730\n",
      "2025-03-18 21:27:54,117 - INFO - Iter 1121/10000, Loss: 0.2331, Avg Adaptive LR: 3.295720\n",
      "2025-03-18 21:27:54,522 - INFO - Iter 1131/10000, Loss: 0.2330, Avg Adaptive LR: 3.295710\n",
      "2025-03-18 21:27:54,920 - INFO - Iter 1141/10000, Loss: 0.2328, Avg Adaptive LR: 3.295701\n",
      "2025-03-18 21:27:55,350 - INFO - Iter 1151/10000, Loss: 0.2327, Avg Adaptive LR: 3.295691\n",
      "2025-03-18 21:27:55,762 - INFO - Iter 1161/10000, Loss: 0.2326, Avg Adaptive LR: 3.295682\n",
      "2025-03-18 21:27:56,174 - INFO - Iter 1171/10000, Loss: 0.2324, Avg Adaptive LR: 3.295672\n",
      "2025-03-18 21:27:56,585 - INFO - Iter 1181/10000, Loss: 0.2323, Avg Adaptive LR: 3.295663\n",
      "2025-03-18 21:27:56,983 - INFO - Iter 1191/10000, Loss: 0.2322, Avg Adaptive LR: 3.295654\n",
      "2025-03-18 21:27:57,402 - INFO - Iter 1201/10000, Loss: 0.2320, Avg Adaptive LR: 3.295645\n",
      "2025-03-18 21:27:57,804 - INFO - Iter 1211/10000, Loss: 0.2319, Avg Adaptive LR: 3.295636\n",
      "2025-03-18 21:27:58,214 - INFO - Iter 1221/10000, Loss: 0.2318, Avg Adaptive LR: 3.295627\n",
      "2025-03-18 21:27:58,631 - INFO - Iter 1231/10000, Loss: 0.2316, Avg Adaptive LR: 3.295618\n",
      "2025-03-18 21:27:59,038 - INFO - Iter 1241/10000, Loss: 0.2315, Avg Adaptive LR: 3.295610\n",
      "2025-03-18 21:27:59,447 - INFO - Iter 1251/10000, Loss: 0.2314, Avg Adaptive LR: 3.295601\n",
      "2025-03-18 21:27:59,847 - INFO - Iter 1261/10000, Loss: 0.2313, Avg Adaptive LR: 3.295593\n",
      "2025-03-18 21:28:00,255 - INFO - Iter 1271/10000, Loss: 0.2312, Avg Adaptive LR: 3.295584\n",
      "2025-03-18 21:28:00,675 - INFO - Iter 1281/10000, Loss: 0.2310, Avg Adaptive LR: 3.295576\n",
      "2025-03-18 21:28:01,081 - INFO - Iter 1291/10000, Loss: 0.2309, Avg Adaptive LR: 3.295568\n",
      "2025-03-18 21:28:01,489 - INFO - Iter 1301/10000, Loss: 0.2308, Avg Adaptive LR: 3.295560\n",
      "2025-03-18 21:28:01,892 - INFO - Iter 1311/10000, Loss: 0.2307, Avg Adaptive LR: 3.295551\n",
      "2025-03-18 21:28:02,300 - INFO - Iter 1321/10000, Loss: 0.2306, Avg Adaptive LR: 3.295543\n",
      "2025-03-18 21:28:02,704 - INFO - Iter 1331/10000, Loss: 0.2304, Avg Adaptive LR: 3.295536\n",
      "2025-03-18 21:28:03,108 - INFO - Iter 1341/10000, Loss: 0.2303, Avg Adaptive LR: 3.295528\n",
      "2025-03-18 21:28:03,539 - INFO - Iter 1351/10000, Loss: 0.2302, Avg Adaptive LR: 3.295520\n",
      "2025-03-18 21:28:03,962 - INFO - Iter 1361/10000, Loss: 0.2301, Avg Adaptive LR: 3.295512\n",
      "2025-03-18 21:28:04,373 - INFO - Iter 1371/10000, Loss: 0.2300, Avg Adaptive LR: 3.295505\n",
      "2025-03-18 21:28:04,771 - INFO - Iter 1381/10000, Loss: 0.2299, Avg Adaptive LR: 3.295497\n",
      "2025-03-18 21:28:05,173 - INFO - Iter 1391/10000, Loss: 0.2298, Avg Adaptive LR: 3.295490\n",
      "2025-03-18 21:28:05,578 - INFO - Iter 1401/10000, Loss: 0.2297, Avg Adaptive LR: 3.295482\n",
      "2025-03-18 21:28:05,983 - INFO - Iter 1411/10000, Loss: 0.2296, Avg Adaptive LR: 3.295475\n",
      "2025-03-18 21:28:06,392 - INFO - Iter 1421/10000, Loss: 0.2295, Avg Adaptive LR: 3.295468\n",
      "2025-03-18 21:28:06,796 - INFO - Iter 1431/10000, Loss: 0.2294, Avg Adaptive LR: 3.295460\n",
      "2025-03-18 21:28:07,189 - INFO - Iter 1441/10000, Loss: 0.2293, Avg Adaptive LR: 3.295453\n",
      "2025-03-18 21:28:07,597 - INFO - Iter 1451/10000, Loss: 0.2292, Avg Adaptive LR: 3.295446\n",
      "2025-03-18 21:28:07,990 - INFO - Iter 1461/10000, Loss: 0.2291, Avg Adaptive LR: 3.295439\n",
      "2025-03-18 21:28:08,399 - INFO - Iter 1471/10000, Loss: 0.2290, Avg Adaptive LR: 3.295432\n",
      "2025-03-18 21:28:08,792 - INFO - Iter 1481/10000, Loss: 0.2289, Avg Adaptive LR: 3.295425\n",
      "2025-03-18 21:28:09,188 - INFO - Iter 1491/10000, Loss: 0.2288, Avg Adaptive LR: 3.295418\n",
      "2025-03-18 21:28:09,586 - INFO - Iter 1501/10000, Loss: 0.2287, Avg Adaptive LR: 3.295412\n",
      "2025-03-18 21:28:09,985 - INFO - Iter 1511/10000, Loss: 0.2286, Avg Adaptive LR: 3.295405\n",
      "2025-03-18 21:28:10,382 - INFO - Iter 1521/10000, Loss: 0.2285, Avg Adaptive LR: 3.295398\n",
      "2025-03-18 21:28:10,774 - INFO - Iter 1531/10000, Loss: 0.2284, Avg Adaptive LR: 3.295392\n",
      "2025-03-18 21:28:11,170 - INFO - Iter 1541/10000, Loss: 0.2283, Avg Adaptive LR: 3.295385\n",
      "2025-03-18 21:28:11,573 - INFO - Iter 1551/10000, Loss: 0.2282, Avg Adaptive LR: 3.295379\n",
      "2025-03-18 21:28:11,966 - INFO - Iter 1561/10000, Loss: 0.2281, Avg Adaptive LR: 3.295372\n",
      "2025-03-18 21:28:12,365 - INFO - Iter 1571/10000, Loss: 0.2280, Avg Adaptive LR: 3.295366\n",
      "2025-03-18 21:28:12,757 - INFO - Iter 1581/10000, Loss: 0.2279, Avg Adaptive LR: 3.295360\n",
      "2025-03-18 21:28:13,151 - INFO - Iter 1591/10000, Loss: 0.2278, Avg Adaptive LR: 3.295353\n",
      "2025-03-18 21:28:13,552 - INFO - Iter 1601/10000, Loss: 0.2277, Avg Adaptive LR: 3.295347\n",
      "2025-03-18 21:28:13,945 - INFO - Iter 1611/10000, Loss: 0.2276, Avg Adaptive LR: 3.295341\n",
      "2025-03-18 21:28:14,366 - INFO - Iter 1621/10000, Loss: 0.2275, Avg Adaptive LR: 3.295335\n",
      "2025-03-18 21:28:14,760 - INFO - Iter 1631/10000, Loss: 0.2275, Avg Adaptive LR: 3.295329\n",
      "2025-03-18 21:28:15,150 - INFO - Iter 1641/10000, Loss: 0.2274, Avg Adaptive LR: 3.295323\n",
      "2025-03-18 21:28:15,547 - INFO - Iter 1651/10000, Loss: 0.2273, Avg Adaptive LR: 3.295317\n",
      "2025-03-18 21:28:15,943 - INFO - Iter 1661/10000, Loss: 0.2272, Avg Adaptive LR: 3.295311\n",
      "2025-03-18 21:28:16,348 - INFO - Iter 1671/10000, Loss: 0.2271, Avg Adaptive LR: 3.295305\n",
      "2025-03-18 21:28:16,739 - INFO - Iter 1681/10000, Loss: 0.2270, Avg Adaptive LR: 3.295299\n",
      "2025-03-18 21:28:17,139 - INFO - Iter 1691/10000, Loss: 0.2269, Avg Adaptive LR: 3.295293\n",
      "2025-03-18 21:28:17,535 - INFO - Iter 1701/10000, Loss: 0.2269, Avg Adaptive LR: 3.295287\n",
      "2025-03-18 21:28:17,932 - INFO - Iter 1711/10000, Loss: 0.2268, Avg Adaptive LR: 3.295282\n",
      "2025-03-18 21:28:18,331 - INFO - Iter 1721/10000, Loss: 0.2267, Avg Adaptive LR: 3.295276\n",
      "2025-03-18 21:28:18,725 - INFO - Iter 1731/10000, Loss: 0.2266, Avg Adaptive LR: 3.295270\n",
      "2025-03-18 21:28:19,123 - INFO - Iter 1741/10000, Loss: 0.2265, Avg Adaptive LR: 3.295265\n",
      "2025-03-18 21:28:19,524 - INFO - Iter 1751/10000, Loss: 0.2265, Avg Adaptive LR: 3.295259\n",
      "2025-03-18 21:28:19,921 - INFO - Iter 1761/10000, Loss: 0.2264, Avg Adaptive LR: 3.295254\n",
      "2025-03-18 21:28:20,326 - INFO - Iter 1771/10000, Loss: 0.2263, Avg Adaptive LR: 3.295248\n",
      "2025-03-18 21:28:20,723 - INFO - Iter 1781/10000, Loss: 0.2262, Avg Adaptive LR: 3.295243\n",
      "2025-03-18 21:28:21,121 - INFO - Iter 1791/10000, Loss: 0.2261, Avg Adaptive LR: 3.295238\n",
      "2025-03-18 21:28:21,522 - INFO - Iter 1801/10000, Loss: 0.2261, Avg Adaptive LR: 3.295232\n",
      "2025-03-18 21:28:21,917 - INFO - Iter 1811/10000, Loss: 0.2260, Avg Adaptive LR: 3.295227\n",
      "2025-03-18 21:28:22,317 - INFO - Iter 1821/10000, Loss: 0.2259, Avg Adaptive LR: 3.295222\n",
      "2025-03-18 21:28:22,713 - INFO - Iter 1831/10000, Loss: 0.2258, Avg Adaptive LR: 3.295216\n",
      "2025-03-18 21:28:23,107 - INFO - Iter 1841/10000, Loss: 0.2258, Avg Adaptive LR: 3.295211\n",
      "2025-03-18 21:28:23,515 - INFO - Iter 1851/10000, Loss: 0.2257, Avg Adaptive LR: 3.295206\n",
      "2025-03-18 21:28:23,905 - INFO - Iter 1861/10000, Loss: 0.2256, Avg Adaptive LR: 3.295201\n",
      "2025-03-18 21:28:24,306 - INFO - Iter 1871/10000, Loss: 0.2255, Avg Adaptive LR: 3.295196\n",
      "2025-03-18 21:28:24,720 - INFO - Iter 1881/10000, Loss: 0.2255, Avg Adaptive LR: 3.295191\n",
      "2025-03-18 21:28:25,118 - INFO - Iter 1891/10000, Loss: 0.2254, Avg Adaptive LR: 3.295186\n",
      "2025-03-18 21:28:25,520 - INFO - Iter 1901/10000, Loss: 0.2253, Avg Adaptive LR: 3.295181\n",
      "2025-03-18 21:28:25,919 - INFO - Iter 1911/10000, Loss: 0.2252, Avg Adaptive LR: 3.295176\n",
      "2025-03-18 21:28:26,341 - INFO - Iter 1921/10000, Loss: 0.2252, Avg Adaptive LR: 3.295171\n",
      "2025-03-18 21:28:26,748 - INFO - Iter 1931/10000, Loss: 0.2251, Avg Adaptive LR: 3.295166\n",
      "2025-03-18 21:28:27,142 - INFO - Iter 1941/10000, Loss: 0.2250, Avg Adaptive LR: 3.295161\n",
      "2025-03-18 21:28:27,548 - INFO - Iter 1951/10000, Loss: 0.2250, Avg Adaptive LR: 3.295157\n",
      "2025-03-18 21:28:27,943 - INFO - Iter 1961/10000, Loss: 0.2249, Avg Adaptive LR: 3.295152\n",
      "2025-03-18 21:28:28,347 - INFO - Iter 1971/10000, Loss: 0.2248, Avg Adaptive LR: 3.295147\n",
      "2025-03-18 21:28:28,737 - INFO - Iter 1981/10000, Loss: 0.2248, Avg Adaptive LR: 3.295142\n",
      "2025-03-18 21:28:29,129 - INFO - Iter 1991/10000, Loss: 0.2247, Avg Adaptive LR: 3.295138\n",
      "2025-03-18 21:28:29,531 - INFO - Iter 2001/10000, Loss: 0.2246, Avg Adaptive LR: 3.295133\n",
      "2025-03-18 21:28:29,923 - INFO - Iter 2011/10000, Loss: 0.2246, Avg Adaptive LR: 3.295128\n",
      "2025-03-18 21:28:30,323 - INFO - Iter 2021/10000, Loss: 0.2245, Avg Adaptive LR: 3.295124\n",
      "2025-03-18 21:28:30,720 - INFO - Iter 2031/10000, Loss: 0.2244, Avg Adaptive LR: 3.295119\n",
      "2025-03-18 21:28:31,114 - INFO - Iter 2041/10000, Loss: 0.2244, Avg Adaptive LR: 3.295115\n",
      "2025-03-18 21:28:31,518 - INFO - Iter 2051/10000, Loss: 0.2243, Avg Adaptive LR: 3.295110\n",
      "2025-03-18 21:28:31,908 - INFO - Iter 2061/10000, Loss: 0.2242, Avg Adaptive LR: 3.295106\n",
      "2025-03-18 21:28:32,308 - INFO - Iter 2071/10000, Loss: 0.2242, Avg Adaptive LR: 3.295101\n",
      "2025-03-18 21:28:32,705 - INFO - Iter 2081/10000, Loss: 0.2241, Avg Adaptive LR: 3.295097\n",
      "2025-03-18 21:28:33,102 - INFO - Iter 2091/10000, Loss: 0.2240, Avg Adaptive LR: 3.295093\n",
      "2025-03-18 21:28:33,500 - INFO - Iter 2101/10000, Loss: 0.2240, Avg Adaptive LR: 3.295088\n",
      "2025-03-18 21:28:33,891 - INFO - Iter 2111/10000, Loss: 0.2239, Avg Adaptive LR: 3.295084\n",
      "2025-03-18 21:28:34,296 - INFO - Iter 2121/10000, Loss: 0.2239, Avg Adaptive LR: 3.295080\n",
      "2025-03-18 21:28:34,686 - INFO - Iter 2131/10000, Loss: 0.2238, Avg Adaptive LR: 3.295075\n",
      "2025-03-18 21:28:35,101 - INFO - Iter 2141/10000, Loss: 0.2237, Avg Adaptive LR: 3.295071\n",
      "2025-03-18 21:28:35,496 - INFO - Iter 2151/10000, Loss: 0.2237, Avg Adaptive LR: 3.295067\n",
      "2025-03-18 21:28:35,893 - INFO - Iter 2161/10000, Loss: 0.2236, Avg Adaptive LR: 3.295063\n",
      "2025-03-18 21:28:36,297 - INFO - Iter 2171/10000, Loss: 0.2235, Avg Adaptive LR: 3.295058\n",
      "2025-03-18 21:28:36,696 - INFO - Iter 2181/10000, Loss: 0.2235, Avg Adaptive LR: 3.295054\n",
      "2025-03-18 21:28:37,090 - INFO - Iter 2191/10000, Loss: 0.2234, Avg Adaptive LR: 3.295050\n",
      "2025-03-18 21:28:37,494 - INFO - Iter 2201/10000, Loss: 0.2234, Avg Adaptive LR: 3.295046\n",
      "2025-03-18 21:28:37,889 - INFO - Iter 2211/10000, Loss: 0.2233, Avg Adaptive LR: 3.295042\n",
      "2025-03-18 21:28:38,287 - INFO - Iter 2221/10000, Loss: 0.2233, Avg Adaptive LR: 3.295038\n",
      "2025-03-18 21:28:38,682 - INFO - Iter 2231/10000, Loss: 0.2232, Avg Adaptive LR: 3.295034\n",
      "2025-03-18 21:28:39,071 - INFO - Iter 2241/10000, Loss: 0.2231, Avg Adaptive LR: 3.295030\n",
      "2025-03-18 21:28:39,469 - INFO - Iter 2251/10000, Loss: 0.2231, Avg Adaptive LR: 3.295026\n",
      "2025-03-18 21:28:39,866 - INFO - Iter 2261/10000, Loss: 0.2230, Avg Adaptive LR: 3.295022\n",
      "2025-03-18 21:28:40,263 - INFO - Iter 2271/10000, Loss: 0.2230, Avg Adaptive LR: 3.295018\n",
      "2025-03-18 21:28:40,654 - INFO - Iter 2281/10000, Loss: 0.2229, Avg Adaptive LR: 3.295014\n",
      "2025-03-18 21:28:41,045 - INFO - Iter 2291/10000, Loss: 0.2229, Avg Adaptive LR: 3.295010\n",
      "2025-03-18 21:28:41,448 - INFO - Iter 2301/10000, Loss: 0.2228, Avg Adaptive LR: 3.295007\n",
      "2025-03-18 21:28:41,847 - INFO - Iter 2311/10000, Loss: 0.2227, Avg Adaptive LR: 3.295003\n",
      "2025-03-18 21:28:42,250 - INFO - Iter 2321/10000, Loss: 0.2227, Avg Adaptive LR: 3.294999\n",
      "2025-03-18 21:28:42,645 - INFO - Iter 2331/10000, Loss: 0.2226, Avg Adaptive LR: 3.294995\n",
      "2025-03-18 21:28:43,042 - INFO - Iter 2341/10000, Loss: 0.2226, Avg Adaptive LR: 3.294991\n",
      "2025-03-18 21:28:43,446 - INFO - Iter 2351/10000, Loss: 0.2225, Avg Adaptive LR: 3.294988\n",
      "2025-03-18 21:28:43,842 - INFO - Iter 2361/10000, Loss: 0.2225, Avg Adaptive LR: 3.294984\n",
      "2025-03-18 21:28:44,243 - INFO - Iter 2371/10000, Loss: 0.2224, Avg Adaptive LR: 3.294980\n",
      "2025-03-18 21:28:44,643 - INFO - Iter 2381/10000, Loss: 0.2224, Avg Adaptive LR: 3.294976\n",
      "2025-03-18 21:28:45,038 - INFO - Iter 2391/10000, Loss: 0.2223, Avg Adaptive LR: 3.294973\n",
      "2025-03-18 21:28:45,460 - INFO - Iter 2401/10000, Loss: 0.2223, Avg Adaptive LR: 3.294969\n",
      "2025-03-18 21:28:45,852 - INFO - Iter 2411/10000, Loss: 0.2222, Avg Adaptive LR: 3.294966\n",
      "2025-03-18 21:28:46,250 - INFO - Iter 2421/10000, Loss: 0.2222, Avg Adaptive LR: 3.294962\n",
      "2025-03-18 21:28:46,662 - INFO - Iter 2431/10000, Loss: 0.2221, Avg Adaptive LR: 3.294958\n",
      "2025-03-18 21:28:47,051 - INFO - Iter 2441/10000, Loss: 0.2220, Avg Adaptive LR: 3.294955\n",
      "2025-03-18 21:28:47,451 - INFO - Iter 2451/10000, Loss: 0.2220, Avg Adaptive LR: 3.294951\n",
      "2025-03-18 21:28:47,847 - INFO - Iter 2461/10000, Loss: 0.2219, Avg Adaptive LR: 3.294948\n",
      "2025-03-18 21:28:48,244 - INFO - Iter 2471/10000, Loss: 0.2219, Avg Adaptive LR: 3.294944\n",
      "2025-03-18 21:28:48,639 - INFO - Iter 2481/10000, Loss: 0.2218, Avg Adaptive LR: 3.294941\n",
      "2025-03-18 21:28:49,030 - INFO - Iter 2491/10000, Loss: 0.2218, Avg Adaptive LR: 3.294937\n",
      "2025-03-18 21:28:49,432 - INFO - Iter 2501/10000, Loss: 0.2217, Avg Adaptive LR: 3.294934\n",
      "2025-03-18 21:28:49,827 - INFO - Iter 2511/10000, Loss: 0.2217, Avg Adaptive LR: 3.294930\n",
      "2025-03-18 21:28:50,221 - INFO - Iter 2521/10000, Loss: 0.2216, Avg Adaptive LR: 3.294927\n",
      "2025-03-18 21:28:50,625 - INFO - Iter 2531/10000, Loss: 0.2216, Avg Adaptive LR: 3.294924\n",
      "2025-03-18 21:28:51,018 - INFO - Iter 2541/10000, Loss: 0.2215, Avg Adaptive LR: 3.294920\n",
      "2025-03-18 21:28:51,425 - INFO - Iter 2551/10000, Loss: 0.2215, Avg Adaptive LR: 3.294917\n",
      "2025-03-18 21:28:51,823 - INFO - Iter 2561/10000, Loss: 0.2215, Avg Adaptive LR: 3.294914\n",
      "2025-03-18 21:28:52,218 - INFO - Iter 2571/10000, Loss: 0.2214, Avg Adaptive LR: 3.294910\n",
      "2025-03-18 21:28:52,618 - INFO - Iter 2581/10000, Loss: 0.2214, Avg Adaptive LR: 3.294907\n",
      "2025-03-18 21:28:53,010 - INFO - Iter 2591/10000, Loss: 0.2213, Avg Adaptive LR: 3.294904\n",
      "2025-03-18 21:28:53,410 - INFO - Iter 2601/10000, Loss: 0.2213, Avg Adaptive LR: 3.294900\n",
      "2025-03-18 21:28:53,807 - INFO - Iter 2611/10000, Loss: 0.2212, Avg Adaptive LR: 3.294897\n",
      "2025-03-18 21:28:54,202 - INFO - Iter 2621/10000, Loss: 0.2212, Avg Adaptive LR: 3.294894\n",
      "2025-03-18 21:28:54,601 - INFO - Iter 2631/10000, Loss: 0.2211, Avg Adaptive LR: 3.294891\n",
      "2025-03-18 21:28:55,001 - INFO - Iter 2641/10000, Loss: 0.2211, Avg Adaptive LR: 3.294887\n",
      "2025-03-18 21:28:55,402 - INFO - Iter 2651/10000, Loss: 0.2210, Avg Adaptive LR: 3.294884\n",
      "2025-03-18 21:28:55,814 - INFO - Iter 2661/10000, Loss: 0.2210, Avg Adaptive LR: 3.294881\n",
      "2025-03-18 21:28:56,207 - INFO - Iter 2671/10000, Loss: 0.2209, Avg Adaptive LR: 3.294878\n",
      "2025-03-18 21:28:56,612 - INFO - Iter 2681/10000, Loss: 0.2209, Avg Adaptive LR: 3.294875\n",
      "2025-03-18 21:28:57,017 - INFO - Iter 2691/10000, Loss: 0.2208, Avg Adaptive LR: 3.294872\n",
      "2025-03-18 21:28:57,416 - INFO - Iter 2701/10000, Loss: 0.2208, Avg Adaptive LR: 3.294869\n",
      "2025-03-18 21:28:57,815 - INFO - Iter 2711/10000, Loss: 0.2208, Avg Adaptive LR: 3.294866\n",
      "2025-03-18 21:28:58,212 - INFO - Iter 2721/10000, Loss: 0.2207, Avg Adaptive LR: 3.294862\n",
      "2025-03-18 21:28:58,618 - INFO - Iter 2731/10000, Loss: 0.2207, Avg Adaptive LR: 3.294859\n",
      "2025-03-18 21:28:59,008 - INFO - Iter 2741/10000, Loss: 0.2206, Avg Adaptive LR: 3.294856\n",
      "2025-03-18 21:28:59,405 - INFO - Iter 2751/10000, Loss: 0.2206, Avg Adaptive LR: 3.294853\n",
      "2025-03-18 21:28:59,800 - INFO - Iter 2761/10000, Loss: 0.2205, Avg Adaptive LR: 3.294850\n",
      "2025-03-18 21:29:00,196 - INFO - Iter 2771/10000, Loss: 0.2205, Avg Adaptive LR: 3.294847\n",
      "2025-03-18 21:29:00,601 - INFO - Iter 2781/10000, Loss: 0.2204, Avg Adaptive LR: 3.294844\n",
      "2025-03-18 21:29:00,995 - INFO - Iter 2791/10000, Loss: 0.2204, Avg Adaptive LR: 3.294841\n",
      "2025-03-18 21:29:01,398 - INFO - Iter 2801/10000, Loss: 0.2204, Avg Adaptive LR: 3.294838\n",
      "2025-03-18 21:29:01,796 - INFO - Iter 2811/10000, Loss: 0.2203, Avg Adaptive LR: 3.294835\n",
      "2025-03-18 21:29:02,198 - INFO - Iter 2821/10000, Loss: 0.2203, Avg Adaptive LR: 3.294833\n",
      "2025-03-18 21:29:02,602 - INFO - Iter 2831/10000, Loss: 0.2202, Avg Adaptive LR: 3.294830\n",
      "2025-03-18 21:29:02,998 - INFO - Iter 2841/10000, Loss: 0.2202, Avg Adaptive LR: 3.294827\n",
      "2025-03-18 21:29:03,395 - INFO - Iter 2851/10000, Loss: 0.2202, Avg Adaptive LR: 3.294824\n",
      "2025-03-18 21:29:03,793 - INFO - Iter 2861/10000, Loss: 0.2201, Avg Adaptive LR: 3.294821\n",
      "2025-03-18 21:29:04,184 - INFO - Iter 2871/10000, Loss: 0.2201, Avg Adaptive LR: 3.294818\n",
      "2025-03-18 21:29:04,583 - INFO - Iter 2881/10000, Loss: 0.2200, Avg Adaptive LR: 3.294815\n",
      "2025-03-18 21:29:04,980 - INFO - Iter 2891/10000, Loss: 0.2200, Avg Adaptive LR: 3.294812\n",
      "2025-03-18 21:29:05,382 - INFO - Iter 2901/10000, Loss: 0.2199, Avg Adaptive LR: 3.294810\n",
      "2025-03-18 21:29:05,799 - INFO - Iter 2911/10000, Loss: 0.2199, Avg Adaptive LR: 3.294807\n",
      "2025-03-18 21:29:06,202 - INFO - Iter 2921/10000, Loss: 0.2199, Avg Adaptive LR: 3.294804\n",
      "2025-03-18 21:29:06,620 - INFO - Iter 2931/10000, Loss: 0.2198, Avg Adaptive LR: 3.294801\n",
      "2025-03-18 21:29:07,035 - INFO - Iter 2941/10000, Loss: 0.2198, Avg Adaptive LR: 3.294799\n",
      "2025-03-18 21:29:07,440 - INFO - Iter 2951/10000, Loss: 0.2197, Avg Adaptive LR: 3.294796\n",
      "2025-03-18 21:29:07,833 - INFO - Iter 2961/10000, Loss: 0.2197, Avg Adaptive LR: 3.294793\n",
      "2025-03-18 21:29:08,227 - INFO - Iter 2971/10000, Loss: 0.2197, Avg Adaptive LR: 3.294790\n",
      "2025-03-18 21:29:08,632 - INFO - Iter 2981/10000, Loss: 0.2196, Avg Adaptive LR: 3.294788\n",
      "2025-03-18 21:29:09,036 - INFO - Iter 2991/10000, Loss: 0.2196, Avg Adaptive LR: 3.294785\n",
      "2025-03-18 21:29:09,438 - INFO - Iter 3001/10000, Loss: 0.2196, Avg Adaptive LR: 3.294782\n",
      "2025-03-18 21:29:09,832 - INFO - Iter 3011/10000, Loss: 0.2195, Avg Adaptive LR: 3.294780\n",
      "2025-03-18 21:29:10,223 - INFO - Iter 3021/10000, Loss: 0.2195, Avg Adaptive LR: 3.294777\n",
      "2025-03-18 21:29:10,628 - INFO - Iter 3031/10000, Loss: 0.2194, Avg Adaptive LR: 3.294774\n",
      "2025-03-18 21:29:11,019 - INFO - Iter 3041/10000, Loss: 0.2194, Avg Adaptive LR: 3.294772\n",
      "2025-03-18 21:29:11,422 - INFO - Iter 3051/10000, Loss: 0.2194, Avg Adaptive LR: 3.294769\n",
      "2025-03-18 21:29:11,820 - INFO - Iter 3061/10000, Loss: 0.2193, Avg Adaptive LR: 3.294766\n",
      "2025-03-18 21:29:12,213 - INFO - Iter 3071/10000, Loss: 0.2193, Avg Adaptive LR: 3.294764\n",
      "2025-03-18 21:29:12,624 - INFO - Iter 3081/10000, Loss: 0.2192, Avg Adaptive LR: 3.294761\n",
      "2025-03-18 21:29:13,015 - INFO - Iter 3091/10000, Loss: 0.2192, Avg Adaptive LR: 3.294759\n",
      "2025-03-18 21:29:13,415 - INFO - Iter 3101/10000, Loss: 0.2192, Avg Adaptive LR: 3.294756\n",
      "2025-03-18 21:29:13,811 - INFO - Iter 3111/10000, Loss: 0.2191, Avg Adaptive LR: 3.294753\n",
      "2025-03-18 21:29:14,209 - INFO - Iter 3121/10000, Loss: 0.2191, Avg Adaptive LR: 3.294751\n",
      "2025-03-18 21:29:14,613 - INFO - Iter 3131/10000, Loss: 0.2191, Avg Adaptive LR: 3.294748\n",
      "2025-03-18 21:29:15,008 - INFO - Iter 3141/10000, Loss: 0.2190, Avg Adaptive LR: 3.294746\n",
      "2025-03-18 21:29:15,408 - INFO - Iter 3151/10000, Loss: 0.2190, Avg Adaptive LR: 3.294743\n",
      "2025-03-18 21:29:15,802 - INFO - Iter 3161/10000, Loss: 0.2190, Avg Adaptive LR: 3.294741\n",
      "2025-03-18 21:29:16,216 - INFO - Iter 3171/10000, Loss: 0.2189, Avg Adaptive LR: 3.294738\n",
      "2025-03-18 21:29:16,617 - INFO - Iter 3181/10000, Loss: 0.2189, Avg Adaptive LR: 3.294736\n",
      "2025-03-18 21:29:17,021 - INFO - Iter 3191/10000, Loss: 0.2188, Avg Adaptive LR: 3.294733\n",
      "2025-03-18 21:29:17,422 - INFO - Iter 3201/10000, Loss: 0.2188, Avg Adaptive LR: 3.294731\n",
      "2025-03-18 21:29:17,812 - INFO - Iter 3211/10000, Loss: 0.2188, Avg Adaptive LR: 3.294729\n",
      "2025-03-18 21:29:18,210 - INFO - Iter 3221/10000, Loss: 0.2187, Avg Adaptive LR: 3.294726\n",
      "2025-03-18 21:29:18,612 - INFO - Iter 3231/10000, Loss: 0.2187, Avg Adaptive LR: 3.294724\n",
      "2025-03-18 21:29:19,002 - INFO - Iter 3241/10000, Loss: 0.2187, Avg Adaptive LR: 3.294721\n",
      "2025-03-18 21:29:19,405 - INFO - Iter 3251/10000, Loss: 0.2186, Avg Adaptive LR: 3.294719\n",
      "2025-03-18 21:29:19,802 - INFO - Iter 3261/10000, Loss: 0.2186, Avg Adaptive LR: 3.294716\n",
      "2025-03-18 21:29:20,193 - INFO - Iter 3271/10000, Loss: 0.2186, Avg Adaptive LR: 3.294714\n",
      "2025-03-18 21:29:20,596 - INFO - Iter 3281/10000, Loss: 0.2185, Avg Adaptive LR: 3.294712\n",
      "2025-03-18 21:29:20,990 - INFO - Iter 3291/10000, Loss: 0.2185, Avg Adaptive LR: 3.294709\n",
      "2025-03-18 21:29:21,396 - INFO - Iter 3301/10000, Loss: 0.2185, Avg Adaptive LR: 3.294707\n",
      "2025-03-18 21:29:21,788 - INFO - Iter 3311/10000, Loss: 0.2184, Avg Adaptive LR: 3.294705\n",
      "2025-03-18 21:29:22,180 - INFO - Iter 3321/10000, Loss: 0.2184, Avg Adaptive LR: 3.294702\n",
      "2025-03-18 21:29:22,579 - INFO - Iter 3331/10000, Loss: 0.2184, Avg Adaptive LR: 3.294700\n",
      "2025-03-18 21:29:22,973 - INFO - Iter 3341/10000, Loss: 0.2183, Avg Adaptive LR: 3.294698\n",
      "2025-03-18 21:29:23,369 - INFO - Iter 3351/10000, Loss: 0.2183, Avg Adaptive LR: 3.294695\n",
      "2025-03-18 21:29:23,763 - INFO - Iter 3361/10000, Loss: 0.2183, Avg Adaptive LR: 3.294693\n",
      "2025-03-18 21:29:24,155 - INFO - Iter 3371/10000, Loss: 0.2182, Avg Adaptive LR: 3.294691\n",
      "2025-03-18 21:29:24,557 - INFO - Iter 3381/10000, Loss: 0.2182, Avg Adaptive LR: 3.294689\n",
      "2025-03-18 21:29:24,949 - INFO - Iter 3391/10000, Loss: 0.2182, Avg Adaptive LR: 3.294686\n",
      "2025-03-18 21:29:25,351 - INFO - Iter 3401/10000, Loss: 0.2181, Avg Adaptive LR: 3.294684\n",
      "2025-03-18 21:29:25,744 - INFO - Iter 3411/10000, Loss: 0.2181, Avg Adaptive LR: 3.294682\n",
      "2025-03-18 21:29:26,137 - INFO - Iter 3421/10000, Loss: 0.2181, Avg Adaptive LR: 3.294680\n",
      "2025-03-18 21:29:26,556 - INFO - Iter 3431/10000, Loss: 0.2180, Avg Adaptive LR: 3.294677\n",
      "2025-03-18 21:29:26,945 - INFO - Iter 3441/10000, Loss: 0.2180, Avg Adaptive LR: 3.294675\n",
      "2025-03-18 21:29:27,356 - INFO - Iter 3451/10000, Loss: 0.2180, Avg Adaptive LR: 3.294673\n",
      "2025-03-18 21:29:27,761 - INFO - Iter 3461/10000, Loss: 0.2179, Avg Adaptive LR: 3.294671\n",
      "2025-03-18 21:29:28,159 - INFO - Iter 3471/10000, Loss: 0.2179, Avg Adaptive LR: 3.294668\n",
      "2025-03-18 21:29:28,563 - INFO - Iter 3481/10000, Loss: 0.2179, Avg Adaptive LR: 3.294666\n",
      "2025-03-18 21:29:28,955 - INFO - Iter 3491/10000, Loss: 0.2178, Avg Adaptive LR: 3.294664\n",
      "2025-03-18 21:29:29,362 - INFO - Iter 3501/10000, Loss: 0.2178, Avg Adaptive LR: 3.294662\n",
      "2025-03-18 21:29:29,750 - INFO - Iter 3511/10000, Loss: 0.2178, Avg Adaptive LR: 3.294660\n",
      "2025-03-18 21:29:30,146 - INFO - Iter 3521/10000, Loss: 0.2177, Avg Adaptive LR: 3.294658\n",
      "2025-03-18 21:29:30,550 - INFO - Iter 3531/10000, Loss: 0.2177, Avg Adaptive LR: 3.294655\n",
      "2025-03-18 21:29:30,944 - INFO - Iter 3541/10000, Loss: 0.2177, Avg Adaptive LR: 3.294653\n",
      "2025-03-18 21:29:31,351 - INFO - Iter 3551/10000, Loss: 0.2177, Avg Adaptive LR: 3.294651\n",
      "2025-03-18 21:29:31,744 - INFO - Iter 3561/10000, Loss: 0.2176, Avg Adaptive LR: 3.294649\n",
      "2025-03-18 21:29:32,137 - INFO - Iter 3571/10000, Loss: 0.2176, Avg Adaptive LR: 3.294647\n",
      "2025-03-18 21:29:32,543 - INFO - Iter 3581/10000, Loss: 0.2176, Avg Adaptive LR: 3.294645\n",
      "2025-03-18 21:29:32,938 - INFO - Iter 3591/10000, Loss: 0.2175, Avg Adaptive LR: 3.294643\n",
      "2025-03-18 21:29:33,336 - INFO - Iter 3601/10000, Loss: 0.2175, Avg Adaptive LR: 3.294641\n",
      "2025-03-18 21:29:33,730 - INFO - Iter 3611/10000, Loss: 0.2175, Avg Adaptive LR: 3.294639\n",
      "2025-03-18 21:29:34,126 - INFO - Iter 3621/10000, Loss: 0.2174, Avg Adaptive LR: 3.294637\n",
      "2025-03-18 21:29:34,526 - INFO - Iter 3631/10000, Loss: 0.2174, Avg Adaptive LR: 3.294635\n",
      "2025-03-18 21:29:34,917 - INFO - Iter 3641/10000, Loss: 0.2174, Avg Adaptive LR: 3.294632\n",
      "2025-03-18 21:29:35,318 - INFO - Iter 3651/10000, Loss: 0.2174, Avg Adaptive LR: 3.294630\n",
      "2025-03-18 21:29:35,713 - INFO - Iter 3661/10000, Loss: 0.2173, Avg Adaptive LR: 3.294628\n",
      "2025-03-18 21:29:36,106 - INFO - Iter 3671/10000, Loss: 0.2173, Avg Adaptive LR: 3.294626\n",
      "2025-03-18 21:29:36,512 - INFO - Iter 3681/10000, Loss: 0.2173, Avg Adaptive LR: 3.294624\n",
      "2025-03-18 21:29:36,918 - INFO - Iter 3691/10000, Loss: 0.2172, Avg Adaptive LR: 3.294622\n",
      "2025-03-18 21:29:37,333 - INFO - Iter 3701/10000, Loss: 0.2172, Avg Adaptive LR: 3.294620\n",
      "2025-03-18 21:29:37,728 - INFO - Iter 3711/10000, Loss: 0.2172, Avg Adaptive LR: 3.294618\n",
      "2025-03-18 21:29:38,125 - INFO - Iter 3721/10000, Loss: 0.2172, Avg Adaptive LR: 3.294616\n",
      "2025-03-18 21:29:38,526 - INFO - Iter 3731/10000, Loss: 0.2171, Avg Adaptive LR: 3.294614\n",
      "2025-03-18 21:29:38,922 - INFO - Iter 3741/10000, Loss: 0.2171, Avg Adaptive LR: 3.294612\n",
      "2025-03-18 21:29:39,332 - INFO - Iter 3751/10000, Loss: 0.2171, Avg Adaptive LR: 3.294610\n",
      "2025-03-18 21:29:39,729 - INFO - Iter 3761/10000, Loss: 0.2170, Avg Adaptive LR: 3.294608\n",
      "2025-03-18 21:29:40,124 - INFO - Iter 3771/10000, Loss: 0.2170, Avg Adaptive LR: 3.294607\n",
      "2025-03-18 21:29:40,526 - INFO - Iter 3781/10000, Loss: 0.2170, Avg Adaptive LR: 3.294605\n",
      "2025-03-18 21:29:40,918 - INFO - Iter 3791/10000, Loss: 0.2170, Avg Adaptive LR: 3.294603\n",
      "2025-03-18 21:29:41,324 - INFO - Iter 3801/10000, Loss: 0.2169, Avg Adaptive LR: 3.294601\n",
      "2025-03-18 21:29:41,716 - INFO - Iter 3811/10000, Loss: 0.2169, Avg Adaptive LR: 3.294599\n",
      "2025-03-18 21:29:42,116 - INFO - Iter 3821/10000, Loss: 0.2169, Avg Adaptive LR: 3.294597\n",
      "2025-03-18 21:29:42,514 - INFO - Iter 3831/10000, Loss: 0.2168, Avg Adaptive LR: 3.294595\n",
      "2025-03-18 21:29:42,918 - INFO - Iter 3841/10000, Loss: 0.2168, Avg Adaptive LR: 3.294593\n",
      "2025-03-18 21:29:43,317 - INFO - Iter 3851/10000, Loss: 0.2168, Avg Adaptive LR: 3.294591\n",
      "2025-03-18 21:29:43,709 - INFO - Iter 3861/10000, Loss: 0.2168, Avg Adaptive LR: 3.294589\n",
      "2025-03-18 21:29:44,103 - INFO - Iter 3871/10000, Loss: 0.2167, Avg Adaptive LR: 3.294587\n",
      "2025-03-18 21:29:44,509 - INFO - Iter 3881/10000, Loss: 0.2167, Avg Adaptive LR: 3.294586\n",
      "2025-03-18 21:29:44,903 - INFO - Iter 3891/10000, Loss: 0.2167, Avg Adaptive LR: 3.294584\n",
      "2025-03-18 21:29:45,303 - INFO - Iter 3901/10000, Loss: 0.2167, Avg Adaptive LR: 3.294582\n",
      "2025-03-18 21:29:45,693 - INFO - Iter 3911/10000, Loss: 0.2166, Avg Adaptive LR: 3.294580\n",
      "2025-03-18 21:29:46,086 - INFO - Iter 3921/10000, Loss: 0.2166, Avg Adaptive LR: 3.294578\n",
      "2025-03-18 21:29:46,494 - INFO - Iter 3931/10000, Loss: 0.2166, Avg Adaptive LR: 3.294576\n",
      "2025-03-18 21:29:46,883 - INFO - Iter 3941/10000, Loss: 0.2165, Avg Adaptive LR: 3.294574\n",
      "2025-03-18 21:29:47,302 - INFO - Iter 3951/10000, Loss: 0.2165, Avg Adaptive LR: 3.294573\n",
      "2025-03-18 21:29:47,704 - INFO - Iter 3961/10000, Loss: 0.2165, Avg Adaptive LR: 3.294571\n",
      "2025-03-18 21:29:48,100 - INFO - Iter 3971/10000, Loss: 0.2165, Avg Adaptive LR: 3.294569\n",
      "2025-03-18 21:29:48,507 - INFO - Iter 3981/10000, Loss: 0.2164, Avg Adaptive LR: 3.294567\n",
      "2025-03-18 21:29:48,907 - INFO - Iter 3991/10000, Loss: 0.2164, Avg Adaptive LR: 3.294565\n",
      "2025-03-18 21:29:49,308 - INFO - Iter 4001/10000, Loss: 0.2164, Avg Adaptive LR: 3.294564\n",
      "2025-03-18 21:29:49,702 - INFO - Iter 4011/10000, Loss: 0.2164, Avg Adaptive LR: 3.294562\n",
      "2025-03-18 21:29:50,095 - INFO - Iter 4021/10000, Loss: 0.2163, Avg Adaptive LR: 3.294560\n",
      "2025-03-18 21:29:50,498 - INFO - Iter 4031/10000, Loss: 0.2163, Avg Adaptive LR: 3.294558\n",
      "2025-03-18 21:29:50,892 - INFO - Iter 4041/10000, Loss: 0.2163, Avg Adaptive LR: 3.294556\n",
      "2025-03-18 21:29:51,296 - INFO - Iter 4051/10000, Loss: 0.2163, Avg Adaptive LR: 3.294555\n",
      "2025-03-18 21:29:51,688 - INFO - Iter 4061/10000, Loss: 0.2162, Avg Adaptive LR: 3.294553\n",
      "2025-03-18 21:29:52,084 - INFO - Iter 4071/10000, Loss: 0.2162, Avg Adaptive LR: 3.294551\n",
      "2025-03-18 21:29:52,484 - INFO - Iter 4081/10000, Loss: 0.2162, Avg Adaptive LR: 3.294549\n",
      "2025-03-18 21:29:52,880 - INFO - Iter 4091/10000, Loss: 0.2162, Avg Adaptive LR: 3.294548\n",
      "2025-03-18 21:29:53,278 - INFO - Iter 4101/10000, Loss: 0.2161, Avg Adaptive LR: 3.294546\n",
      "2025-03-18 21:29:53,671 - INFO - Iter 4111/10000, Loss: 0.2161, Avg Adaptive LR: 3.294544\n",
      "2025-03-18 21:29:54,065 - INFO - Iter 4121/10000, Loss: 0.2161, Avg Adaptive LR: 3.294542\n",
      "2025-03-18 21:29:54,466 - INFO - Iter 4131/10000, Loss: 0.2161, Avg Adaptive LR: 3.294541\n",
      "2025-03-18 21:29:54,861 - INFO - Iter 4141/10000, Loss: 0.2160, Avg Adaptive LR: 3.294539\n",
      "2025-03-18 21:29:55,254 - INFO - Iter 4151/10000, Loss: 0.2160, Avg Adaptive LR: 3.294537\n",
      "2025-03-18 21:29:55,650 - INFO - Iter 4161/10000, Loss: 0.2160, Avg Adaptive LR: 3.294536\n",
      "2025-03-18 21:29:56,045 - INFO - Iter 4171/10000, Loss: 0.2160, Avg Adaptive LR: 3.294534\n",
      "2025-03-18 21:29:56,446 - INFO - Iter 4181/10000, Loss: 0.2159, Avg Adaptive LR: 3.294532\n",
      "2025-03-18 21:29:56,842 - INFO - Iter 4191/10000, Loss: 0.2159, Avg Adaptive LR: 3.294531\n",
      "2025-03-18 21:29:57,236 - INFO - Iter 4201/10000, Loss: 0.2159, Avg Adaptive LR: 3.294529\n",
      "2025-03-18 21:29:57,671 - INFO - Iter 4211/10000, Loss: 0.2159, Avg Adaptive LR: 3.294527\n",
      "2025-03-18 21:29:58,083 - INFO - Iter 4221/10000, Loss: 0.2158, Avg Adaptive LR: 3.294526\n",
      "2025-03-18 21:29:58,487 - INFO - Iter 4231/10000, Loss: 0.2158, Avg Adaptive LR: 3.294524\n",
      "2025-03-18 21:29:58,878 - INFO - Iter 4241/10000, Loss: 0.2158, Avg Adaptive LR: 3.294522\n",
      "2025-03-18 21:29:59,284 - INFO - Iter 4251/10000, Loss: 0.2158, Avg Adaptive LR: 3.294521\n",
      "2025-03-18 21:29:59,682 - INFO - Iter 4261/10000, Loss: 0.2157, Avg Adaptive LR: 3.294519\n",
      "2025-03-18 21:30:00,080 - INFO - Iter 4271/10000, Loss: 0.2157, Avg Adaptive LR: 3.294517\n",
      "2025-03-18 21:30:00,482 - INFO - Iter 4281/10000, Loss: 0.2157, Avg Adaptive LR: 3.294516\n",
      "2025-03-18 21:30:00,877 - INFO - Iter 4291/10000, Loss: 0.2157, Avg Adaptive LR: 3.294514\n",
      "2025-03-18 21:30:01,275 - INFO - Iter 4301/10000, Loss: 0.2156, Avg Adaptive LR: 3.294512\n",
      "2025-03-18 21:30:01,667 - INFO - Iter 4311/10000, Loss: 0.2156, Avg Adaptive LR: 3.294511\n",
      "2025-03-18 21:30:02,061 - INFO - Iter 4321/10000, Loss: 0.2156, Avg Adaptive LR: 3.294509\n",
      "2025-03-18 21:30:02,460 - INFO - Iter 4331/10000, Loss: 0.2156, Avg Adaptive LR: 3.294508\n",
      "2025-03-18 21:30:02,850 - INFO - Iter 4341/10000, Loss: 0.2156, Avg Adaptive LR: 3.294506\n",
      "2025-03-18 21:30:03,251 - INFO - Iter 4351/10000, Loss: 0.2155, Avg Adaptive LR: 3.294504\n",
      "2025-03-18 21:30:03,645 - INFO - Iter 4361/10000, Loss: 0.2155, Avg Adaptive LR: 3.294503\n",
      "2025-03-18 21:30:04,036 - INFO - Iter 4371/10000, Loss: 0.2155, Avg Adaptive LR: 3.294501\n",
      "2025-03-18 21:30:04,438 - INFO - Iter 4381/10000, Loss: 0.2155, Avg Adaptive LR: 3.294500\n",
      "2025-03-18 21:30:04,833 - INFO - Iter 4391/10000, Loss: 0.2154, Avg Adaptive LR: 3.294498\n",
      "2025-03-18 21:30:05,227 - INFO - Iter 4401/10000, Loss: 0.2154, Avg Adaptive LR: 3.294497\n",
      "2025-03-18 21:30:05,626 - INFO - Iter 4411/10000, Loss: 0.2154, Avg Adaptive LR: 3.294495\n",
      "2025-03-18 21:30:06,017 - INFO - Iter 4421/10000, Loss: 0.2154, Avg Adaptive LR: 3.294493\n",
      "2025-03-18 21:30:06,418 - INFO - Iter 4431/10000, Loss: 0.2154, Avg Adaptive LR: 3.294492\n",
      "2025-03-18 21:30:06,810 - INFO - Iter 4441/10000, Loss: 0.2153, Avg Adaptive LR: 3.294490\n",
      "2025-03-18 21:30:07,206 - INFO - Iter 4451/10000, Loss: 0.2153, Avg Adaptive LR: 3.294489\n",
      "2025-03-18 21:30:07,610 - INFO - Iter 4461/10000, Loss: 0.2153, Avg Adaptive LR: 3.294487\n",
      "2025-03-18 21:30:08,034 - INFO - Iter 4471/10000, Loss: 0.2153, Avg Adaptive LR: 3.294486\n",
      "2025-03-18 21:30:08,434 - INFO - Iter 4481/10000, Loss: 0.2152, Avg Adaptive LR: 3.294484\n",
      "2025-03-18 21:30:08,832 - INFO - Iter 4491/10000, Loss: 0.2152, Avg Adaptive LR: 3.294483\n",
      "2025-03-18 21:30:09,227 - INFO - Iter 4501/10000, Loss: 0.2152, Avg Adaptive LR: 3.294481\n",
      "2025-03-18 21:30:09,626 - INFO - Iter 4511/10000, Loss: 0.2152, Avg Adaptive LR: 3.294480\n",
      "2025-03-18 21:30:10,019 - INFO - Iter 4521/10000, Loss: 0.2152, Avg Adaptive LR: 3.294478\n",
      "2025-03-18 21:30:10,428 - INFO - Iter 4531/10000, Loss: 0.2151, Avg Adaptive LR: 3.294477\n",
      "2025-03-18 21:30:10,825 - INFO - Iter 4541/10000, Loss: 0.2151, Avg Adaptive LR: 3.294475\n",
      "2025-03-18 21:30:11,221 - INFO - Iter 4551/10000, Loss: 0.2151, Avg Adaptive LR: 3.294474\n",
      "2025-03-18 21:30:11,628 - INFO - Iter 4561/10000, Loss: 0.2151, Avg Adaptive LR: 3.294472\n",
      "2025-03-18 21:30:12,019 - INFO - Iter 4571/10000, Loss: 0.2150, Avg Adaptive LR: 3.294471\n",
      "2025-03-18 21:30:12,427 - INFO - Iter 4581/10000, Loss: 0.2150, Avg Adaptive LR: 3.294469\n",
      "2025-03-18 21:30:12,820 - INFO - Iter 4591/10000, Loss: 0.2150, Avg Adaptive LR: 3.294468\n",
      "2025-03-18 21:30:13,218 - INFO - Iter 4601/10000, Loss: 0.2150, Avg Adaptive LR: 3.294466\n",
      "2025-03-18 21:30:13,616 - INFO - Iter 4611/10000, Loss: 0.2150, Avg Adaptive LR: 3.294465\n",
      "2025-03-18 21:30:14,013 - INFO - Iter 4621/10000, Loss: 0.2149, Avg Adaptive LR: 3.294463\n",
      "2025-03-18 21:30:14,419 - INFO - Iter 4631/10000, Loss: 0.2149, Avg Adaptive LR: 3.294462\n",
      "2025-03-18 21:30:14,814 - INFO - Iter 4641/10000, Loss: 0.2149, Avg Adaptive LR: 3.294460\n",
      "2025-03-18 21:30:15,204 - INFO - Iter 4651/10000, Loss: 0.2149, Avg Adaptive LR: 3.294459\n",
      "2025-03-18 21:30:15,605 - INFO - Iter 4661/10000, Loss: 0.2149, Avg Adaptive LR: 3.294458\n",
      "2025-03-18 21:30:15,999 - INFO - Iter 4671/10000, Loss: 0.2148, Avg Adaptive LR: 3.294456\n",
      "2025-03-18 21:30:16,402 - INFO - Iter 4681/10000, Loss: 0.2148, Avg Adaptive LR: 3.294455\n",
      "2025-03-18 21:30:16,802 - INFO - Iter 4691/10000, Loss: 0.2148, Avg Adaptive LR: 3.294453\n",
      "2025-03-18 21:30:17,203 - INFO - Iter 4701/10000, Loss: 0.2148, Avg Adaptive LR: 3.294452\n",
      "2025-03-18 21:30:17,607 - INFO - Iter 4711/10000, Loss: 0.2147, Avg Adaptive LR: 3.294450\n",
      "2025-03-18 21:30:18,011 - INFO - Iter 4721/10000, Loss: 0.2147, Avg Adaptive LR: 3.294449\n",
      "2025-03-18 21:30:18,430 - INFO - Iter 4731/10000, Loss: 0.2147, Avg Adaptive LR: 3.294448\n",
      "2025-03-18 21:30:18,825 - INFO - Iter 4741/10000, Loss: 0.2147, Avg Adaptive LR: 3.294446\n",
      "2025-03-18 21:30:19,219 - INFO - Iter 4751/10000, Loss: 0.2147, Avg Adaptive LR: 3.294445\n",
      "2025-03-18 21:30:19,618 - INFO - Iter 4761/10000, Loss: 0.2146, Avg Adaptive LR: 3.294443\n",
      "2025-03-18 21:30:20,013 - INFO - Iter 4771/10000, Loss: 0.2146, Avg Adaptive LR: 3.294442\n",
      "2025-03-18 21:30:20,415 - INFO - Iter 4781/10000, Loss: 0.2146, Avg Adaptive LR: 3.294441\n",
      "2025-03-18 21:30:20,811 - INFO - Iter 4791/10000, Loss: 0.2146, Avg Adaptive LR: 3.294439\n",
      "2025-03-18 21:30:21,204 - INFO - Iter 4801/10000, Loss: 0.2146, Avg Adaptive LR: 3.294438\n",
      "2025-03-18 21:30:21,611 - INFO - Iter 4811/10000, Loss: 0.2145, Avg Adaptive LR: 3.294436\n",
      "2025-03-18 21:30:22,003 - INFO - Iter 4821/10000, Loss: 0.2145, Avg Adaptive LR: 3.294435\n",
      "2025-03-18 21:30:22,415 - INFO - Iter 4831/10000, Loss: 0.2145, Avg Adaptive LR: 3.294434\n",
      "2025-03-18 21:30:22,821 - INFO - Iter 4841/10000, Loss: 0.2145, Avg Adaptive LR: 3.294432\n",
      "2025-03-18 21:30:23,215 - INFO - Iter 4851/10000, Loss: 0.2145, Avg Adaptive LR: 3.294431\n",
      "2025-03-18 21:30:23,615 - INFO - Iter 4861/10000, Loss: 0.2144, Avg Adaptive LR: 3.294430\n",
      "2025-03-18 21:30:24,019 - INFO - Iter 4871/10000, Loss: 0.2144, Avg Adaptive LR: 3.294428\n",
      "2025-03-18 21:30:24,423 - INFO - Iter 4881/10000, Loss: 0.2144, Avg Adaptive LR: 3.294427\n",
      "2025-03-18 21:30:24,814 - INFO - Iter 4891/10000, Loss: 0.2144, Avg Adaptive LR: 3.294426\n",
      "2025-03-18 21:30:25,209 - INFO - Iter 4901/10000, Loss: 0.2144, Avg Adaptive LR: 3.294424\n",
      "2025-03-18 21:30:25,609 - INFO - Iter 4911/10000, Loss: 0.2144, Avg Adaptive LR: 3.294423\n",
      "2025-03-18 21:30:25,998 - INFO - Iter 4921/10000, Loss: 0.2143, Avg Adaptive LR: 3.294422\n",
      "2025-03-18 21:30:26,401 - INFO - Iter 4931/10000, Loss: 0.2143, Avg Adaptive LR: 3.294420\n",
      "2025-03-18 21:30:26,799 - INFO - Iter 4941/10000, Loss: 0.2143, Avg Adaptive LR: 3.294419\n",
      "2025-03-18 21:30:27,210 - INFO - Iter 4951/10000, Loss: 0.2143, Avg Adaptive LR: 3.294418\n",
      "2025-03-18 21:30:27,609 - INFO - Iter 4961/10000, Loss: 0.2143, Avg Adaptive LR: 3.294416\n",
      "2025-03-18 21:30:28,008 - INFO - Iter 4971/10000, Loss: 0.2142, Avg Adaptive LR: 3.294415\n",
      "2025-03-18 21:30:28,424 - INFO - Iter 4981/10000, Loss: 0.2142, Avg Adaptive LR: 3.294414\n",
      "2025-03-18 21:30:28,821 - INFO - Iter 4991/10000, Loss: 0.2142, Avg Adaptive LR: 3.294412\n",
      "2025-03-18 21:30:29,216 - INFO - Iter 5001/10000, Loss: 0.2142, Avg Adaptive LR: 3.294411\n",
      "2025-03-18 21:30:29,616 - INFO - Iter 5011/10000, Loss: 0.2142, Avg Adaptive LR: 3.294410\n",
      "2025-03-18 21:30:30,010 - INFO - Iter 5021/10000, Loss: 0.2141, Avg Adaptive LR: 3.294408\n",
      "2025-03-18 21:30:30,409 - INFO - Iter 5031/10000, Loss: 0.2141, Avg Adaptive LR: 3.294407\n",
      "2025-03-18 21:30:30,804 - INFO - Iter 5041/10000, Loss: 0.2141, Avg Adaptive LR: 3.294406\n",
      "2025-03-18 21:30:31,197 - INFO - Iter 5051/10000, Loss: 0.2141, Avg Adaptive LR: 3.294405\n",
      "2025-03-18 21:30:31,600 - INFO - Iter 5061/10000, Loss: 0.2141, Avg Adaptive LR: 3.294403\n",
      "2025-03-18 21:30:31,997 - INFO - Iter 5071/10000, Loss: 0.2140, Avg Adaptive LR: 3.294402\n",
      "2025-03-18 21:30:32,398 - INFO - Iter 5081/10000, Loss: 0.2140, Avg Adaptive LR: 3.294401\n",
      "2025-03-18 21:30:32,794 - INFO - Iter 5091/10000, Loss: 0.2140, Avg Adaptive LR: 3.294399\n",
      "2025-03-18 21:30:33,200 - INFO - Iter 5101/10000, Loss: 0.2140, Avg Adaptive LR: 3.294398\n",
      "2025-03-18 21:30:33,603 - INFO - Iter 5111/10000, Loss: 0.2140, Avg Adaptive LR: 3.294397\n",
      "2025-03-18 21:30:33,996 - INFO - Iter 5121/10000, Loss: 0.2140, Avg Adaptive LR: 3.294396\n",
      "2025-03-18 21:30:34,395 - INFO - Iter 5131/10000, Loss: 0.2139, Avg Adaptive LR: 3.294394\n",
      "2025-03-18 21:30:34,790 - INFO - Iter 5141/10000, Loss: 0.2139, Avg Adaptive LR: 3.294393\n",
      "2025-03-18 21:30:35,184 - INFO - Iter 5151/10000, Loss: 0.2139, Avg Adaptive LR: 3.294392\n",
      "2025-03-18 21:30:35,584 - INFO - Iter 5161/10000, Loss: 0.2139, Avg Adaptive LR: 3.294391\n",
      "2025-03-18 21:30:35,975 - INFO - Iter 5171/10000, Loss: 0.2139, Avg Adaptive LR: 3.294389\n",
      "2025-03-18 21:30:36,379 - INFO - Iter 5181/10000, Loss: 0.2138, Avg Adaptive LR: 3.294388\n",
      "2025-03-18 21:30:36,775 - INFO - Iter 5191/10000, Loss: 0.2138, Avg Adaptive LR: 3.294387\n",
      "2025-03-18 21:30:37,169 - INFO - Iter 5201/10000, Loss: 0.2138, Avg Adaptive LR: 3.294386\n",
      "2025-03-18 21:30:37,583 - INFO - Iter 5211/10000, Loss: 0.2138, Avg Adaptive LR: 3.294384\n",
      "2025-03-18 21:30:37,977 - INFO - Iter 5221/10000, Loss: 0.2138, Avg Adaptive LR: 3.294383\n",
      "2025-03-18 21:30:38,391 - INFO - Iter 5231/10000, Loss: 0.2138, Avg Adaptive LR: 3.294382\n",
      "2025-03-18 21:30:38,784 - INFO - Iter 5241/10000, Loss: 0.2137, Avg Adaptive LR: 3.294381\n",
      "2025-03-18 21:30:39,176 - INFO - Iter 5251/10000, Loss: 0.2137, Avg Adaptive LR: 3.294380\n",
      "2025-03-18 21:30:39,575 - INFO - Iter 5261/10000, Loss: 0.2137, Avg Adaptive LR: 3.294378\n",
      "2025-03-18 21:30:39,968 - INFO - Iter 5271/10000, Loss: 0.2137, Avg Adaptive LR: 3.294377\n",
      "2025-03-18 21:30:40,374 - INFO - Iter 5281/10000, Loss: 0.2137, Avg Adaptive LR: 3.294376\n",
      "2025-03-18 21:30:40,769 - INFO - Iter 5291/10000, Loss: 0.2137, Avg Adaptive LR: 3.294375\n",
      "2025-03-18 21:30:41,165 - INFO - Iter 5301/10000, Loss: 0.2136, Avg Adaptive LR: 3.294374\n",
      "2025-03-18 21:30:41,568 - INFO - Iter 5311/10000, Loss: 0.2136, Avg Adaptive LR: 3.294372\n",
      "2025-03-18 21:30:41,960 - INFO - Iter 5321/10000, Loss: 0.2136, Avg Adaptive LR: 3.294371\n",
      "2025-03-18 21:30:42,365 - INFO - Iter 5331/10000, Loss: 0.2136, Avg Adaptive LR: 3.294370\n",
      "2025-03-18 21:30:42,757 - INFO - Iter 5341/10000, Loss: 0.2136, Avg Adaptive LR: 3.294369\n",
      "2025-03-18 21:30:43,151 - INFO - Iter 5351/10000, Loss: 0.2136, Avg Adaptive LR: 3.294368\n",
      "2025-03-18 21:30:43,556 - INFO - Iter 5361/10000, Loss: 0.2135, Avg Adaptive LR: 3.294366\n",
      "2025-03-18 21:30:43,950 - INFO - Iter 5371/10000, Loss: 0.2135, Avg Adaptive LR: 3.294365\n",
      "2025-03-18 21:30:44,357 - INFO - Iter 5381/10000, Loss: 0.2135, Avg Adaptive LR: 3.294364\n",
      "2025-03-18 21:30:44,751 - INFO - Iter 5391/10000, Loss: 0.2135, Avg Adaptive LR: 3.294363\n",
      "2025-03-18 21:30:45,146 - INFO - Iter 5401/10000, Loss: 0.2135, Avg Adaptive LR: 3.294362\n",
      "2025-03-18 21:30:45,549 - INFO - Iter 5411/10000, Loss: 0.2135, Avg Adaptive LR: 3.294361\n",
      "2025-03-18 21:30:45,947 - INFO - Iter 5421/10000, Loss: 0.2134, Avg Adaptive LR: 3.294359\n",
      "2025-03-18 21:30:46,350 - INFO - Iter 5431/10000, Loss: 0.2134, Avg Adaptive LR: 3.294358\n",
      "2025-03-18 21:30:46,740 - INFO - Iter 5441/10000, Loss: 0.2134, Avg Adaptive LR: 3.294357\n",
      "2025-03-18 21:30:47,132 - INFO - Iter 5451/10000, Loss: 0.2134, Avg Adaptive LR: 3.294356\n",
      "2025-03-18 21:30:47,558 - INFO - Iter 5461/10000, Loss: 0.2134, Avg Adaptive LR: 3.294355\n",
      "2025-03-18 21:30:47,953 - INFO - Iter 5471/10000, Loss: 0.2134, Avg Adaptive LR: 3.294354\n",
      "2025-03-18 21:30:48,364 - INFO - Iter 5481/10000, Loss: 0.2133, Avg Adaptive LR: 3.294353\n",
      "2025-03-18 21:30:48,774 - INFO - Iter 5491/10000, Loss: 0.2133, Avg Adaptive LR: 3.294351\n",
      "2025-03-18 21:30:49,165 - INFO - Iter 5501/10000, Loss: 0.2133, Avg Adaptive LR: 3.294350\n",
      "2025-03-18 21:30:49,568 - INFO - Iter 5511/10000, Loss: 0.2133, Avg Adaptive LR: 3.294349\n",
      "2025-03-18 21:30:49,961 - INFO - Iter 5521/10000, Loss: 0.2133, Avg Adaptive LR: 3.294348\n",
      "2025-03-18 21:30:50,366 - INFO - Iter 5531/10000, Loss: 0.2133, Avg Adaptive LR: 3.294347\n",
      "2025-03-18 21:30:50,760 - INFO - Iter 5541/10000, Loss: 0.2132, Avg Adaptive LR: 3.294346\n",
      "2025-03-18 21:30:51,148 - INFO - Iter 5551/10000, Loss: 0.2132, Avg Adaptive LR: 3.294345\n",
      "2025-03-18 21:30:51,549 - INFO - Iter 5561/10000, Loss: 0.2132, Avg Adaptive LR: 3.294344\n",
      "2025-03-18 21:30:51,948 - INFO - Iter 5571/10000, Loss: 0.2132, Avg Adaptive LR: 3.294342\n",
      "2025-03-18 21:30:52,350 - INFO - Iter 5581/10000, Loss: 0.2132, Avg Adaptive LR: 3.294341\n",
      "2025-03-18 21:30:52,744 - INFO - Iter 5591/10000, Loss: 0.2132, Avg Adaptive LR: 3.294340\n",
      "2025-03-18 21:30:53,135 - INFO - Iter 5601/10000, Loss: 0.2131, Avg Adaptive LR: 3.294339\n",
      "2025-03-18 21:30:53,540 - INFO - Iter 5611/10000, Loss: 0.2131, Avg Adaptive LR: 3.294338\n",
      "2025-03-18 21:30:53,934 - INFO - Iter 5621/10000, Loss: 0.2131, Avg Adaptive LR: 3.294337\n",
      "2025-03-18 21:30:54,338 - INFO - Iter 5631/10000, Loss: 0.2131, Avg Adaptive LR: 3.294336\n",
      "2025-03-18 21:30:54,732 - INFO - Iter 5641/10000, Loss: 0.2131, Avg Adaptive LR: 3.294335\n",
      "2025-03-18 21:30:55,127 - INFO - Iter 5651/10000, Loss: 0.2131, Avg Adaptive LR: 3.294334\n",
      "2025-03-18 21:30:55,534 - INFO - Iter 5661/10000, Loss: 0.2130, Avg Adaptive LR: 3.294333\n",
      "2025-03-18 21:30:55,929 - INFO - Iter 5671/10000, Loss: 0.2130, Avg Adaptive LR: 3.294331\n",
      "2025-03-18 21:30:56,329 - INFO - Iter 5681/10000, Loss: 0.2130, Avg Adaptive LR: 3.294330\n",
      "2025-03-18 21:30:56,728 - INFO - Iter 5691/10000, Loss: 0.2130, Avg Adaptive LR: 3.294329\n",
      "2025-03-18 21:30:57,126 - INFO - Iter 5701/10000, Loss: 0.2130, Avg Adaptive LR: 3.294328\n",
      "2025-03-18 21:30:57,541 - INFO - Iter 5711/10000, Loss: 0.2130, Avg Adaptive LR: 3.294327\n",
      "2025-03-18 21:30:57,958 - INFO - Iter 5721/10000, Loss: 0.2130, Avg Adaptive LR: 3.294326\n",
      "2025-03-18 21:30:58,367 - INFO - Iter 5731/10000, Loss: 0.2129, Avg Adaptive LR: 3.294325\n",
      "2025-03-18 21:30:58,776 - INFO - Iter 5741/10000, Loss: 0.2129, Avg Adaptive LR: 3.294324\n",
      "2025-03-18 21:30:59,170 - INFO - Iter 5751/10000, Loss: 0.2129, Avg Adaptive LR: 3.294323\n",
      "2025-03-18 21:30:59,581 - INFO - Iter 5761/10000, Loss: 0.2129, Avg Adaptive LR: 3.294322\n",
      "2025-03-18 21:30:59,983 - INFO - Iter 5771/10000, Loss: 0.2129, Avg Adaptive LR: 3.294321\n",
      "2025-03-18 21:31:00,390 - INFO - Iter 5781/10000, Loss: 0.2129, Avg Adaptive LR: 3.294320\n",
      "2025-03-18 21:31:00,790 - INFO - Iter 5791/10000, Loss: 0.2128, Avg Adaptive LR: 3.294319\n",
      "2025-03-18 21:31:01,182 - INFO - Iter 5801/10000, Loss: 0.2128, Avg Adaptive LR: 3.294318\n",
      "2025-03-18 21:31:01,580 - INFO - Iter 5811/10000, Loss: 0.2128, Avg Adaptive LR: 3.294317\n",
      "2025-03-18 21:31:01,968 - INFO - Iter 5821/10000, Loss: 0.2128, Avg Adaptive LR: 3.294316\n",
      "2025-03-18 21:31:02,380 - INFO - Iter 5831/10000, Loss: 0.2128, Avg Adaptive LR: 3.294314\n",
      "2025-03-18 21:31:02,771 - INFO - Iter 5841/10000, Loss: 0.2128, Avg Adaptive LR: 3.294313\n",
      "2025-03-18 21:31:03,164 - INFO - Iter 5851/10000, Loss: 0.2128, Avg Adaptive LR: 3.294312\n",
      "2025-03-18 21:31:03,566 - INFO - Iter 5861/10000, Loss: 0.2127, Avg Adaptive LR: 3.294311\n",
      "2025-03-18 21:31:03,965 - INFO - Iter 5871/10000, Loss: 0.2127, Avg Adaptive LR: 3.294310\n",
      "2025-03-18 21:31:04,366 - INFO - Iter 5881/10000, Loss: 0.2127, Avg Adaptive LR: 3.294309\n",
      "2025-03-18 21:31:04,762 - INFO - Iter 5891/10000, Loss: 0.2127, Avg Adaptive LR: 3.294308\n",
      "2025-03-18 21:31:05,154 - INFO - Iter 5901/10000, Loss: 0.2127, Avg Adaptive LR: 3.294307\n",
      "2025-03-18 21:31:05,552 - INFO - Iter 5911/10000, Loss: 0.2127, Avg Adaptive LR: 3.294306\n",
      "2025-03-18 21:31:05,945 - INFO - Iter 5921/10000, Loss: 0.2126, Avg Adaptive LR: 3.294305\n",
      "2025-03-18 21:31:06,350 - INFO - Iter 5931/10000, Loss: 0.2126, Avg Adaptive LR: 3.294304\n",
      "2025-03-18 21:31:06,742 - INFO - Iter 5941/10000, Loss: 0.2126, Avg Adaptive LR: 3.294303\n",
      "2025-03-18 21:31:07,136 - INFO - Iter 5951/10000, Loss: 0.2126, Avg Adaptive LR: 3.294302\n",
      "2025-03-18 21:31:07,541 - INFO - Iter 5961/10000, Loss: 0.2126, Avg Adaptive LR: 3.294301\n",
      "2025-03-18 21:31:07,934 - INFO - Iter 5971/10000, Loss: 0.2126, Avg Adaptive LR: 3.294300\n",
      "2025-03-18 21:31:08,349 - INFO - Iter 5981/10000, Loss: 0.2126, Avg Adaptive LR: 3.294299\n",
      "2025-03-18 21:31:08,745 - INFO - Iter 5991/10000, Loss: 0.2125, Avg Adaptive LR: 3.294298\n",
      "2025-03-18 21:31:09,147 - INFO - Iter 6001/10000, Loss: 0.2125, Avg Adaptive LR: 3.294297\n",
      "2025-03-18 21:31:09,547 - INFO - Iter 6011/10000, Loss: 0.2125, Avg Adaptive LR: 3.294296\n",
      "2025-03-18 21:31:09,941 - INFO - Iter 6021/10000, Loss: 0.2125, Avg Adaptive LR: 3.294295\n",
      "2025-03-18 21:31:10,340 - INFO - Iter 6031/10000, Loss: 0.2125, Avg Adaptive LR: 3.294294\n",
      "2025-03-18 21:31:10,730 - INFO - Iter 6041/10000, Loss: 0.2125, Avg Adaptive LR: 3.294293\n",
      "2025-03-18 21:31:11,125 - INFO - Iter 6051/10000, Loss: 0.2125, Avg Adaptive LR: 3.294292\n",
      "2025-03-18 21:31:11,531 - INFO - Iter 6061/10000, Loss: 0.2124, Avg Adaptive LR: 3.294291\n",
      "2025-03-18 21:31:11,927 - INFO - Iter 6071/10000, Loss: 0.2124, Avg Adaptive LR: 3.294290\n",
      "2025-03-18 21:31:12,329 - INFO - Iter 6081/10000, Loss: 0.2124, Avg Adaptive LR: 3.294289\n",
      "2025-03-18 21:31:12,719 - INFO - Iter 6091/10000, Loss: 0.2124, Avg Adaptive LR: 3.294288\n",
      "2025-03-18 21:31:13,115 - INFO - Iter 6101/10000, Loss: 0.2124, Avg Adaptive LR: 3.294287\n",
      "2025-03-18 21:31:13,518 - INFO - Iter 6111/10000, Loss: 0.2124, Avg Adaptive LR: 3.294286\n",
      "2025-03-18 21:31:13,921 - INFO - Iter 6121/10000, Loss: 0.2124, Avg Adaptive LR: 3.294285\n",
      "2025-03-18 21:31:14,321 - INFO - Iter 6131/10000, Loss: 0.2123, Avg Adaptive LR: 3.294284\n",
      "2025-03-18 21:31:14,713 - INFO - Iter 6141/10000, Loss: 0.2123, Avg Adaptive LR: 3.294283\n",
      "2025-03-18 21:31:15,109 - INFO - Iter 6151/10000, Loss: 0.2123, Avg Adaptive LR: 3.294282\n",
      "2025-03-18 21:31:15,512 - INFO - Iter 6161/10000, Loss: 0.2123, Avg Adaptive LR: 3.294282\n",
      "2025-03-18 21:31:15,910 - INFO - Iter 6171/10000, Loss: 0.2123, Avg Adaptive LR: 3.294281\n",
      "2025-03-18 21:31:16,316 - INFO - Iter 6181/10000, Loss: 0.2123, Avg Adaptive LR: 3.294280\n",
      "2025-03-18 21:31:16,711 - INFO - Iter 6191/10000, Loss: 0.2123, Avg Adaptive LR: 3.294279\n",
      "2025-03-18 21:31:17,103 - INFO - Iter 6201/10000, Loss: 0.2123, Avg Adaptive LR: 3.294278\n",
      "2025-03-18 21:31:17,509 - INFO - Iter 6211/10000, Loss: 0.2122, Avg Adaptive LR: 3.294277\n",
      "2025-03-18 21:31:17,909 - INFO - Iter 6221/10000, Loss: 0.2122, Avg Adaptive LR: 3.294276\n",
      "2025-03-18 21:31:18,314 - INFO - Iter 6231/10000, Loss: 0.2122, Avg Adaptive LR: 3.294275\n",
      "2025-03-18 21:31:18,723 - INFO - Iter 6241/10000, Loss: 0.2122, Avg Adaptive LR: 3.294274\n",
      "2025-03-18 21:31:19,131 - INFO - Iter 6251/10000, Loss: 0.2122, Avg Adaptive LR: 3.294273\n",
      "2025-03-18 21:31:19,537 - INFO - Iter 6261/10000, Loss: 0.2122, Avg Adaptive LR: 3.294272\n",
      "2025-03-18 21:31:19,930 - INFO - Iter 6271/10000, Loss: 0.2122, Avg Adaptive LR: 3.294271\n",
      "2025-03-18 21:31:20,334 - INFO - Iter 6281/10000, Loss: 0.2121, Avg Adaptive LR: 3.294270\n",
      "2025-03-18 21:31:20,730 - INFO - Iter 6291/10000, Loss: 0.2121, Avg Adaptive LR: 3.294269\n",
      "2025-03-18 21:31:21,130 - INFO - Iter 6301/10000, Loss: 0.2121, Avg Adaptive LR: 3.294268\n",
      "2025-03-18 21:31:21,531 - INFO - Iter 6311/10000, Loss: 0.2121, Avg Adaptive LR: 3.294267\n",
      "2025-03-18 21:31:21,927 - INFO - Iter 6321/10000, Loss: 0.2121, Avg Adaptive LR: 3.294266\n",
      "2025-03-18 21:31:22,334 - INFO - Iter 6331/10000, Loss: 0.2121, Avg Adaptive LR: 3.294266\n",
      "2025-03-18 21:31:22,731 - INFO - Iter 6341/10000, Loss: 0.2121, Avg Adaptive LR: 3.294265\n",
      "2025-03-18 21:31:23,128 - INFO - Iter 6351/10000, Loss: 0.2120, Avg Adaptive LR: 3.294264\n",
      "2025-03-18 21:31:23,533 - INFO - Iter 6361/10000, Loss: 0.2120, Avg Adaptive LR: 3.294263\n",
      "2025-03-18 21:31:23,933 - INFO - Iter 6371/10000, Loss: 0.2120, Avg Adaptive LR: 3.294262\n",
      "2025-03-18 21:31:24,338 - INFO - Iter 6381/10000, Loss: 0.2120, Avg Adaptive LR: 3.294261\n",
      "2025-03-18 21:31:24,734 - INFO - Iter 6391/10000, Loss: 0.2120, Avg Adaptive LR: 3.294260\n",
      "2025-03-18 21:31:25,134 - INFO - Iter 6401/10000, Loss: 0.2120, Avg Adaptive LR: 3.294259\n",
      "2025-03-18 21:31:25,533 - INFO - Iter 6411/10000, Loss: 0.2120, Avg Adaptive LR: 3.294258\n",
      "2025-03-18 21:31:25,929 - INFO - Iter 6421/10000, Loss: 0.2120, Avg Adaptive LR: 3.294257\n",
      "2025-03-18 21:31:26,330 - INFO - Iter 6431/10000, Loss: 0.2119, Avg Adaptive LR: 3.294256\n",
      "2025-03-18 21:31:26,727 - INFO - Iter 6441/10000, Loss: 0.2119, Avg Adaptive LR: 3.294256\n",
      "2025-03-18 21:31:27,117 - INFO - Iter 6451/10000, Loss: 0.2119, Avg Adaptive LR: 3.294255\n",
      "2025-03-18 21:31:27,515 - INFO - Iter 6461/10000, Loss: 0.2119, Avg Adaptive LR: 3.294254\n",
      "2025-03-18 21:31:27,908 - INFO - Iter 6471/10000, Loss: 0.2119, Avg Adaptive LR: 3.294253\n",
      "2025-03-18 21:31:28,312 - INFO - Iter 6481/10000, Loss: 0.2119, Avg Adaptive LR: 3.294252\n",
      "2025-03-18 21:31:28,718 - INFO - Iter 6491/10000, Loss: 0.2119, Avg Adaptive LR: 3.294251\n",
      "2025-03-18 21:31:29,133 - INFO - Iter 6501/10000, Loss: 0.2119, Avg Adaptive LR: 3.294250\n",
      "2025-03-18 21:31:29,533 - INFO - Iter 6511/10000, Loss: 0.2118, Avg Adaptive LR: 3.294249\n",
      "2025-03-18 21:31:29,928 - INFO - Iter 6521/10000, Loss: 0.2118, Avg Adaptive LR: 3.294248\n",
      "2025-03-18 21:31:30,325 - INFO - Iter 6531/10000, Loss: 0.2118, Avg Adaptive LR: 3.294248\n",
      "2025-03-18 21:31:30,724 - INFO - Iter 6541/10000, Loss: 0.2118, Avg Adaptive LR: 3.294247\n",
      "2025-03-18 21:31:31,118 - INFO - Iter 6551/10000, Loss: 0.2118, Avg Adaptive LR: 3.294246\n",
      "2025-03-18 21:31:31,520 - INFO - Iter 6561/10000, Loss: 0.2118, Avg Adaptive LR: 3.294245\n",
      "2025-03-18 21:31:31,918 - INFO - Iter 6571/10000, Loss: 0.2118, Avg Adaptive LR: 3.294244\n",
      "2025-03-18 21:31:32,320 - INFO - Iter 6581/10000, Loss: 0.2118, Avg Adaptive LR: 3.294243\n",
      "2025-03-18 21:31:32,714 - INFO - Iter 6591/10000, Loss: 0.2117, Avg Adaptive LR: 3.294242\n",
      "2025-03-18 21:31:33,110 - INFO - Iter 6601/10000, Loss: 0.2117, Avg Adaptive LR: 3.294241\n",
      "2025-03-18 21:31:33,510 - INFO - Iter 6611/10000, Loss: 0.2117, Avg Adaptive LR: 3.294241\n",
      "2025-03-18 21:31:33,908 - INFO - Iter 6621/10000, Loss: 0.2117, Avg Adaptive LR: 3.294240\n",
      "2025-03-18 21:31:34,309 - INFO - Iter 6631/10000, Loss: 0.2117, Avg Adaptive LR: 3.294239\n",
      "2025-03-18 21:31:34,704 - INFO - Iter 6641/10000, Loss: 0.2117, Avg Adaptive LR: 3.294238\n",
      "2025-03-18 21:31:35,098 - INFO - Iter 6651/10000, Loss: 0.2117, Avg Adaptive LR: 3.294237\n",
      "2025-03-18 21:31:35,500 - INFO - Iter 6661/10000, Loss: 0.2117, Avg Adaptive LR: 3.294236\n",
      "2025-03-18 21:31:35,893 - INFO - Iter 6671/10000, Loss: 0.2116, Avg Adaptive LR: 3.294235\n",
      "2025-03-18 21:31:36,297 - INFO - Iter 6681/10000, Loss: 0.2116, Avg Adaptive LR: 3.294235\n",
      "2025-03-18 21:31:36,694 - INFO - Iter 6691/10000, Loss: 0.2116, Avg Adaptive LR: 3.294234\n",
      "2025-03-18 21:31:37,085 - INFO - Iter 6701/10000, Loss: 0.2116, Avg Adaptive LR: 3.294233\n",
      "2025-03-18 21:31:37,489 - INFO - Iter 6711/10000, Loss: 0.2116, Avg Adaptive LR: 3.294232\n",
      "2025-03-18 21:31:37,883 - INFO - Iter 6721/10000, Loss: 0.2116, Avg Adaptive LR: 3.294231\n",
      "2025-03-18 21:31:38,287 - INFO - Iter 6731/10000, Loss: 0.2116, Avg Adaptive LR: 3.294230\n",
      "2025-03-18 21:31:38,680 - INFO - Iter 6741/10000, Loss: 0.2116, Avg Adaptive LR: 3.294229\n",
      "2025-03-18 21:31:39,070 - INFO - Iter 6751/10000, Loss: 0.2115, Avg Adaptive LR: 3.294229\n",
      "2025-03-18 21:31:39,496 - INFO - Iter 6761/10000, Loss: 0.2115, Avg Adaptive LR: 3.294228\n",
      "2025-03-18 21:31:39,897 - INFO - Iter 6771/10000, Loss: 0.2115, Avg Adaptive LR: 3.294227\n",
      "2025-03-18 21:31:40,295 - INFO - Iter 6781/10000, Loss: 0.2115, Avg Adaptive LR: 3.294226\n",
      "2025-03-18 21:31:40,690 - INFO - Iter 6791/10000, Loss: 0.2115, Avg Adaptive LR: 3.294225\n",
      "2025-03-18 21:31:41,080 - INFO - Iter 6801/10000, Loss: 0.2115, Avg Adaptive LR: 3.294224\n",
      "2025-03-18 21:31:41,486 - INFO - Iter 6811/10000, Loss: 0.2115, Avg Adaptive LR: 3.294224\n",
      "2025-03-18 21:31:41,889 - INFO - Iter 6821/10000, Loss: 0.2115, Avg Adaptive LR: 3.294223\n",
      "2025-03-18 21:31:42,292 - INFO - Iter 6831/10000, Loss: 0.2114, Avg Adaptive LR: 3.294222\n",
      "2025-03-18 21:31:42,686 - INFO - Iter 6841/10000, Loss: 0.2114, Avg Adaptive LR: 3.294221\n",
      "2025-03-18 21:31:43,077 - INFO - Iter 6851/10000, Loss: 0.2114, Avg Adaptive LR: 3.294220\n",
      "2025-03-18 21:31:43,474 - INFO - Iter 6861/10000, Loss: 0.2114, Avg Adaptive LR: 3.294219\n",
      "2025-03-18 21:31:43,869 - INFO - Iter 6871/10000, Loss: 0.2114, Avg Adaptive LR: 3.294219\n",
      "2025-03-18 21:31:44,276 - INFO - Iter 6881/10000, Loss: 0.2114, Avg Adaptive LR: 3.294218\n",
      "2025-03-18 21:31:44,673 - INFO - Iter 6891/10000, Loss: 0.2114, Avg Adaptive LR: 3.294217\n",
      "2025-03-18 21:31:45,069 - INFO - Iter 6901/10000, Loss: 0.2114, Avg Adaptive LR: 3.294216\n",
      "2025-03-18 21:31:45,478 - INFO - Iter 6911/10000, Loss: 0.2113, Avg Adaptive LR: 3.294215\n",
      "2025-03-18 21:31:45,879 - INFO - Iter 6921/10000, Loss: 0.2113, Avg Adaptive LR: 3.294215\n",
      "2025-03-18 21:31:46,290 - INFO - Iter 6931/10000, Loss: 0.2113, Avg Adaptive LR: 3.294214\n",
      "2025-03-18 21:31:46,681 - INFO - Iter 6941/10000, Loss: 0.2113, Avg Adaptive LR: 3.294213\n",
      "2025-03-18 21:31:47,074 - INFO - Iter 6951/10000, Loss: 0.2113, Avg Adaptive LR: 3.294212\n",
      "2025-03-18 21:31:47,476 - INFO - Iter 6961/10000, Loss: 0.2113, Avg Adaptive LR: 3.294211\n",
      "2025-03-18 21:31:47,872 - INFO - Iter 6971/10000, Loss: 0.2113, Avg Adaptive LR: 3.294211\n",
      "2025-03-18 21:31:48,276 - INFO - Iter 6981/10000, Loss: 0.2113, Avg Adaptive LR: 3.294210\n",
      "2025-03-18 21:31:48,675 - INFO - Iter 6991/10000, Loss: 0.2113, Avg Adaptive LR: 3.294209\n",
      "2025-03-18 21:31:49,067 - INFO - Iter 7001/10000, Loss: 0.2112, Avg Adaptive LR: 3.294208\n",
      "2025-03-18 21:31:49,475 - INFO - Iter 7011/10000, Loss: 0.2112, Avg Adaptive LR: 3.294207\n",
      "2025-03-18 21:31:49,889 - INFO - Iter 7021/10000, Loss: 0.2112, Avg Adaptive LR: 3.294207\n",
      "2025-03-18 21:31:50,298 - INFO - Iter 7031/10000, Loss: 0.2112, Avg Adaptive LR: 3.294206\n",
      "2025-03-18 21:31:50,691 - INFO - Iter 7041/10000, Loss: 0.2112, Avg Adaptive LR: 3.294205\n",
      "2025-03-18 21:31:51,085 - INFO - Iter 7051/10000, Loss: 0.2112, Avg Adaptive LR: 3.294204\n",
      "2025-03-18 21:31:51,491 - INFO - Iter 7061/10000, Loss: 0.2112, Avg Adaptive LR: 3.294203\n",
      "2025-03-18 21:31:51,886 - INFO - Iter 7071/10000, Loss: 0.2112, Avg Adaptive LR: 3.294203\n",
      "2025-03-18 21:31:52,288 - INFO - Iter 7081/10000, Loss: 0.2112, Avg Adaptive LR: 3.294202\n",
      "2025-03-18 21:31:52,680 - INFO - Iter 7091/10000, Loss: 0.2111, Avg Adaptive LR: 3.294201\n",
      "2025-03-18 21:31:53,078 - INFO - Iter 7101/10000, Loss: 0.2111, Avg Adaptive LR: 3.294200\n",
      "2025-03-18 21:31:53,492 - INFO - Iter 7111/10000, Loss: 0.2111, Avg Adaptive LR: 3.294200\n",
      "2025-03-18 21:31:53,885 - INFO - Iter 7121/10000, Loss: 0.2111, Avg Adaptive LR: 3.294199\n",
      "2025-03-18 21:31:54,289 - INFO - Iter 7131/10000, Loss: 0.2111, Avg Adaptive LR: 3.294198\n",
      "2025-03-18 21:31:54,692 - INFO - Iter 7141/10000, Loss: 0.2111, Avg Adaptive LR: 3.294197\n",
      "2025-03-18 21:31:55,088 - INFO - Iter 7151/10000, Loss: 0.2111, Avg Adaptive LR: 3.294196\n",
      "2025-03-18 21:31:55,490 - INFO - Iter 7161/10000, Loss: 0.2111, Avg Adaptive LR: 3.294196\n",
      "2025-03-18 21:31:55,879 - INFO - Iter 7171/10000, Loss: 0.2111, Avg Adaptive LR: 3.294195\n",
      "2025-03-18 21:31:56,287 - INFO - Iter 7181/10000, Loss: 0.2110, Avg Adaptive LR: 3.294194\n",
      "2025-03-18 21:31:56,683 - INFO - Iter 7191/10000, Loss: 0.2110, Avg Adaptive LR: 3.294193\n",
      "2025-03-18 21:31:57,077 - INFO - Iter 7201/10000, Loss: 0.2110, Avg Adaptive LR: 3.294193\n",
      "2025-03-18 21:31:57,483 - INFO - Iter 7211/10000, Loss: 0.2110, Avg Adaptive LR: 3.294192\n",
      "2025-03-18 21:31:57,881 - INFO - Iter 7221/10000, Loss: 0.2110, Avg Adaptive LR: 3.294191\n",
      "2025-03-18 21:31:58,289 - INFO - Iter 7231/10000, Loss: 0.2110, Avg Adaptive LR: 3.294190\n",
      "2025-03-18 21:31:58,695 - INFO - Iter 7241/10000, Loss: 0.2110, Avg Adaptive LR: 3.294190\n",
      "2025-03-18 21:31:59,100 - INFO - Iter 7251/10000, Loss: 0.2110, Avg Adaptive LR: 3.294189\n",
      "2025-03-18 21:31:59,500 - INFO - Iter 7261/10000, Loss: 0.2110, Avg Adaptive LR: 3.294188\n",
      "2025-03-18 21:31:59,930 - INFO - Iter 7271/10000, Loss: 0.2109, Avg Adaptive LR: 3.294187\n",
      "2025-03-18 21:32:00,341 - INFO - Iter 7281/10000, Loss: 0.2109, Avg Adaptive LR: 3.294187\n",
      "2025-03-18 21:32:00,734 - INFO - Iter 7291/10000, Loss: 0.2109, Avg Adaptive LR: 3.294186\n",
      "2025-03-18 21:32:01,128 - INFO - Iter 7301/10000, Loss: 0.2109, Avg Adaptive LR: 3.294185\n",
      "2025-03-18 21:32:01,533 - INFO - Iter 7311/10000, Loss: 0.2109, Avg Adaptive LR: 3.294184\n",
      "2025-03-18 21:32:01,928 - INFO - Iter 7321/10000, Loss: 0.2109, Avg Adaptive LR: 3.294184\n",
      "2025-03-18 21:32:02,328 - INFO - Iter 7331/10000, Loss: 0.2109, Avg Adaptive LR: 3.294183\n",
      "2025-03-18 21:32:02,728 - INFO - Iter 7341/10000, Loss: 0.2109, Avg Adaptive LR: 3.294182\n",
      "2025-03-18 21:32:03,137 - INFO - Iter 7351/10000, Loss: 0.2109, Avg Adaptive LR: 3.294181\n",
      "2025-03-18 21:32:03,540 - INFO - Iter 7361/10000, Loss: 0.2108, Avg Adaptive LR: 3.294181\n",
      "2025-03-18 21:32:03,937 - INFO - Iter 7371/10000, Loss: 0.2108, Avg Adaptive LR: 3.294180\n",
      "2025-03-18 21:32:04,356 - INFO - Iter 7381/10000, Loss: 0.2108, Avg Adaptive LR: 3.294179\n",
      "2025-03-18 21:32:04,747 - INFO - Iter 7391/10000, Loss: 0.2108, Avg Adaptive LR: 3.294178\n",
      "2025-03-18 21:32:05,143 - INFO - Iter 7401/10000, Loss: 0.2108, Avg Adaptive LR: 3.294178\n",
      "2025-03-18 21:32:05,545 - INFO - Iter 7411/10000, Loss: 0.2108, Avg Adaptive LR: 3.294177\n",
      "2025-03-18 21:32:05,944 - INFO - Iter 7421/10000, Loss: 0.2108, Avg Adaptive LR: 3.294176\n",
      "2025-03-18 21:32:06,361 - INFO - Iter 7431/10000, Loss: 0.2108, Avg Adaptive LR: 3.294176\n",
      "2025-03-18 21:32:06,761 - INFO - Iter 7441/10000, Loss: 0.2108, Avg Adaptive LR: 3.294175\n",
      "2025-03-18 21:32:07,157 - INFO - Iter 7451/10000, Loss: 0.2108, Avg Adaptive LR: 3.294174\n",
      "2025-03-18 21:32:07,566 - INFO - Iter 7461/10000, Loss: 0.2107, Avg Adaptive LR: 3.294173\n",
      "2025-03-18 21:32:07,968 - INFO - Iter 7471/10000, Loss: 0.2107, Avg Adaptive LR: 3.294173\n",
      "2025-03-18 21:32:08,373 - INFO - Iter 7481/10000, Loss: 0.2107, Avg Adaptive LR: 3.294172\n",
      "2025-03-18 21:32:08,795 - INFO - Iter 7491/10000, Loss: 0.2107, Avg Adaptive LR: 3.294171\n",
      "2025-03-18 21:32:09,200 - INFO - Iter 7501/10000, Loss: 0.2107, Avg Adaptive LR: 3.294171\n",
      "2025-03-18 21:32:09,604 - INFO - Iter 7511/10000, Loss: 0.2107, Avg Adaptive LR: 3.294170\n",
      "2025-03-18 21:32:10,014 - INFO - Iter 7521/10000, Loss: 0.2107, Avg Adaptive LR: 3.294169\n",
      "2025-03-18 21:32:10,427 - INFO - Iter 7531/10000, Loss: 0.2107, Avg Adaptive LR: 3.294168\n",
      "2025-03-18 21:32:10,826 - INFO - Iter 7541/10000, Loss: 0.2107, Avg Adaptive LR: 3.294168\n",
      "2025-03-18 21:32:11,223 - INFO - Iter 7551/10000, Loss: 0.2106, Avg Adaptive LR: 3.294167\n",
      "2025-03-18 21:32:11,637 - INFO - Iter 7561/10000, Loss: 0.2106, Avg Adaptive LR: 3.294166\n",
      "2025-03-18 21:32:12,035 - INFO - Iter 7571/10000, Loss: 0.2106, Avg Adaptive LR: 3.294166\n",
      "2025-03-18 21:32:12,447 - INFO - Iter 7581/10000, Loss: 0.2106, Avg Adaptive LR: 3.294165\n",
      "2025-03-18 21:32:12,844 - INFO - Iter 7591/10000, Loss: 0.2106, Avg Adaptive LR: 3.294164\n",
      "2025-03-18 21:32:13,236 - INFO - Iter 7601/10000, Loss: 0.2106, Avg Adaptive LR: 3.294163\n",
      "2025-03-18 21:32:13,638 - INFO - Iter 7611/10000, Loss: 0.2106, Avg Adaptive LR: 3.294163\n",
      "2025-03-18 21:32:14,036 - INFO - Iter 7621/10000, Loss: 0.2106, Avg Adaptive LR: 3.294162\n",
      "2025-03-18 21:32:14,441 - INFO - Iter 7631/10000, Loss: 0.2106, Avg Adaptive LR: 3.294161\n",
      "2025-03-18 21:32:14,835 - INFO - Iter 7641/10000, Loss: 0.2106, Avg Adaptive LR: 3.294161\n",
      "2025-03-18 21:32:15,232 - INFO - Iter 7651/10000, Loss: 0.2105, Avg Adaptive LR: 3.294160\n",
      "2025-03-18 21:32:15,630 - INFO - Iter 7661/10000, Loss: 0.2105, Avg Adaptive LR: 3.294159\n",
      "2025-03-18 21:32:16,023 - INFO - Iter 7671/10000, Loss: 0.2105, Avg Adaptive LR: 3.294159\n",
      "2025-03-18 21:32:16,429 - INFO - Iter 7681/10000, Loss: 0.2105, Avg Adaptive LR: 3.294158\n",
      "2025-03-18 21:32:16,621 - INFO - Early stopping triggered at iteration 7686 with training loss 0.210512\n",
      "2025-03-18 21:32:16,622 - INFO - SoftmaxRegression training completed in 307.98 seconds.\n",
      "2025-03-18 21:32:16,622 - INFO - --- Linear Regression LR=0.1/Iter=10000 ---\n",
      "2025-03-18 21:32:19,793 - INFO - Iter 100/10000, Loss: 0.8123, Gradient Norm: 17.2331, Avg Adaptive LR: 1.3966073375164807\n",
      "2025-03-18 21:32:22,997 - INFO - Iter 200/10000, Loss: 0.4286, Gradient Norm: 12.2269, Avg Adaptive LR: 0.9913722486345247\n",
      "2025-03-18 21:32:26,248 - INFO - Iter 300/10000, Loss: 0.2906, Gradient Norm: 9.8200, Avg Adaptive LR: 0.810855690898011\n",
      "2025-03-18 21:32:29,526 - INFO - Iter 400/10000, Loss: 0.2200, Gradient Norm: 8.3230, Avg Adaptive LR: 0.7029677295511769\n",
      "2025-03-18 21:32:32,810 - INFO - Iter 500/10000, Loss: 0.1793, Gradient Norm: 7.3236, Avg Adaptive LR: 0.6292007549960658\n",
      "2025-03-18 21:32:36,099 - INFO - Iter 600/10000, Loss: 0.1520, Gradient Norm: 6.5704, Avg Adaptive LR: 0.5746837799952516\n",
      "2025-03-18 21:32:39,377 - INFO - Iter 700/10000, Loss: 0.1330, Gradient Norm: 5.9904, Avg Adaptive LR: 0.532289891484879\n",
      "2025-03-18 21:32:42,684 - INFO - Iter 800/10000, Loss: 0.1182, Gradient Norm: 5.4928, Avg Adaptive LR: 0.4980731501312876\n",
      "2025-03-18 21:32:46,028 - INFO - Iter 900/10000, Loss: 0.1077, Gradient Norm: 5.1159, Avg Adaptive LR: 0.4697202013045508\n",
      "2025-03-18 21:32:49,378 - INFO - Iter 1000/10000, Loss: 0.1009, Gradient Norm: 4.8531, Avg Adaptive LR: 0.44572949233599296\n",
      "2025-03-18 21:32:52,708 - INFO - Iter 1100/10000, Loss: 0.0930, Gradient Norm: 4.5317, Avg Adaptive LR: 0.42506690371555017\n",
      "2025-03-18 21:32:56,089 - INFO - Iter 1200/10000, Loss: 0.0851, Gradient Norm: 4.1839, Avg Adaptive LR: 0.40703644929455796\n",
      "2025-03-18 21:32:59,506 - INFO - Iter 1300/10000, Loss: 0.0804, Gradient Norm: 3.9600, Avg Adaptive LR: 0.39113766904744734\n",
      "2025-03-18 21:33:02,872 - INFO - Iter 1400/10000, Loss: 0.0750, Gradient Norm: 3.6925, Avg Adaptive LR: 0.3769463937595444\n",
      "2025-03-18 21:33:06,230 - INFO - Iter 1500/10000, Loss: 0.0706, Gradient Norm: 3.4557, Avg Adaptive LR: 0.3642246466931996\n",
      "2025-03-18 21:33:09,578 - INFO - Iter 1600/10000, Loss: 0.0663, Gradient Norm: 3.2042, Avg Adaptive LR: 0.3526928071902571\n",
      "2025-03-18 21:33:12,889 - INFO - Iter 1700/10000, Loss: 0.0634, Gradient Norm: 3.0285, Avg Adaptive LR: 0.3421815082687663\n",
      "2025-03-18 21:33:16,193 - INFO - Iter 1800/10000, Loss: 0.0611, Gradient Norm: 2.8820, Avg Adaptive LR: 0.33256543370813635\n",
      "2025-03-18 21:33:19,538 - INFO - Iter 1900/10000, Loss: 0.0592, Gradient Norm: 2.7519, Avg Adaptive LR: 0.3237335626034096\n",
      "2025-03-18 21:33:22,861 - INFO - Iter 2000/10000, Loss: 0.0580, Gradient Norm: 2.6673, Avg Adaptive LR: 0.31556449240432066\n",
      "2025-03-18 21:33:26,191 - INFO - Iter 2100/10000, Loss: 0.0567, Gradient Norm: 2.5695, Avg Adaptive LR: 0.3079672287593788\n",
      "2025-03-18 21:33:29,523 - INFO - Iter 2200/10000, Loss: 0.0556, Gradient Norm: 2.4883, Avg Adaptive LR: 0.30091517138626694\n",
      "2025-03-18 21:33:32,823 - INFO - Iter 2300/10000, Loss: 0.0546, Gradient Norm: 2.4127, Avg Adaptive LR: 0.2943190170649717\n",
      "2025-03-18 21:33:36,154 - INFO - Iter 2400/10000, Loss: 0.0538, Gradient Norm: 2.3548, Avg Adaptive LR: 0.28813571220757567\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# TRAINING CELL\n",
    "# =========================================================\n",
    "\n",
    "# 1) Dictionaries to store trained models\n",
    "trained_models_clean   = {}\n",
    "trained_models_pocket  = {}\n",
    "trained_models_softmax = {}\n",
    "trained_models_linear  = {}\n",
    "\n",
    "# 2) Train Regression Models (Softmax & Linear)\n",
    "logger.info(\"=== TRAINING REGRESSION MODELS (Softmax & Linear) ===\")\n",
    "for cfg in tqdm(regression_run_configs, desc=\"Train Regressions\"):\n",
    "    lr_val = cfg[\"learning_rate\"]\n",
    "    max_iter_val = cfg[\"max_iter\"]\n",
    "    label = cfg[\"label\"]  # e.g. \"LR=0.001/Iter=1000\"\n",
    "\n",
    "    # --- Softmax ---\n",
    "    logger.info(f\"--- Softmax {label} ---\")\n",
    "    s_model = SoftmaxRegression(\n",
    "        num_classes=10,\n",
    "        max_iter=max_iter_val,\n",
    "        learning_rate=lr_val,\n",
    "        adaptive_lr=True\n",
    "    )\n",
    "    s_model.fit(X_train, y_train)\n",
    "    trained_models_softmax[(lr_val, max_iter_val)] = s_model\n",
    "\n",
    "    # --- Linear ---\n",
    "    logger.info(f\"--- Linear Regression {label} ---\")\n",
    "    lin_model = LinearRegression(\n",
    "        num_classes=10,\n",
    "        max_iter=max_iter_val,\n",
    "        learning_rate=lr_val,\n",
    "        adaptive_lr=True,\n",
    "        early_stopping=False\n",
    "    )\n",
    "    lin_model.fit(X_train, y_train)\n",
    "    trained_models_linear[(lr_val, max_iter_val)] = lin_model\n",
    "\n",
    "logger.info(\"Training complete for Softmax and Linear.\")\n",
    "\n",
    "# 3) Train Perceptron Models (Clean & Pocket)\n",
    "logger.info(\"=== TRAINING PERCEPTRON MODELS (Clean & Pocket) ===\")\n",
    "for max_iter in tqdm(perceptron_max_iter_values, desc=\"Train Clean & Pocket\"):\n",
    "    logger.info(f\"--- Clean PLA, max_iter={max_iter} ---\")\n",
    "    clean_perc = MultiClassPerceptron(num_classes=10, max_iter=max_iter, use_pocket=False)\n",
    "    clean_perc.fit(X_train, y_train)\n",
    "    trained_models_clean[max_iter] = clean_perc\n",
    "\n",
    "    logger.info(f\"--- Pocket PLA, max_iter={max_iter} ---\")\n",
    "    pocket_perc = MultiClassPerceptron(num_classes=10, max_iter=max_iter, use_pocket=True)\n",
    "    pocket_perc.fit(X_train, y_train)\n",
    "    trained_models_pocket[max_iter] = pocket_perc\n",
    "\n",
    "logger.info(\"Training complete for Clean PLA and Pocket PLA.\")\n",
    "logger.info(\"=== ALL TRAINING COMPLETE ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VepUzPkWrANq"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Hech6_lrANq",
    "outputId": "4f9fa79c-c5d4-4a07-b8ee-87427ac13bba"
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# EVALUATION CELL (with pandas DataFrame)\n",
    "##################################################\n",
    "\n",
    "\n",
    "# 1) Evaluate Perceptrons: Clean & Pocket\n",
    "accuracies_clean, accuracies_pocket = [], []\n",
    "runtimes_clean,   runtimes_pocket   = [], []\n",
    "sensitivities_clean, sensitivities_pocket = [], []\n",
    "selectivities_clean, selectivities_pocket = [], []\n",
    "\n",
    "conf_clean, conf_pocket = [], []\n",
    "meta_clean, meta_pocket = [], []\n",
    "\n",
    "for max_iter in tqdm(perceptron_max_iter_values, desc=\"Evaluate Clean & Pocket\"):\n",
    "    # === Evaluate Clean PLA ===\n",
    "    c_model = trained_models_clean[max_iter]\n",
    "    cm_c, acc_c, s_c, sp_c, rt_c, ex_c = evaluate_model(\n",
    "        c_model, X_test, y_test, classes=range(10), model_name=\"Clean PLA\"\n",
    "    )\n",
    "    accuracies_clean.append(acc_c)\n",
    "    runtimes_clean.append(rt_c)\n",
    "    sensitivities_clean.append(np.mean(s_c))\n",
    "    selectivities_clean.append(np.mean(sp_c))\n",
    "    conf_clean.append(cm_c)\n",
    "\n",
    "    cdict = {\n",
    "        \"max_iter\": max_iter,\n",
    "        \"accuracy\": acc_c,\n",
    "        \"runtime\": rt_c,\n",
    "        \"avg_sensitivity\": np.mean(s_c),\n",
    "        \"avg_selectivity\": np.mean(sp_c),\n",
    "        \"method\": \"Clean PLA\"\n",
    "    }\n",
    "    cdict.update(ex_c)\n",
    "    meta_clean.append(cdict)\n",
    "\n",
    "    # === Evaluate Pocket PLA ===\n",
    "    p_model = trained_models_pocket[max_iter]\n",
    "    cm_p, acc_p, s_p, sp_p, rt_p, ex_p = evaluate_model(\n",
    "        p_model, X_test, y_test, classes=range(10), model_name=\"Pocket PLA\"\n",
    "    )\n",
    "    accuracies_pocket.append(acc_p)\n",
    "    runtimes_pocket.append(rt_p)\n",
    "    sensitivities_pocket.append(np.mean(s_p))\n",
    "    selectivities_pocket.append(np.mean(sp_p))\n",
    "    conf_pocket.append(cm_p)\n",
    "\n",
    "    pdict = {\n",
    "        \"max_iter\": max_iter,\n",
    "        \"accuracy\": acc_p,\n",
    "        \"runtime\": rt_p,\n",
    "        \"avg_sensitivity\": np.mean(s_p),\n",
    "        \"avg_selectivity\": np.mean(sp_p),\n",
    "        \"method\": \"Pocket PLA\"\n",
    "    }\n",
    "    pdict.update(ex_p)\n",
    "    meta_pocket.append(pdict)\n",
    "\n",
    "# Aggregated iteration-level training curves for Perceptrons\n",
    "clean_train_curve = aggregate_iteration_losses(\n",
    "    [trained_models_clean[m] for m in perceptron_max_iter_values]\n",
    ")\n",
    "pocket_train_curve = aggregate_iteration_losses(\n",
    "    [trained_models_pocket[m] for m in perceptron_max_iter_values]\n",
    ")\n",
    "\n",
    "# 2) Evaluate Regression Models: Softmax & Linear\n",
    "accuracies_softmax = []\n",
    "runtimes_softmax   = []\n",
    "sensitivities_soft = []\n",
    "selectivities_soft = []\n",
    "conf_soft          = []\n",
    "meta_soft          = []\n",
    "\n",
    "accuracies_linear = []\n",
    "runtimes_linear   = []\n",
    "sensitivities_lin = []\n",
    "selectivities_lin = []\n",
    "conf_linear       = []\n",
    "meta_linear       = []\n",
    "\n",
    "for cfg in tqdm(regression_run_configs, desc=\"Evaluate Regressions\"):\n",
    "    lr_val = cfg[\"learning_rate\"]\n",
    "    max_iter_val = cfg[\"max_iter\"]\n",
    "    label = cfg[\"label\"]\n",
    "\n",
    "    # === Evaluate Softmax ===\n",
    "    s_model = trained_models_softmax[(lr_val, max_iter_val)]\n",
    "    cm_s, a_s, se_s, sp_s, r_s, ex_s = evaluate_model(\n",
    "        s_model, X_test, y_test, classes=range(10),\n",
    "        model_name=f\"Softmax ({label})\"\n",
    "    )\n",
    "    accuracies_softmax.append(a_s)\n",
    "    runtimes_softmax.append(r_s)\n",
    "    sensitivities_soft.append(np.mean(se_s))\n",
    "    selectivities_soft.append(np.mean(sp_s))\n",
    "    conf_soft.append(cm_s)\n",
    "\n",
    "    ms = {\n",
    "        \"label\": label,\n",
    "        \"learning_rate\": lr_val,\n",
    "        \"max_iter\": max_iter_val,\n",
    "        \"accuracy\": a_s,\n",
    "        \"runtime\": r_s,\n",
    "        \"avg_sensitivity\": np.mean(se_s),\n",
    "        \"avg_selectivity\": np.mean(sp_s),\n",
    "        \"method\": \"Softmax\"\n",
    "    }\n",
    "    ms.update(ex_s)\n",
    "    meta_soft.append(ms)\n",
    "\n",
    "    # === Evaluate Linear ===\n",
    "    lin_model = trained_models_linear[(lr_val, max_iter_val)]\n",
    "    cm_l, a_l, se_l, sp_l, r_l, ex_l = evaluate_model(\n",
    "        lin_model, X_test, y_test, classes=range(10),\n",
    "        model_name=f\"Linear ({label})\"\n",
    "    )\n",
    "    accuracies_linear.append(a_l)\n",
    "    runtimes_linear.append(r_l)\n",
    "    sensitivities_lin.append(np.mean(se_l))\n",
    "    selectivities_lin.append(np.mean(sp_l))\n",
    "    conf_linear.append(cm_l)\n",
    "\n",
    "    ml = {\n",
    "        \"label\": label,\n",
    "        \"learning_rate\": lr_val,\n",
    "        \"max_iter\": max_iter_val,\n",
    "        \"accuracy\": a_l,\n",
    "        \"runtime\": r_l,\n",
    "        \"avg_sensitivity\": np.mean(se_l),\n",
    "        \"avg_selectivity\": np.mean(sp_l),\n",
    "        \"method\": \"Linear Regression\"\n",
    "    }\n",
    "    ml.update(ex_l)\n",
    "    meta_linear.append(ml)\n",
    "\n",
    "\n",
    "logger.info(\"Evaluation complete for Perceptrons & Regressions.\")\n",
    "\n",
    "\n",
    "# 1) Build the DataFrame of all model results\n",
    "all_rows = []\n",
    "\n",
    "# A) Clean PLA\n",
    "for i, max_iter in tqdm(\n",
    "    enumerate(perceptron_max_iter_values),\n",
    "    desc=\"Collecting Clean PLA\",\n",
    "    total=len(perceptron_max_iter_values)\n",
    "):\n",
    "    all_rows.append({\n",
    "        'model': 'Clean PLA',\n",
    "        'max_iter': max_iter,\n",
    "        'runtime': runtimes_clean[i],\n",
    "        'accuracy': accuracies_clean[i],\n",
    "        'sensitivity': sensitivities_clean[i],\n",
    "        'selectivity': selectivities_clean[i]\n",
    "    })\n",
    "\n",
    "# B) Pocket PLA\n",
    "for i, max_iter in tqdm(\n",
    "    enumerate(perceptron_max_iter_values),\n",
    "    desc=\"Collecting Pocket PLA\",\n",
    "    total=len(perceptron_max_iter_values)\n",
    "):\n",
    "    all_rows.append({\n",
    "        'model': 'Pocket PLA',\n",
    "        'max_iter': max_iter,\n",
    "        'runtime': runtimes_pocket[i],\n",
    "        'accuracy': accuracies_pocket[i],\n",
    "        'sensitivity': sensitivities_pocket[i],\n",
    "        'selectivity': selectivities_pocket[i]\n",
    "    })\n",
    "\n",
    "# C) Softmax\n",
    "for i, row_meta in tqdm(\n",
    "    enumerate(meta_soft),\n",
    "    desc=\"Collecting Softmax\",\n",
    "    total=len(meta_soft)\n",
    "):\n",
    "    all_rows.append({\n",
    "        'model': 'Softmax',\n",
    "        'max_iter': row_meta['max_iter'],\n",
    "        'runtime': runtimes_softmax[i],\n",
    "        'accuracy': accuracies_softmax[i],\n",
    "        'sensitivity': sensitivities_soft[i],\n",
    "        'selectivity': selectivities_soft[i]\n",
    "    })\n",
    "\n",
    "# D) Linear\n",
    "for i, row_meta in tqdm(\n",
    "    enumerate(meta_linear),\n",
    "    desc=\"Collecting Linear\",\n",
    "    total=len(meta_linear)\n",
    "):\n",
    "    all_rows.append({\n",
    "        'model': 'Linear',\n",
    "        'max_iter': row_meta['max_iter'],\n",
    "        'runtime': runtimes_linear[i],\n",
    "        'accuracy': accuracies_linear[i],\n",
    "        'sensitivity': sensitivities_lin[i],\n",
    "        'selectivity': selectivities_lin[i]\n",
    "    })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwCAybO5owmg"
   },
   "source": [
    "# Visualize (Generate Plots, Confusion Matricies, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "440bffb0f7a04811af436629cd0ba10b",
      "4fb5b6045eea41d28f6edd25ee9cda06",
      "14726988043243fe99eca723da4a4099",
      "0cbd05a3887e484495d3bb11896237cb",
      "1e05e47a4a15421d8822e57577ae609d",
      "60e3d573c1e044318237451c7a0c8aad",
      "1e3e22c7284f4743915c18de95cd64a0",
      "0b3f9af806324753adc050479fb336fb",
      "d9a0a5aeb76549fea17a3d1ce8468f17",
      "079a8fb14ad24640a8fac4d3e12263a9",
      "0859ad0519ff4d8285c31174d5e2e4e9",
      "264b028a039f4591a2aaa1ab15a7d470",
      "99c08ac44f434eaab2f109f89a946671",
      "724fbde6b9d64afa991048e89c290923",
      "859e65466a714a238ed59c9068883b8f",
      "8a50c7faad2e43f9974634d68e5b31cf",
      "92f1b404736e45cfb27e4f15c447c906",
      "696bff6b7269418a8a7b70755fb91f6c",
      "b8573fb12e8d4d8791893b10805f3477",
      "e757ad3f3ba941d69b6086f357c7d199",
      "3b8b5201eadd481b992566b28276f101",
      "f0b3d7a4b49b40739c94c2bd0f3d920f"
     ]
    },
    "id": "rC4vaIjVowmg",
    "outputId": "fcfc389b-51f4-48da-db04-b2b7ea12759f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "##################################################\n",
    "# 1) CREATE A SINGLE PANDAS DATAFRAME FOR ALL RESULTS\n",
    "##################################################\n",
    "all_rows = []\n",
    "\n",
    "# (A) Clean PLA\n",
    "for i, max_iter in tqdm(\n",
    "    enumerate(perceptron_max_iter_values),\n",
    "    desc=\"Collecting Clean PLA\",\n",
    "    total=len(perceptron_max_iter_values)\n",
    "):\n",
    "    all_rows.append({\n",
    "        'model': 'Clean PLA',\n",
    "        'max_iter': max_iter,\n",
    "        'runtime': runtimes_clean[i],\n",
    "        'accuracy': accuracies_clean[i],\n",
    "        'sensitivity': sensitivities_clean[i],\n",
    "        'selectivity': selectivities_clean[i]\n",
    "    })\n",
    "\n",
    "# (B) Pocket PLA\n",
    "for i, max_iter in tqdm(\n",
    "    enumerate(perceptron_max_iter_values),\n",
    "    desc=\"Collecting Pocket PLA\",\n",
    "    total=len(perceptron_max_iter_values)\n",
    "):\n",
    "    all_rows.append({\n",
    "        'model': 'Pocket PLA',\n",
    "        'max_iter': max_iter,\n",
    "        'runtime': runtimes_pocket[i],\n",
    "        'accuracy': accuracies_pocket[i],\n",
    "        'sensitivity': sensitivities_pocket[i],\n",
    "        'selectivity': selectivities_pocket[i]\n",
    "    })\n",
    "\n",
    "# (C) Softmax\n",
    "for i, row_meta in tqdm(\n",
    "    enumerate(meta_soft),\n",
    "    desc=\"Collecting Softmax\",\n",
    "    total=len(meta_soft)\n",
    "):\n",
    "    all_rows.append({\n",
    "        'model': 'Softmax',\n",
    "        'max_iter': row_meta['max_iter'],\n",
    "        'runtime': runtimes_softmax[i],\n",
    "        'accuracy': accuracies_softmax[i],\n",
    "        'sensitivity': sensitivities_soft[i],\n",
    "        'selectivity': selectivities_soft[i]\n",
    "    })\n",
    "\n",
    "# (D) Linear\n",
    "for i, row_meta in tqdm(\n",
    "    enumerate(meta_linear),\n",
    "    desc=\"Collecting Linear\",\n",
    "    total=len(meta_linear)\n",
    "):\n",
    "    all_rows.append({\n",
    "        'model': 'Linear',\n",
    "        'max_iter': row_meta['max_iter'],\n",
    "        'runtime': runtimes_linear[i],\n",
    "        'accuracy': accuracies_linear[i],\n",
    "        'sensitivity': sensitivities_lin[i],\n",
    "        'selectivity': selectivities_lin[i]\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(all_rows)\n",
    "logger.info(\"Combined Results DataFrame:\\n%s\", df_results)\n",
    "display(df_results.head(20))\n",
    "\n",
    "############################################################################\n",
    "# 2) CONFUSION MATRICES FOR ALL MODELS (GROUPED BY PLOT TYPE)\n",
    "############################################################################\n",
    "\n",
    "logger.info(\"=== Plotting ALL Confusion Matrices ===\")\n",
    "\n",
    "# 2A) Perceptron: Clean\n",
    "for idx, meta in tqdm(enumerate(meta_clean), total=len(meta_clean), desc=\"Confusions: Clean PLA\"):\n",
    "    title = f\"Clean PLA (max_iter={meta['max_iter']}, Acc={meta['accuracy']*100:.2f}%)\"\n",
    "    plot_confusion_matrix_annotated(\n",
    "        conf_clean[idx],\n",
    "        classes=range(10),\n",
    "        title=title,\n",
    "        method=meta[\"method\"],\n",
    "        max_iter=meta[\"max_iter\"]\n",
    "    )\n",
    "\n",
    "# 2B) Perceptron: Pocket\n",
    "for idx, meta in tqdm(enumerate(meta_pocket), total=len(meta_pocket), desc=\"Confusions: Pocket PLA\"):\n",
    "    title = f\"Pocket PLA (max_iter={meta['max_iter']}, Acc={meta['accuracy']*100:.2f}%)\"\n",
    "    plot_confusion_matrix_annotated(\n",
    "        conf_pocket[idx],\n",
    "        classes=range(10),\n",
    "        title=title,\n",
    "        method=meta[\"method\"],\n",
    "        max_iter=meta[\"max_iter\"]\n",
    "    )\n",
    "\n",
    "# 2C) Softmax\n",
    "for idx, meta in tqdm(enumerate(meta_soft), total=len(meta_soft), desc=\"Confusions: Softmax\"):\n",
    "    title = f\"Softmax ({meta['label']}, Acc={meta['accuracy']*100:.2f}%)\"\n",
    "    plot_confusion_matrix_annotated(\n",
    "        conf_soft[idx],\n",
    "        classes=range(10),\n",
    "        title=title,\n",
    "        method=meta[\"method\"],\n",
    "        max_iter=meta[\"max_iter\"]\n",
    "    )\n",
    "\n",
    "# 2D) Linear\n",
    "for idx, meta in tqdm(enumerate(meta_linear), total=len(meta_linear), desc=\"Confusions: Linear\"):\n",
    "    title = f\"Linear ({meta['label']}, Acc={meta['accuracy']*100:.2f}%)\"\n",
    "    plot_confusion_matrix_annotated(\n",
    "        conf_linear[idx],\n",
    "        classes=range(10),\n",
    "        title=title,\n",
    "        method=meta[\"method\"],\n",
    "        max_iter=meta[\"max_iter\"]\n",
    "    )\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# 3) ITERATION-LEVEL PLOTS (ALL MODELS)\n",
    "############################################################################\n",
    "\n",
    "logger.info(\"=== Iteration-Level Visualization (All Models) ===\")\n",
    "\n",
    "# 3A) Perceptron: Clean & Pocket\n",
    "for max_iter, c_model in trained_models_clean.items():\n",
    "    df_iter = c_model.get_iteration_df()\n",
    "    if not df_iter.empty and \"train_error\" in df_iter.columns:\n",
    "        title = f\"Clean PLA max_iter={max_iter}: Train Error vs. Iteration\"\n",
    "        df_iter.plot(x=\"iteration\", y=\"train_error\", marker='o', figsize=(8,5), title=title)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "for max_iter, p_model in trained_models_pocket.items():\n",
    "    df_iter = p_model.get_iteration_df()\n",
    "    if not df_iter.empty and \"train_error\" in df_iter.columns:\n",
    "        title = f\"Pocket PLA max_iter={max_iter}: Train Error vs. Iteration\"\n",
    "        df_iter.plot(x=\"iteration\", y=\"train_error\", marker='o', figsize=(8,5), title=title)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "# 3B) Softmax\n",
    "for (lr_val, max_iter_val), s_model in trained_models_softmax.items():\n",
    "    df_iter = s_model.get_iteration_df()  # Must be implemented in your SoftmaxRegression\n",
    "    if not df_iter.empty:\n",
    "        title = f\"Softmax LR={lr_val}, max_iter={max_iter_val}: Train Loss vs. Iteration\"\n",
    "        df_iter.plot(x=\"iteration\", y=\"train_loss\", marker='o', figsize=(8,5), title=title)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "        if \"test_loss\" in df_iter.columns:\n",
    "            title = f\"Softmax LR={lr_val}, max_iter={max_iter_val}: Train & Test Loss\"\n",
    "            df_iter.plot(x=\"iteration\", y=[\"train_loss\",\"test_loss\"], marker='o', figsize=(8,5), title=title)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.show()\n",
    "\n",
    "        if \"avg_adaptive_lr\" in df_iter.columns:\n",
    "            title = f\"Softmax LR={lr_val}, max_iter={max_iter_val}: Avg Adaptive LR vs. Iteration\"\n",
    "            df_iter.plot(x=\"iteration\", y=\"avg_adaptive_lr\", marker='x', figsize=(8,5), title=title)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.show()\n",
    "\n",
    "# 3C) Linear\n",
    "for (lr_val, max_iter_val), lin_model in trained_models_linear.items():\n",
    "    df_iter = lin_model.get_iteration_df()  # Must be implemented in your LinearRegression\n",
    "    if not df_iter.empty:\n",
    "        title = f\"Linear LR={lr_val}, max_iter={max_iter_val}: Train Loss vs. Iteration\"\n",
    "        df_iter.plot(x=\"iteration\", y=\"train_loss\", marker='o', figsize=(8,5), title=title)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "        if \"test_loss\" in df_iter.columns:\n",
    "            title = f\"Linear LR={lr_val}, max_iter={max_iter_val}: Train & Test Loss\"\n",
    "            df_iter.plot(x=\"iteration\", y=[\"train_loss\",\"test_loss\"], marker='o', figsize=(8,5), title=title)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.show()\n",
    "\n",
    "        if \"avg_adaptive_lr\" in df_iter.columns:\n",
    "            title = f\"Linear LR={lr_val}, max_iter={max_iter_val}: Avg Adaptive LR vs. Iteration\"\n",
    "            df_iter.plot(x=\"iteration\", y=\"avg_adaptive_lr\", marker='x', figsize=(8,5), title=title)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# 4) PANDAS + SEABORN PLOTS\n",
    "############################################################################\n",
    "\n",
    "logger.info(\"=== Pandas + Seaborn Plots ===\")\n",
    "\n",
    "# 4A) LINE PLOT: Accuracy vs. max_iter (Perceptrons Only)\n",
    "df_perc = df_results[df_results['model'].isin(['Clean PLA','Pocket PLA'])].copy()\n",
    "df_perc.sort_values(['model','max_iter'], inplace=True)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.lineplot(\n",
    "    data=df_perc,\n",
    "    x='max_iter', y='accuracy',\n",
    "    hue='model', marker='o'\n",
    ")\n",
    "plt.title(\"Perceptrons: Accuracy vs. max_iter (Pandas/Seaborn)\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# 4B) BAR CHART: Average Accuracy by Model\n",
    "df_mean = df_results.groupby('model', as_index=False)['accuracy'].mean()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(data=df_mean, x='model', y='accuracy')\n",
    "plt.title(\"Average Accuracy by Model (Pandas/Seaborn)\")\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# 4C) SCATTER PLOT: Accuracy vs. Runtime, colored by model\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(\n",
    "    data=df_results,\n",
    "    x='runtime', y='accuracy',\n",
    "    hue='model', style='model',\n",
    "    s=100\n",
    ")\n",
    "plt.title(\"Accuracy vs. Runtime (All Models) (Pandas/Seaborn)\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# 5) CUSTOM SUMMARY PLOTS (AGGREGATED CURVES, ETC.)\n",
    "############################################################################\n",
    "\n",
    "logger.info(\"=== Custom Summaries (Aggregated Curves, etc.) ===\")\n",
    "\n",
    "# 5A) Aggregated Perceptron Curves\n",
    "plot_train_curves_three_models(\n",
    "    clean_train_curve=clean_train_curve,\n",
    "    pocket_train_curve=pocket_train_curve,\n",
    "    softmax_train_curve=None,  # no Softmax aggregator\n",
    "    title=\"Aggregated Perceptron Train Curves (Clean vs. Pocket)\",\n",
    "    max_iter=perceptron_max_iter_values[-1]\n",
    ")\n",
    "\n",
    "# 5B) Summaries for Perceptron\n",
    "plot_accuracy_vs_max_iter(\n",
    "    max_iter_values=perceptron_max_iter_values,\n",
    "    accuracies_clean=accuracies_clean,\n",
    "    accuracies_pocket=accuracies_pocket,\n",
    "    accuracies_softmax=None\n",
    ")\n",
    "\n",
    "plot_runtime_vs_max_iter(\n",
    "    max_iter_values=perceptron_max_iter_values,\n",
    "    runtimes_clean=runtimes_clean,\n",
    "    runtimes_pocket=runtimes_pocket,\n",
    "    runtimes_softmax=None\n",
    ")\n",
    "\n",
    "plot_accuracy_vs_runtime(\n",
    "    runtimes_clean=runtimes_clean,\n",
    "    accuracies_clean=accuracies_clean,\n",
    "    runtimes_pocket=runtimes_pocket,\n",
    "    accuracies_pocket=accuracies_pocket,\n",
    "    title=\"Perceptrons: Accuracy vs. Runtime\"\n",
    ")\n",
    "\n",
    "plot_performance_summary_extended_by_runtime(\n",
    "    runtimes_clean=runtimes_clean,\n",
    "    accuracies_clean=accuracies_clean,\n",
    "    sensitivities_clean=sensitivities_clean,\n",
    "    selectivities_clean=selectivities_clean,\n",
    "    runtimes_pocket=runtimes_pocket,\n",
    "    accuracies_pocket=accuracies_pocket,\n",
    "    sensitivities_pocket=sensitivities_pocket,\n",
    "    selectivities_pocket=selectivities_pocket,\n",
    "    title=\"Perceptrons: Performance vs. Runtime\"\n",
    ")\n",
    "\n",
    "# 5C) Summaries for Softmax & Linear\n",
    "plot_accuracy_vs_runtime(\n",
    "    runtimes_clean=runtimes_softmax,\n",
    "    accuracies_clean=accuracies_softmax,\n",
    "    title=\"Softmax: Accuracy vs. Runtime\"\n",
    ")\n",
    "plot_accuracy_vs_runtime(\n",
    "    runtimes_clean=runtimes_linear,\n",
    "    accuracies_clean=accuracies_linear,\n",
    "    title=\"Linear: Accuracy vs. Runtime\"\n",
    ")\n",
    "plot_accuracy_vs_runtime(\n",
    "    runtimes_clean=runtimes_softmax,\n",
    "    accuracies_clean=accuracies_softmax,\n",
    "    runtimes_pocket=runtimes_linear,\n",
    "    accuracies_pocket=accuracies_linear,\n",
    "    title=\"Softmax vs. Linear: Accuracy vs. Runtime\"\n",
    ")\n",
    "plot_performance_summary_extended_by_runtime(\n",
    "    runtimes_clean=runtimes_softmax,\n",
    "    accuracies_clean=accuracies_softmax,\n",
    "    sensitivities_clean=sensitivities_soft,\n",
    "    selectivities_clean=selectivities_soft,\n",
    "    runtimes_pocket=runtimes_linear,\n",
    "    accuracies_pocket=accuracies_linear,\n",
    "    sensitivities_pocket=sensitivities_lin,\n",
    "    selectivities_pocket=selectivities_lin,\n",
    "    title=\"Softmax vs. Linear: TPR/TNR vs. Runtime\"\n",
    ")\n",
    "\n",
    "# 5D) 4-Model Comparison\n",
    "plot_performance_summary_4models_by_runtime(\n",
    "    runtimes_clean, accuracies_clean, sensitivities_clean, selectivities_clean,\n",
    "    runtimes_pocket, accuracies_pocket, sensitivities_pocket, selectivities_pocket,\n",
    "    runtimes_softmax, accuracies_softmax, sensitivities_soft, selectivities_soft,\n",
    "    runtimes_linear, accuracies_linear, sensitivities_lin, selectivities_lin,\n",
    "    title=\"Performance vs. Runtime (4-Model Comparison)\"\n",
    ")\n",
    "\n",
    "plot_accuracy_vs_runtime_4models(\n",
    "    rt_clean=runtimes_clean,\n",
    "    acc_clean=accuracies_clean,\n",
    "    rt_pocket=runtimes_pocket,\n",
    "    acc_pocket=accuracies_pocket,\n",
    "    rt_softmax=runtimes_softmax,\n",
    "    acc_softmax=accuracies_softmax,\n",
    "    rt_linear=runtimes_linear,\n",
    "    acc_linear=accuracies_linear,\n",
    "    title=\"Accuracy vs. Runtime (4 Models)\"\n",
    ")\n",
    "\n",
    "logger.info(\"=== All Visualizations Complete ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mRmCfTiHxuc"
   },
   "source": [
    "# Final Results Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSf_XT9J3Km1"
   },
   "source": [
    "\n",
    "**Observations:**\n",
    "- **Pocket PLA** consistently outperforms Clean PLA in both accuracy and sensitivity (TPR) across all tested iteration counts.\n",
    "- Increasing `max_iter` improves performance, though gains tend to plateau beyond roughly 50–100 iterations.\n",
    "- **Runtime** increases nearly linearly with `max_iter` for both methods, highlighting a clear trade-off between higher accuracy and computational cost.\n",
    "- Perfect linear separation is not achieved—even at higher iteration counts, neither method reaches 100% accuracy, indicating that the dataset is not strictly linearly separable.\n",
    "\n",
    "**Trade-off Analysis:**\n",
    "- **Low Iterations (max_iter = 10–30):**  \n",
    "  Fast training with modest accuracy and TPR, suitable for rapid prototyping or time-sensitive applications.\n",
    "- **Medium Iterations (max_iter = 50–100):**  \n",
    "  Balanced performance and runtime, capturing most achievable gains without excessive overhead.\n",
    "- **High Iterations (max_iter > 100):**  \n",
    "  Marginal performance improvements with significant runtime increase; diminishing returns for practical applications.\n",
    "\n",
    "**Recommendations for Future Work:**\n",
    "- Experiment with alternative update rules (e.g., adaptive learning rates) to accelerate convergence.\n",
    "- Compare against more sophisticated models (e.g., Logistic Regression, SVMs, neural networks) for broader insights.\n",
    "- Evaluate model robustness under noisy or adversarial conditions.\n",
    "\n",
    "This comprehensive analysis—including confusion matrices, error curves, and summary plots—provides detailed insights into the performance of the multi-class Perceptron on MNIST and informs the optimal balance between training efficiency and classification performance.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "079a8fb14ad24640a8fac4d3e12263a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0859ad0519ff4d8285c31174d5e2e4e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b3f9af806324753adc050479fb336fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cbd05a3887e484495d3bb11896237cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_079a8fb14ad24640a8fac4d3e12263a9",
      "placeholder": "​",
      "style": "IPY_MODEL_0859ad0519ff4d8285c31174d5e2e4e9",
      "value": " 4/4 [00:01&lt;00:00,  2.13it/s]"
     }
    },
    "14726988043243fe99eca723da4a4099": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b3f9af806324753adc050479fb336fb",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d9a0a5aeb76549fea17a3d1ce8468f17",
      "value": 4
     }
    },
    "1e05e47a4a15421d8822e57577ae609d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e3e22c7284f4743915c18de95cd64a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "264b028a039f4591a2aaa1ab15a7d470": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_99c08ac44f434eaab2f109f89a946671",
       "IPY_MODEL_724fbde6b9d64afa991048e89c290923",
       "IPY_MODEL_859e65466a714a238ed59c9068883b8f"
      ],
      "layout": "IPY_MODEL_8a50c7faad2e43f9974634d68e5b31cf"
     }
    },
    "3b66d8c07f8540998106f3e1d30fcf0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b8b5201eadd481b992566b28276f101": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "440bffb0f7a04811af436629cd0ba10b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4fb5b6045eea41d28f6edd25ee9cda06",
       "IPY_MODEL_14726988043243fe99eca723da4a4099",
       "IPY_MODEL_0cbd05a3887e484495d3bb11896237cb"
      ],
      "layout": "IPY_MODEL_1e05e47a4a15421d8822e57577ae609d"
     }
    },
    "4fb5b6045eea41d28f6edd25ee9cda06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60e3d573c1e044318237451c7a0c8aad",
      "placeholder": "​",
      "style": "IPY_MODEL_1e3e22c7284f4743915c18de95cd64a0",
      "value": "Plotting Clean Confusion Matrices: 100%"
     }
    },
    "60e3d573c1e044318237451c7a0c8aad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "696bff6b7269418a8a7b70755fb91f6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "724fbde6b9d64afa991048e89c290923": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8573fb12e8d4d8791893b10805f3477",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e757ad3f3ba941d69b6086f357c7d199",
      "value": 4
     }
    },
    "7eb0fc2ebb6449e2888386391ca55d70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f52eb9ff71ec48feb8b09d0d80228c58",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fba5d1c8e57c44adb12faeb482e60424",
      "value": 4
     }
    },
    "859e65466a714a238ed59c9068883b8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b8b5201eadd481b992566b28276f101",
      "placeholder": "​",
      "style": "IPY_MODEL_f0b3d7a4b49b40739c94c2bd0f3d920f",
      "value": " 4/4 [00:01&lt;00:00,  2.07it/s]"
     }
    },
    "8a50c7faad2e43f9974634d68e5b31cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92f1b404736e45cfb27e4f15c447c906": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99a9e2e08aa046a49b6332e2b054416b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99c08ac44f434eaab2f109f89a946671": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92f1b404736e45cfb27e4f15c447c906",
      "placeholder": "​",
      "style": "IPY_MODEL_696bff6b7269418a8a7b70755fb91f6c",
      "value": "Plotting Pocket Confusion Matrices: 100%"
     }
    },
    "ae3831170c414a1eaf29434c4aaec771": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8573fb12e8d4d8791893b10805f3477": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2b28605e3b44cc7846a45c039351958": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c88135114994459b8babe54b307dd75b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99a9e2e08aa046a49b6332e2b054416b",
      "placeholder": "​",
      "style": "IPY_MODEL_e371b2dc55164dcfab419c03cd2e1e25",
      "value": " 4/4 [36:04&lt;00:00, 794.90s/it]"
     }
    },
    "d8ddbe8dea3d4f7f99361f9eb81f2ab0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae3831170c414a1eaf29434c4aaec771",
      "placeholder": "​",
      "style": "IPY_MODEL_3b66d8c07f8540998106f3e1d30fcf0c",
      "value": "Training Models: 100%"
     }
    },
    "d9a0a5aeb76549fea17a3d1ce8468f17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e371b2dc55164dcfab419c03cd2e1e25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e637ccdffee5428897190964713e4df0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d8ddbe8dea3d4f7f99361f9eb81f2ab0",
       "IPY_MODEL_7eb0fc2ebb6449e2888386391ca55d70",
       "IPY_MODEL_c88135114994459b8babe54b307dd75b"
      ],
      "layout": "IPY_MODEL_c2b28605e3b44cc7846a45c039351958"
     }
    },
    "e757ad3f3ba941d69b6086f357c7d199": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f0b3d7a4b49b40739c94c2bd0f3d920f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f52eb9ff71ec48feb8b09d0d80228c58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fba5d1c8e57c44adb12faeb482e60424": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
