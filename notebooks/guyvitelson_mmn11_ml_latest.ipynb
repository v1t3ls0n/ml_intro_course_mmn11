{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/v1t3ls0n/ml_intro_course_mmn11/blob/main/notebooks/guyvitelson_mmn11_ml_latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ztlf-kRcUjIz"
   },
   "source": [
    "# ממן 11 - מבוא ללמידה חישובית - סמסטר 2025ב - גיא ויטלזון 203379706"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXiZOQmO5Tak"
   },
   "source": [
    "##**If you run this within Google Collab, Dont Worry!**\n",
    "all the missing python files/directories/modules will be automatically feteched from my github repository\n",
    "\n",
    "**My GitHub Profile** : https://github.com/v1t3ls0n\n",
    "\n",
    "**The Repository:** https://github.com/v1t3ls0n/ml_intro_course_mmn11\n",
    "\n",
    "**Student ID:** 203379706"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqLIUat1dEr2"
   },
   "source": [
    "## Fetch Resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmauUwgLR-mx"
   },
   "source": [
    "### External Code Imports (pip packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xUkaAHQFR-mx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import seaborn as sns # type: ignore\n",
    "import time\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlMUAQraR-my"
   },
   "source": [
    "### Fetch Missing Files For Google Colab Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "msKnbktXR-my"
   },
   "outputs": [],
   "source": [
    "\n",
    "# %%capture run_output\n",
    "# %matplotlib inline\n",
    "\n",
    "if sys.platform != 'win32': # check if we are running on google collab\n",
    "  repo_url = \"https://github.com/v1t3ls0n/ml_intro_course_mmn11\"\n",
    "  repo_name = \"ml_intro_course_mmn11\"\n",
    "  from tqdm.notebook import tqdm # type: ignore\n",
    "\n",
    "\n",
    "  # Clone the repository if it doesn't exist\n",
    "  if not os.path.exists(repo_name):\n",
    "    os.system(f\"git clone {repo_url}\")\n",
    "\n",
    "  # Construct the path to the repository directory\n",
    "  repo_path = os.path.join(os.getcwd(), repo_name)\n",
    "\n",
    "  # Add the repository directory to the Python path\n",
    "  if repo_path not in sys.path:\n",
    "    sys.path.insert(0, repo_path)\n",
    "\n",
    "  # --- Extract 'core' and 'notebooks' directories ---\n",
    "  def extract_directories(source_dir, destination_dir, dir_names):\n",
    "      for dir_name in dir_names:\n",
    "          source_path = os.path.join(source_dir, dir_name)\n",
    "          destination_path = os.path.join(destination_dir, dir_name)\n",
    "          if os.path.exists(source_path):\n",
    "              shutil.copytree(source_path, destination_path, dirs_exist_ok=True)\n",
    "\n",
    "  destination_path = \".\"\n",
    "  # Extract the directories\n",
    "  extract_directories(repo_path, destination_path, [\"core\"])\n",
    "  project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "  sys.path.insert(0, project_root)\n",
    "  if os.path.exists(\"ml_intro_course_mmn11\"):\n",
    "    shutil.rmtree(\"ml_intro_course_mmn11\")\n",
    "  if os.path.exists(\"sample_data\"):\n",
    "    shutil.rmtree(\"sample_data\")\n",
    "else:\n",
    "  from tqdm import tqdm  # type: ignore\n",
    "  current_dir = os.getcwd()  # Current working directory\n",
    "  project_root = os.path.abspath(os.path.join(current_dir, '..'))  # Root directory of the project\n",
    "  sys.path.insert(0, project_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYrRE0dcR-my"
   },
   "source": [
    "### Internal Code Imports (original code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VTaL_MsqeE00"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ========== Internal Code Imports ==========\n",
    "\n",
    "#Logger\n",
    "from core.logger.config import logger\n",
    "\n",
    "# Data Preprocessing\n",
    "from core.data.mnist_loader import load_mnist\n",
    "from core.data.data_preprocessing import preprocess_data\n",
    "\n",
    "# Models\n",
    "from core.models.perceptron.multi_class_perceptron import MultiClassPerceptron\n",
    "from core.models.logistic_regression.softmax_lregression import SoftmaxRegression\n",
    "from core.models.linear_regression.linear_regression import  LinearRegression\n",
    "\n",
    "# Performance & Plotting\n",
    "from core.analysis.evaluation_functions import (\n",
    "    evaluate_model,\n",
    "    aggregate_iteration_losses,\n",
    "    aggregate_iteration_losses_softmax\n",
    ")\n",
    "\n",
    "from core.analysis.plotting import (\n",
    "    plot_confusion_matrix_annotated,\n",
    "    plot_error_curves,\n",
    "    plot_accuracy_vs_max_iter,\n",
    "    plot_runtime_vs_max_iter,\n",
    "    plot_performance_summary_extended,\n",
    "    plot_train_curves_three_models,\n",
    "    plot_metric_vs_learning_rate,\n",
    "    plot_accuracy_vs_max_iter_4models,\n",
    "    plot_runtime_vs_max_iter_4models,\n",
    "    plot_accuracy_vs_runtime,\n",
    "    plot_performance_summary_extended_by_runtime,\n",
    "    plot_performance_summary_4models_by_runtime,\n",
    "    plot_accuracy_vs_runtime_4models\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"MyGlobalLogger\") # configured in core/logger/config.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKs0r5ROHxuY"
   },
   "source": [
    "# Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWMyo4jnR-mx"
   },
   "source": [
    "## MNIST Digit Classification Report\n",
    "\n",
    "### Approach\n",
    "\n",
    "#### Data Preprocessing\n",
    "The MNIST dataset was prepared by:\n",
    "- Splitting into training (60,000 samples) and test sets (10,000 samples).\n",
    "- Normalizing pixel values to the [0,1] range.\n",
    "- Flattening images into vectors (784 pixels plus 1 bias term).\n",
    "- Encoding labels into one-hot vectors.\n",
    "\n",
    "#### Model Implementation\n",
    "- **Multi-Class Perceptron:**\n",
    "  - One-vs-all strategy implemented with standard Perceptron and Pocket Perceptron algorithms.\n",
    "- **Softmax Regression:**\n",
    "  - Implemented using cross-entropy loss and adaptive learning rates (AdaGrad).\n",
    "  - Included early stopping based on loss improvement.\n",
    "- **Linear Regression:**\n",
    "  - Utilized mean squared error loss with gradient descent.\n",
    "  - AdaGrad adaptive learning rate and early stopping were applied.\n",
    "\n",
    "### Results\n",
    "\n",
    "- **Accuracy:**\n",
    "  - Softmax Regression achieved the highest accuracy.\n",
    "  - Multi-class Pocket Perceptron showed good performance, surpassing standard Perceptron.\n",
    "  - Linear Regression exhibited relatively lower accuracy due to its limitations for classification tasks.\n",
    "\n",
    "#### Confusion Matrices and Metrics\n",
    "- Softmax Regression demonstrated the lowest misclassification rates across digits.\n",
    "- Pocket Perceptron reduced errors compared to standard Perceptron, indicating improved robustness.\n",
    "- Sensitivity and accuracy clearly highlighted Softmax Regression as superior for multi-class digit classification.\n",
    "\n",
    "### Discussion\n",
    "- Softmax Regression proved best for digit classification, providing reliable probability estimations and stable convergence.\n",
    "- Pocket Perceptron algorithm offered notable improvements over standard Perceptron, highlighting its utility in non-linearly separable scenarios.\n",
    "- Linear Regression's limitations in classification tasks were evident, reaffirming theoretical expectations.\n",
    "\n",
    "### Conclusions\n",
    "- Softmax Regression is the most suitable algorithm for multi-class digit recognition problems.\n",
    "- Pocket Perceptron serves as an effective alternative, offering a balance between simplicity and performance.\n",
    "- Linear Regression, while straightforward, is suboptimal for classification due to its inherent limitations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keDSGERzwvrB"
   },
   "source": [
    "# Choose Run Parameters **(Significant Effect On Model's Runtime!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tNJHO7mQrANq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - === Perceptron Run Parameters ===\n",
      "INFO - max_iter_values = [50, 100, 2000, 3000]\n",
      "INFO - === Regression Run Parameters ===\n",
      "INFO - LR=0.1/Iter=50 -> learning_rate=0.1, max_iter=50\n",
      "INFO - LR=0.1/Iter=100 -> learning_rate=0.1, max_iter=100\n",
      "INFO - LR=0.1/Iter=2000 -> learning_rate=0.1, max_iter=2000\n",
      "INFO - LR=0.1/Iter=3000 -> learning_rate=0.1, max_iter=3000\n",
      "INFO - max_iter_values = [50, 100, 2000, 3000]\n",
      "INFO - === Regression Run Parameters ===\n",
      "INFO - LR=0.1/Iter=50 -> learning_rate=0.1, max_iter=50\n",
      "INFO - LR=0.1/Iter=100 -> learning_rate=0.1, max_iter=100\n",
      "INFO - LR=0.1/Iter=2000 -> learning_rate=0.1, max_iter=2000\n",
      "INFO - LR=0.1/Iter=3000 -> learning_rate=0.1, max_iter=3000\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# SEPARATE RUN PARAMETERS FOR PERCEPTRONS vs. REGRESSIONS\n",
    "#######################################################################\n",
    "\n",
    "# Perceptrons (Clean & Pocket) iteration-based run\n",
    "perceptron_max_iter_values = [50,100,2000,3000]  # for Clean PLA & Pocket PLA\n",
    "# Logging the run parameters\n",
    "logger.info(f\"=== Perceptron Run Parameters ===\")\n",
    "logger.info(f\"max_iter_values = {perceptron_max_iter_values}\")\n",
    "\n",
    "\n",
    "# Regression (Softmax & Linear) run parameters.\n",
    "learning_rates = [0.1]  # for Softm vax & Linear Regression\n",
    "iteration_counts = [50,100,2000,3000]\n",
    "regression_run_configs = [\n",
    "    {\n",
    "        \"label\": f\"LR={lr}/Iter={it}\",\n",
    "        \"learning_rate\": lr,\n",
    "        \"max_iter\": it\n",
    "    }\n",
    "    for lr in learning_rates\n",
    "    for it in iteration_counts\n",
    "]\n",
    "\n",
    "logger.info(f\"=== Regression Run Parameters ===\")\n",
    "for cfg in regression_run_configs:\n",
    "    logger.info(f\"{cfg['label']} -> learning_rate={cfg['learning_rate']}, max_iter={cfg['max_iter']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e73BoKY7cmJU"
   },
   "source": [
    "# Load and Preprocess the MNIST Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "osGLi3Hic5qW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Raw MNIST data shapes: X_raw: (70000, 784), y_raw: (70000,)\n",
      "INFO - Preprocessed shape: (70000, 785)\n",
      "INFO - Train set: X_train: (60000, 785), y_train: (60000,)\n",
      "INFO - Test set: X_test: (10000, 785), y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We'll load the MNIST dataset using our custom loader (`mnist_loader`) and then apply preprocessing (`data_preprocessing`).\n",
    "The preprocessing step normalizes each image to the range [0, 1] and adds a bias term, resulting in input samples with 785 features.\n",
    "This setup ensures that the training set contains 60,000 samples and the test set 10,000 samples, preparing the data for the subsequent classification tasks.\n",
    "'''\n",
    "\n",
    "# New section\n",
    "# Load raw MNIST data (X: images, y: labels)\n",
    "X_raw, y_raw = load_mnist()\n",
    "\n",
    "\n",
    "logger.info(\"Raw MNIST data shapes: X_raw: %s, y_raw: %s\", X_raw.shape, y_raw.shape)\n",
    "\n",
    "# Preprocess (normalize & add bias = True)\n",
    "X = preprocess_data(X_raw, add_bias=True, normalize=True)\n",
    "logger.info(\"Preprocessed shape: %s\", X.shape)\n",
    "\n",
    "# Split into train/test manually or with 60k/10k as the task suggests\n",
    "X_train, y_train = X[:60000], y_raw[:60000]\n",
    "X_test,  y_test  = X[60000:], y_raw[60000:]\n",
    "\n",
    "logger.info(\"Train set: X_train: %s, y_train: %s\", X_train.shape, y_train.shape)\n",
    "logger.info(\"Test set: X_test: %s, y_test: %s\", X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-O4hrMBCejtr"
   },
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sik1JDX6Hxub"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - === TRAINING REGRESSION MODELS (Softmax & Linear) ===\n",
      "Train Regressions:   0%|          | 0/4 [00:00<?, ?it/s]INFO - --- Softmax LR=0.1/Iter=50 ---\n",
      "Train Regressions:   0%|          | 0/4 [00:00<?, ?it/s]INFO - --- Softmax LR=0.1/Iter=50 ---\n",
      "INFO - Iter 1/50, Loss: 2.3446, Avg Adaptive LR: 14.144338\n",
      "INFO - Iter 11/50, Loss: 0.4314, Avg Adaptive LR: 3.019835\n",
      "INFO - Iter 21/50, Loss: 0.3745, Avg Adaptive LR: 3.001877\n",
      "INFO - Iter 31/50, Loss: 0.3500, Avg Adaptive LR: 2.999318\n",
      "INFO - Iter 41/50, Loss: 0.3354, Avg Adaptive LR: 2.998457\n",
      "INFO - SoftmaxRegression training completed in 2.01 seconds.\n",
      "INFO - --- Linear Regression LR=0.1/Iter=50 ---\n",
      "INFO - LinearRegressionClassifier training completed in 1.54 seconds.\n",
      "Train Regressions:  25%|██▌       | 1/4 [00:03<00:10,  3.55s/it]INFO - --- Softmax LR=0.1/Iter=100 ---\n",
      "INFO - Iter 1/100, Loss: 2.3862, Avg Adaptive LR: 14.077829\n",
      "INFO - Iter 11/100, Loss: 0.4332, Avg Adaptive LR: 3.181821\n",
      "INFO - Iter 21/100, Loss: 0.3769, Avg Adaptive LR: 3.174093\n",
      "INFO - Iter 31/100, Loss: 0.3522, Avg Adaptive LR: 3.172495\n",
      "INFO - Iter 41/100, Loss: 0.3366, Avg Adaptive LR: 3.171523\n",
      "INFO - Iter 51/100, Loss: 0.3254, Avg Adaptive LR: 3.170821\n",
      "INFO - Iter 61/100, Loss: 0.3168, Avg Adaptive LR: 3.170281\n",
      "INFO - Iter 71/100, Loss: 0.3099, Avg Adaptive LR: 3.169847\n",
      "INFO - Iter 81/100, Loss: 0.3042, Avg Adaptive LR: 3.169489\n",
      "INFO - Iter 91/100, Loss: 0.2994, Avg Adaptive LR: 3.169185\n",
      "INFO - SoftmaxRegression training completed in 3.90 seconds.\n",
      "INFO - --- Linear Regression LR=0.1/Iter=100 ---\n",
      "INFO - Iter 100/100, Loss: 0.5185, Gradient Norm: 13.5634, Avg Adaptive LR: 1.4043025216183762\n",
      "INFO - LinearRegressionClassifier training completed in 3.07 seconds.\n",
      "Train Regressions:  50%|█████     | 2/4 [00:10<00:11,  5.56s/it]INFO - --- Softmax LR=0.1/Iter=2000 ---\n",
      "INFO - Iter 1/2000, Loss: 2.3588, Avg Adaptive LR: 13.631132\n",
      "INFO - Iter 11/2000, Loss: 0.4321, Avg Adaptive LR: 3.051432\n",
      "INFO - Iter 21/2000, Loss: 0.3708, Avg Adaptive LR: 3.044953\n",
      "INFO - Iter 31/2000, Loss: 0.3484, Avg Adaptive LR: 3.043681\n",
      "INFO - Iter 41/2000, Loss: 0.3339, Avg Adaptive LR: 3.042849\n",
      "INFO - Iter 51/2000, Loss: 0.3233, Avg Adaptive LR: 3.042241\n",
      "INFO - Iter 61/2000, Loss: 0.3152, Avg Adaptive LR: 3.041768\n",
      "INFO - Iter 71/2000, Loss: 0.3086, Avg Adaptive LR: 3.041387\n",
      "INFO - Iter 81/2000, Loss: 0.3032, Avg Adaptive LR: 3.041070\n",
      "INFO - Iter 91/2000, Loss: 0.2986, Avg Adaptive LR: 3.040803\n",
      "INFO - Iter 101/2000, Loss: 0.2946, Avg Adaptive LR: 3.040572\n",
      "INFO - Iter 111/2000, Loss: 0.2912, Avg Adaptive LR: 3.040371\n",
      "INFO - Iter 121/2000, Loss: 0.2881, Avg Adaptive LR: 3.040193\n",
      "INFO - Iter 131/2000, Loss: 0.2854, Avg Adaptive LR: 3.040034\n",
      "INFO - Iter 141/2000, Loss: 0.2829, Avg Adaptive LR: 3.039891\n",
      "INFO - Iter 151/2000, Loss: 0.2807, Avg Adaptive LR: 3.039762\n",
      "INFO - Iter 161/2000, Loss: 0.2787, Avg Adaptive LR: 3.039644\n",
      "INFO - Iter 171/2000, Loss: 0.2769, Avg Adaptive LR: 3.039535\n",
      "INFO - Iter 181/2000, Loss: 0.2752, Avg Adaptive LR: 3.039435\n",
      "INFO - Iter 191/2000, Loss: 0.2736, Avg Adaptive LR: 3.039343\n",
      "INFO - Iter 201/2000, Loss: 0.2721, Avg Adaptive LR: 3.039257\n",
      "INFO - Iter 211/2000, Loss: 0.2707, Avg Adaptive LR: 3.039176\n",
      "INFO - Iter 221/2000, Loss: 0.2694, Avg Adaptive LR: 3.039101\n",
      "INFO - Iter 231/2000, Loss: 0.2682, Avg Adaptive LR: 3.039030\n",
      "INFO - Iter 241/2000, Loss: 0.2671, Avg Adaptive LR: 3.038963\n",
      "INFO - Iter 251/2000, Loss: 0.2660, Avg Adaptive LR: 3.038900\n",
      "INFO - Iter 261/2000, Loss: 0.2650, Avg Adaptive LR: 3.038841\n",
      "INFO - Iter 271/2000, Loss: 0.2640, Avg Adaptive LR: 3.038784\n",
      "INFO - Iter 281/2000, Loss: 0.2631, Avg Adaptive LR: 3.038730\n",
      "INFO - Iter 291/2000, Loss: 0.2622, Avg Adaptive LR: 3.038678\n",
      "INFO - Iter 301/2000, Loss: 0.2614, Avg Adaptive LR: 3.038629\n",
      "INFO - Iter 311/2000, Loss: 0.2606, Avg Adaptive LR: 3.038582\n",
      "INFO - Iter 321/2000, Loss: 0.2598, Avg Adaptive LR: 3.038538\n",
      "INFO - Iter 331/2000, Loss: 0.2591, Avg Adaptive LR: 3.038494\n",
      "INFO - Iter 341/2000, Loss: 0.2584, Avg Adaptive LR: 3.038453\n",
      "INFO - Iter 351/2000, Loss: 0.2577, Avg Adaptive LR: 3.038413\n",
      "INFO - Iter 361/2000, Loss: 0.2570, Avg Adaptive LR: 3.038375\n",
      "INFO - Iter 371/2000, Loss: 0.2564, Avg Adaptive LR: 3.038338\n",
      "INFO - Iter 381/2000, Loss: 0.2558, Avg Adaptive LR: 3.038302\n",
      "INFO - Iter 391/2000, Loss: 0.2552, Avg Adaptive LR: 3.038268\n",
      "INFO - Iter 401/2000, Loss: 0.2547, Avg Adaptive LR: 3.038235\n",
      "INFO - Iter 411/2000, Loss: 0.2541, Avg Adaptive LR: 3.038202\n",
      "INFO - Iter 421/2000, Loss: 0.2536, Avg Adaptive LR: 3.038171\n",
      "INFO - Iter 431/2000, Loss: 0.2531, Avg Adaptive LR: 3.038141\n",
      "INFO - Iter 441/2000, Loss: 0.2526, Avg Adaptive LR: 3.038112\n",
      "INFO - Iter 451/2000, Loss: 0.2521, Avg Adaptive LR: 3.038083\n",
      "INFO - Iter 461/2000, Loss: 0.2516, Avg Adaptive LR: 3.038056\n",
      "INFO - Iter 471/2000, Loss: 0.2511, Avg Adaptive LR: 3.038029\n",
      "INFO - Iter 481/2000, Loss: 0.2507, Avg Adaptive LR: 3.038003\n",
      "INFO - Iter 491/2000, Loss: 0.2503, Avg Adaptive LR: 3.037977\n",
      "INFO - Iter 501/2000, Loss: 0.2498, Avg Adaptive LR: 3.037952\n",
      "INFO - Iter 511/2000, Loss: 0.2494, Avg Adaptive LR: 3.037928\n",
      "INFO - Iter 521/2000, Loss: 0.2490, Avg Adaptive LR: 3.037905\n",
      "INFO - Iter 531/2000, Loss: 0.2486, Avg Adaptive LR: 3.037882\n",
      "INFO - Iter 541/2000, Loss: 0.2483, Avg Adaptive LR: 3.037860\n",
      "INFO - Iter 551/2000, Loss: 0.2479, Avg Adaptive LR: 3.037838\n",
      "INFO - Iter 561/2000, Loss: 0.2475, Avg Adaptive LR: 3.037816\n",
      "INFO - Iter 571/2000, Loss: 0.2472, Avg Adaptive LR: 3.037795\n",
      "INFO - Iter 581/2000, Loss: 0.2468, Avg Adaptive LR: 3.037775\n",
      "INFO - Iter 591/2000, Loss: 0.2465, Avg Adaptive LR: 3.037755\n",
      "INFO - Iter 601/2000, Loss: 0.2462, Avg Adaptive LR: 3.037736\n",
      "INFO - Iter 611/2000, Loss: 0.2458, Avg Adaptive LR: 3.037716\n",
      "INFO - Iter 621/2000, Loss: 0.2455, Avg Adaptive LR: 3.037698\n",
      "INFO - Iter 631/2000, Loss: 0.2452, Avg Adaptive LR: 3.037679\n",
      "INFO - Iter 641/2000, Loss: 0.2449, Avg Adaptive LR: 3.037662\n",
      "INFO - Iter 651/2000, Loss: 0.2446, Avg Adaptive LR: 3.037644\n",
      "INFO - Iter 661/2000, Loss: 0.2443, Avg Adaptive LR: 3.037627\n",
      "INFO - Iter 671/2000, Loss: 0.2440, Avg Adaptive LR: 3.037610\n",
      "INFO - Iter 681/2000, Loss: 0.2437, Avg Adaptive LR: 3.037593\n",
      "INFO - Iter 691/2000, Loss: 0.2434, Avg Adaptive LR: 3.037577\n",
      "INFO - Iter 701/2000, Loss: 0.2432, Avg Adaptive LR: 3.037561\n",
      "INFO - Iter 711/2000, Loss: 0.2429, Avg Adaptive LR: 3.037545\n",
      "INFO - Iter 721/2000, Loss: 0.2426, Avg Adaptive LR: 3.037530\n",
      "INFO - Iter 731/2000, Loss: 0.2424, Avg Adaptive LR: 3.037515\n",
      "INFO - Iter 741/2000, Loss: 0.2421, Avg Adaptive LR: 3.037500\n",
      "INFO - Iter 751/2000, Loss: 0.2419, Avg Adaptive LR: 3.037485\n",
      "INFO - Iter 761/2000, Loss: 0.2416, Avg Adaptive LR: 3.037471\n",
      "INFO - Iter 771/2000, Loss: 0.2414, Avg Adaptive LR: 3.037456\n",
      "INFO - Iter 781/2000, Loss: 0.2412, Avg Adaptive LR: 3.037443\n",
      "INFO - Iter 791/2000, Loss: 0.2409, Avg Adaptive LR: 3.037429\n",
      "INFO - Iter 801/2000, Loss: 0.2407, Avg Adaptive LR: 3.037415\n",
      "INFO - Iter 811/2000, Loss: 0.2405, Avg Adaptive LR: 3.037402\n",
      "INFO - Iter 821/2000, Loss: 0.2402, Avg Adaptive LR: 3.037389\n",
      "INFO - Iter 831/2000, Loss: 0.2400, Avg Adaptive LR: 3.037376\n",
      "INFO - Iter 841/2000, Loss: 0.2398, Avg Adaptive LR: 3.037364\n",
      "INFO - Iter 851/2000, Loss: 0.2396, Avg Adaptive LR: 3.037351\n",
      "INFO - Iter 861/2000, Loss: 0.2394, Avg Adaptive LR: 3.037339\n",
      "INFO - Iter 871/2000, Loss: 0.2392, Avg Adaptive LR: 3.037327\n",
      "INFO - Iter 881/2000, Loss: 0.2390, Avg Adaptive LR: 3.037315\n",
      "INFO - Iter 891/2000, Loss: 0.2388, Avg Adaptive LR: 3.037303\n",
      "INFO - Iter 901/2000, Loss: 0.2386, Avg Adaptive LR: 3.037291\n",
      "INFO - Iter 911/2000, Loss: 0.2384, Avg Adaptive LR: 3.037280\n",
      "INFO - Iter 921/2000, Loss: 0.2382, Avg Adaptive LR: 3.037269\n",
      "INFO - Iter 931/2000, Loss: 0.2380, Avg Adaptive LR: 3.037258\n",
      "INFO - Iter 941/2000, Loss: 0.2378, Avg Adaptive LR: 3.037247\n",
      "INFO - Iter 951/2000, Loss: 0.2376, Avg Adaptive LR: 3.037236\n",
      "INFO - Iter 961/2000, Loss: 0.2375, Avg Adaptive LR: 3.037225\n",
      "INFO - Iter 971/2000, Loss: 0.2373, Avg Adaptive LR: 3.037215\n",
      "INFO - Iter 981/2000, Loss: 0.2371, Avg Adaptive LR: 3.037204\n",
      "INFO - Iter 991/2000, Loss: 0.2369, Avg Adaptive LR: 3.037194\n",
      "INFO - Iter 1001/2000, Loss: 0.2368, Avg Adaptive LR: 3.037184\n",
      "INFO - Iter 1011/2000, Loss: 0.2366, Avg Adaptive LR: 3.037174\n",
      "INFO - Iter 1021/2000, Loss: 0.2364, Avg Adaptive LR: 3.037164\n",
      "INFO - Iter 1031/2000, Loss: 0.2362, Avg Adaptive LR: 3.037154\n",
      "INFO - Iter 1041/2000, Loss: 0.2361, Avg Adaptive LR: 3.037145\n",
      "INFO - Iter 1051/2000, Loss: 0.2359, Avg Adaptive LR: 3.037135\n",
      "INFO - Iter 1061/2000, Loss: 0.2358, Avg Adaptive LR: 3.037126\n",
      "INFO - Iter 1071/2000, Loss: 0.2356, Avg Adaptive LR: 3.037116\n",
      "INFO - Iter 1081/2000, Loss: 0.2354, Avg Adaptive LR: 3.037107\n",
      "INFO - Iter 1091/2000, Loss: 0.2353, Avg Adaptive LR: 3.037098\n",
      "INFO - Iter 1101/2000, Loss: 0.2351, Avg Adaptive LR: 3.037089\n",
      "INFO - Iter 1111/2000, Loss: 0.2350, Avg Adaptive LR: 3.037080\n",
      "INFO - Iter 1121/2000, Loss: 0.2348, Avg Adaptive LR: 3.037071\n",
      "INFO - Iter 1131/2000, Loss: 0.2347, Avg Adaptive LR: 3.037063\n",
      "INFO - Iter 1141/2000, Loss: 0.2345, Avg Adaptive LR: 3.037054\n",
      "INFO - Iter 1151/2000, Loss: 0.2344, Avg Adaptive LR: 3.037046\n",
      "INFO - Iter 1161/2000, Loss: 0.2343, Avg Adaptive LR: 3.037037\n",
      "INFO - Iter 1171/2000, Loss: 0.2341, Avg Adaptive LR: 3.037029\n",
      "INFO - Iter 1181/2000, Loss: 0.2340, Avg Adaptive LR: 3.037021\n",
      "INFO - Iter 1191/2000, Loss: 0.2338, Avg Adaptive LR: 3.037013\n",
      "INFO - Iter 1201/2000, Loss: 0.2337, Avg Adaptive LR: 3.037005\n",
      "INFO - Iter 1211/2000, Loss: 0.2336, Avg Adaptive LR: 3.036997\n",
      "INFO - Iter 1221/2000, Loss: 0.2334, Avg Adaptive LR: 3.036989\n",
      "INFO - Iter 1231/2000, Loss: 0.2333, Avg Adaptive LR: 3.036981\n",
      "INFO - Iter 1241/2000, Loss: 0.2332, Avg Adaptive LR: 3.036973\n",
      "INFO - Iter 1251/2000, Loss: 0.2330, Avg Adaptive LR: 3.036966\n",
      "INFO - Iter 1261/2000, Loss: 0.2329, Avg Adaptive LR: 3.036958\n",
      "INFO - Iter 1271/2000, Loss: 0.2328, Avg Adaptive LR: 3.036951\n",
      "INFO - Iter 1281/2000, Loss: 0.2327, Avg Adaptive LR: 3.036943\n",
      "INFO - Iter 1291/2000, Loss: 0.2325, Avg Adaptive LR: 3.036936\n",
      "INFO - Iter 1301/2000, Loss: 0.2324, Avg Adaptive LR: 3.036929\n",
      "INFO - Iter 1311/2000, Loss: 0.2323, Avg Adaptive LR: 3.036922\n",
      "INFO - Iter 1321/2000, Loss: 0.2322, Avg Adaptive LR: 3.036915\n",
      "INFO - Iter 1331/2000, Loss: 0.2320, Avg Adaptive LR: 3.036908\n",
      "INFO - Iter 1341/2000, Loss: 0.2319, Avg Adaptive LR: 3.036901\n",
      "INFO - Iter 1351/2000, Loss: 0.2318, Avg Adaptive LR: 3.036894\n",
      "INFO - Iter 1361/2000, Loss: 0.2317, Avg Adaptive LR: 3.036887\n",
      "INFO - Iter 1371/2000, Loss: 0.2316, Avg Adaptive LR: 3.036880\n",
      "INFO - Iter 1381/2000, Loss: 0.2315, Avg Adaptive LR: 3.036873\n",
      "INFO - Iter 1391/2000, Loss: 0.2313, Avg Adaptive LR: 3.036867\n",
      "INFO - Iter 1401/2000, Loss: 0.2312, Avg Adaptive LR: 3.036860\n",
      "INFO - Iter 1411/2000, Loss: 0.2311, Avg Adaptive LR: 3.036854\n",
      "INFO - Iter 1421/2000, Loss: 0.2310, Avg Adaptive LR: 3.036847\n",
      "INFO - Iter 1431/2000, Loss: 0.2309, Avg Adaptive LR: 3.036841\n",
      "INFO - Iter 1441/2000, Loss: 0.2308, Avg Adaptive LR: 3.036834\n",
      "INFO - Iter 1451/2000, Loss: 0.2307, Avg Adaptive LR: 3.036828\n",
      "INFO - Iter 1461/2000, Loss: 0.2306, Avg Adaptive LR: 3.036822\n",
      "INFO - Iter 1471/2000, Loss: 0.2305, Avg Adaptive LR: 3.036816\n",
      "INFO - Iter 1481/2000, Loss: 0.2304, Avg Adaptive LR: 3.036810\n",
      "INFO - Iter 1491/2000, Loss: 0.2303, Avg Adaptive LR: 3.036804\n",
      "INFO - Iter 1501/2000, Loss: 0.2302, Avg Adaptive LR: 3.036798\n",
      "INFO - Iter 1511/2000, Loss: 0.2301, Avg Adaptive LR: 3.036792\n",
      "INFO - Iter 1521/2000, Loss: 0.2300, Avg Adaptive LR: 3.036786\n",
      "INFO - Iter 1531/2000, Loss: 0.2299, Avg Adaptive LR: 3.036780\n",
      "INFO - Iter 1541/2000, Loss: 0.2298, Avg Adaptive LR: 3.036774\n",
      "INFO - Iter 1551/2000, Loss: 0.2297, Avg Adaptive LR: 3.036768\n",
      "INFO - Iter 1561/2000, Loss: 0.2296, Avg Adaptive LR: 3.036763\n",
      "INFO - Iter 1571/2000, Loss: 0.2295, Avg Adaptive LR: 3.036757\n",
      "INFO - Iter 1581/2000, Loss: 0.2294, Avg Adaptive LR: 3.036751\n",
      "INFO - Iter 1591/2000, Loss: 0.2293, Avg Adaptive LR: 3.036746\n",
      "INFO - Iter 1601/2000, Loss: 0.2292, Avg Adaptive LR: 3.036740\n",
      "INFO - Iter 1611/2000, Loss: 0.2291, Avg Adaptive LR: 3.036735\n",
      "INFO - Iter 1621/2000, Loss: 0.2290, Avg Adaptive LR: 3.036729\n",
      "INFO - Iter 1631/2000, Loss: 0.2289, Avg Adaptive LR: 3.036724\n",
      "INFO - Iter 1641/2000, Loss: 0.2288, Avg Adaptive LR: 3.036718\n",
      "INFO - Iter 1651/2000, Loss: 0.2287, Avg Adaptive LR: 3.036713\n",
      "INFO - Iter 1661/2000, Loss: 0.2286, Avg Adaptive LR: 3.036708\n",
      "INFO - Iter 1671/2000, Loss: 0.2286, Avg Adaptive LR: 3.036703\n",
      "INFO - Iter 1681/2000, Loss: 0.2285, Avg Adaptive LR: 3.036697\n",
      "INFO - Iter 1691/2000, Loss: 0.2284, Avg Adaptive LR: 3.036692\n",
      "INFO - Iter 1701/2000, Loss: 0.2283, Avg Adaptive LR: 3.036687\n",
      "INFO - Iter 1711/2000, Loss: 0.2282, Avg Adaptive LR: 3.036682\n",
      "INFO - Iter 1721/2000, Loss: 0.2281, Avg Adaptive LR: 3.036677\n",
      "INFO - Iter 1731/2000, Loss: 0.2280, Avg Adaptive LR: 3.036672\n",
      "INFO - Iter 1741/2000, Loss: 0.2279, Avg Adaptive LR: 3.036667\n",
      "INFO - Iter 1751/2000, Loss: 0.2279, Avg Adaptive LR: 3.036662\n",
      "INFO - Iter 1761/2000, Loss: 0.2278, Avg Adaptive LR: 3.036657\n",
      "INFO - Iter 1771/2000, Loss: 0.2277, Avg Adaptive LR: 3.036652\n",
      "INFO - Iter 1781/2000, Loss: 0.2276, Avg Adaptive LR: 3.036648\n",
      "INFO - Iter 1791/2000, Loss: 0.2275, Avg Adaptive LR: 3.036643\n",
      "INFO - Iter 1801/2000, Loss: 0.2275, Avg Adaptive LR: 3.036638\n",
      "INFO - Iter 1811/2000, Loss: 0.2274, Avg Adaptive LR: 3.036633\n",
      "INFO - Iter 1821/2000, Loss: 0.2273, Avg Adaptive LR: 3.036629\n",
      "INFO - Iter 1831/2000, Loss: 0.2272, Avg Adaptive LR: 3.036624\n",
      "INFO - Iter 1841/2000, Loss: 0.2271, Avg Adaptive LR: 3.036619\n",
      "INFO - Iter 1851/2000, Loss: 0.2271, Avg Adaptive LR: 3.036615\n",
      "INFO - Iter 1861/2000, Loss: 0.2270, Avg Adaptive LR: 3.036610\n",
      "INFO - Iter 1871/2000, Loss: 0.2269, Avg Adaptive LR: 3.036606\n",
      "INFO - Iter 1881/2000, Loss: 0.2268, Avg Adaptive LR: 3.036601\n",
      "INFO - Iter 1891/2000, Loss: 0.2268, Avg Adaptive LR: 3.036597\n",
      "INFO - Iter 1901/2000, Loss: 0.2267, Avg Adaptive LR: 3.036593\n",
      "INFO - Iter 1911/2000, Loss: 0.2266, Avg Adaptive LR: 3.036588\n",
      "INFO - Iter 1921/2000, Loss: 0.2265, Avg Adaptive LR: 3.036584\n",
      "INFO - Iter 1931/2000, Loss: 0.2265, Avg Adaptive LR: 3.036579\n",
      "INFO - Iter 1941/2000, Loss: 0.2264, Avg Adaptive LR: 3.036575\n",
      "INFO - Iter 1951/2000, Loss: 0.2263, Avg Adaptive LR: 3.036571\n",
      "INFO - Iter 1961/2000, Loss: 0.2262, Avg Adaptive LR: 3.036567\n",
      "INFO - Iter 1971/2000, Loss: 0.2262, Avg Adaptive LR: 3.036562\n",
      "INFO - Iter 1981/2000, Loss: 0.2261, Avg Adaptive LR: 3.036558\n",
      "INFO - Iter 1991/2000, Loss: 0.2260, Avg Adaptive LR: 3.036554\n",
      "INFO - SoftmaxRegression training completed in 78.14 seconds.\n",
      "INFO - --- Linear Regression LR=0.1/Iter=2000 ---\n",
      "INFO - Iter 100/2000, Loss: 1.0308, Gradient Norm: 19.5337, Avg Adaptive LR: 1.3966674945235422\n",
      "INFO - Iter 200/2000, Loss: 0.5402, Gradient Norm: 13.8895, Avg Adaptive LR: 0.9907905664429116\n",
      "INFO - Iter 300/2000, Loss: 0.3321, Gradient Norm: 10.6174, Avg Adaptive LR: 0.8101783950153758\n",
      "INFO - Iter 400/2000, Loss: 0.2244, Gradient Norm: 8.4339, Avg Adaptive LR: 0.7023652851444648\n",
      "INFO - Iter 500/2000, Loss: 0.1658, Gradient Norm: 6.9645, Avg Adaptive LR: 0.6287117510518254\n",
      "INFO - Iter 600/2000, Loss: 0.1309, Gradient Norm: 5.9195, Avg Adaptive LR: 0.5742803246572616\n",
      "INFO - Iter 700/2000, Loss: 0.1086, Gradient Norm: 5.1425, Avg Adaptive LR: 0.5319345164557255\n",
      "INFO - Iter 800/2000, Loss: 0.0936, Gradient Norm: 4.5472, Avg Adaptive LR: 0.49777954817742803\n",
      "INFO - Iter 900/2000, Loss: 0.0830, Gradient Norm: 4.0733, Avg Adaptive LR: 0.46946861229196296\n",
      "INFO - Iter 1000/2000, Loss: 0.0751, Gradient Norm: 3.6802, Avg Adaptive LR: 0.44550575086018046\n",
      "INFO - Iter 1100/2000, Loss: 0.0692, Gradient Norm: 3.3597, Avg Adaptive LR: 0.4248789312457259\n",
      "INFO - Iter 1200/2000, Loss: 0.0648, Gradient Norm: 3.1025, Avg Adaptive LR: 0.40688084231996935\n",
      "INFO - Iter 1300/2000, Loss: 0.0612, Gradient Norm: 2.8705, Avg Adaptive LR: 0.39099588485459913\n",
      "INFO - Iter 1400/2000, Loss: 0.0584, Gradient Norm: 2.6814, Avg Adaptive LR: 0.37683939248516457\n",
      "INFO - Iter 1500/2000, Loss: 0.0562, Gradient Norm: 2.5198, Avg Adaptive LR: 0.36411917380693276\n",
      "INFO - Iter 1600/2000, Loss: 0.0543, Gradient Norm: 2.3692, Avg Adaptive LR: 0.35260812985773154\n",
      "INFO - Iter 1700/2000, Loss: 0.0528, Gradient Norm: 2.2507, Avg Adaptive LR: 0.3421256799671795\n",
      "INFO - Iter 1800/2000, Loss: 0.0514, Gradient Norm: 2.1293, Avg Adaptive LR: 0.3325270426964418\n",
      "INFO - Iter 1900/2000, Loss: 0.0504, Gradient Norm: 2.0389, Avg Adaptive LR: 0.3236949113667735\n",
      "INFO - Iter 2000/2000, Loss: 0.0493, Gradient Norm: 1.9403, Avg Adaptive LR: 0.3155319454048128\n",
      "INFO - LinearRegressionClassifier training completed in 64.18 seconds.\n",
      "Train Regressions:  75%|███████▌  | 3/4 [02:32<01:08, 68.01s/it]INFO - --- Softmax LR=0.1/Iter=3000 ---\n",
      "INFO - Iter 1/3000, Loss: 2.4383, Avg Adaptive LR: 14.561848\n",
      "INFO - Iter 11/3000, Loss: 0.4727, Avg Adaptive LR: 3.375636\n",
      "INFO - Iter 21/3000, Loss: 0.3708, Avg Adaptive LR: 3.352319\n",
      "INFO - Iter 31/3000, Loss: 0.3474, Avg Adaptive LR: 3.350694\n",
      "INFO - Iter 41/3000, Loss: 0.3325, Avg Adaptive LR: 3.349654\n",
      "INFO - Iter 51/3000, Loss: 0.3217, Avg Adaptive LR: 3.348898\n",
      "INFO - Iter 61/3000, Loss: 0.3133, Avg Adaptive LR: 3.348315\n",
      "INFO - Iter 71/3000, Loss: 0.3067, Avg Adaptive LR: 3.347846\n",
      "INFO - Iter 81/3000, Loss: 0.3012, Avg Adaptive LR: 3.347458\n",
      "INFO - Iter 91/3000, Loss: 0.2965, Avg Adaptive LR: 3.347129\n",
      "INFO - Iter 101/3000, Loss: 0.2925, Avg Adaptive LR: 3.346847\n",
      "INFO - Iter 111/3000, Loss: 0.2890, Avg Adaptive LR: 3.346600\n",
      "INFO - Iter 121/3000, Loss: 0.2859, Avg Adaptive LR: 3.346382\n",
      "INFO - Iter 131/3000, Loss: 0.2832, Avg Adaptive LR: 3.346188\n",
      "INFO - Iter 141/3000, Loss: 0.2807, Avg Adaptive LR: 3.346014\n",
      "INFO - Iter 151/3000, Loss: 0.2785, Avg Adaptive LR: 3.345855\n",
      "INFO - Iter 161/3000, Loss: 0.2765, Avg Adaptive LR: 3.345711\n",
      "INFO - Iter 171/3000, Loss: 0.2746, Avg Adaptive LR: 3.345579\n",
      "INFO - Iter 181/3000, Loss: 0.2729, Avg Adaptive LR: 3.345457\n",
      "INFO - Iter 191/3000, Loss: 0.2713, Avg Adaptive LR: 3.345344\n",
      "INFO - Iter 201/3000, Loss: 0.2698, Avg Adaptive LR: 3.345239\n",
      "INFO - Iter 211/3000, Loss: 0.2684, Avg Adaptive LR: 3.345141\n",
      "INFO - Iter 221/3000, Loss: 0.2671, Avg Adaptive LR: 3.345049\n",
      "INFO - Iter 231/3000, Loss: 0.2659, Avg Adaptive LR: 3.344963\n",
      "INFO - Iter 241/3000, Loss: 0.2648, Avg Adaptive LR: 3.344882\n",
      "INFO - Iter 251/3000, Loss: 0.2637, Avg Adaptive LR: 3.344806\n",
      "INFO - Iter 261/3000, Loss: 0.2627, Avg Adaptive LR: 3.344733\n",
      "INFO - Iter 271/3000, Loss: 0.2617, Avg Adaptive LR: 3.344665\n",
      "INFO - Iter 281/3000, Loss: 0.2608, Avg Adaptive LR: 3.344599\n",
      "INFO - Iter 291/3000, Loss: 0.2599, Avg Adaptive LR: 3.344537\n",
      "INFO - Iter 301/3000, Loss: 0.2591, Avg Adaptive LR: 3.344478\n",
      "INFO - Iter 311/3000, Loss: 0.2583, Avg Adaptive LR: 3.344421\n",
      "INFO - Iter 321/3000, Loss: 0.2575, Avg Adaptive LR: 3.344367\n",
      "INFO - Iter 331/3000, Loss: 0.2568, Avg Adaptive LR: 3.344315\n",
      "INFO - Iter 341/3000, Loss: 0.2561, Avg Adaptive LR: 3.344265\n",
      "INFO - Iter 351/3000, Loss: 0.2554, Avg Adaptive LR: 3.344217\n",
      "INFO - Iter 361/3000, Loss: 0.2547, Avg Adaptive LR: 3.344171\n",
      "INFO - Iter 371/3000, Loss: 0.2541, Avg Adaptive LR: 3.344127\n",
      "INFO - Iter 381/3000, Loss: 0.2535, Avg Adaptive LR: 3.344084\n",
      "INFO - Iter 391/3000, Loss: 0.2529, Avg Adaptive LR: 3.344043\n",
      "INFO - Iter 401/3000, Loss: 0.2524, Avg Adaptive LR: 3.344003\n",
      "INFO - Iter 411/3000, Loss: 0.2518, Avg Adaptive LR: 3.343965\n",
      "INFO - Iter 421/3000, Loss: 0.2513, Avg Adaptive LR: 3.343927\n",
      "INFO - Iter 431/3000, Loss: 0.2508, Avg Adaptive LR: 3.343891\n",
      "INFO - Iter 441/3000, Loss: 0.2503, Avg Adaptive LR: 3.343856\n",
      "INFO - Iter 451/3000, Loss: 0.2498, Avg Adaptive LR: 3.343823\n",
      "INFO - Iter 461/3000, Loss: 0.2494, Avg Adaptive LR: 3.343790\n",
      "INFO - Iter 471/3000, Loss: 0.2489, Avg Adaptive LR: 3.343758\n",
      "INFO - Iter 481/3000, Loss: 0.2485, Avg Adaptive LR: 3.343727\n",
      "INFO - Iter 491/3000, Loss: 0.2481, Avg Adaptive LR: 3.343696\n",
      "INFO - Iter 501/3000, Loss: 0.2477, Avg Adaptive LR: 3.343667\n",
      "INFO - Iter 511/3000, Loss: 0.2472, Avg Adaptive LR: 3.343639\n",
      "INFO - Iter 521/3000, Loss: 0.2469, Avg Adaptive LR: 3.343611\n",
      "INFO - Iter 531/3000, Loss: 0.2465, Avg Adaptive LR: 3.343584\n",
      "INFO - Iter 541/3000, Loss: 0.2461, Avg Adaptive LR: 3.343557\n",
      "INFO - Iter 551/3000, Loss: 0.2457, Avg Adaptive LR: 3.343531\n",
      "INFO - Iter 561/3000, Loss: 0.2454, Avg Adaptive LR: 3.343506\n",
      "INFO - Iter 571/3000, Loss: 0.2450, Avg Adaptive LR: 3.343481\n",
      "INFO - Iter 581/3000, Loss: 0.2447, Avg Adaptive LR: 3.343457\n",
      "INFO - Iter 591/3000, Loss: 0.2444, Avg Adaptive LR: 3.343434\n",
      "INFO - Iter 601/3000, Loss: 0.2440, Avg Adaptive LR: 3.343411\n",
      "INFO - Iter 611/3000, Loss: 0.2437, Avg Adaptive LR: 3.343388\n",
      "INFO - Iter 621/3000, Loss: 0.2434, Avg Adaptive LR: 3.343366\n",
      "INFO - Iter 631/3000, Loss: 0.2431, Avg Adaptive LR: 3.343345\n",
      "INFO - Iter 641/3000, Loss: 0.2428, Avg Adaptive LR: 3.343324\n",
      "INFO - Iter 651/3000, Loss: 0.2425, Avg Adaptive LR: 3.343303\n",
      "INFO - Iter 661/3000, Loss: 0.2422, Avg Adaptive LR: 3.343283\n",
      "INFO - Iter 671/3000, Loss: 0.2420, Avg Adaptive LR: 3.343263\n",
      "INFO - Iter 681/3000, Loss: 0.2417, Avg Adaptive LR: 3.343244\n",
      "INFO - Iter 691/3000, Loss: 0.2414, Avg Adaptive LR: 3.343225\n",
      "INFO - Iter 701/3000, Loss: 0.2412, Avg Adaptive LR: 3.343206\n",
      "INFO - Iter 711/3000, Loss: 0.2409, Avg Adaptive LR: 3.343187\n",
      "INFO - Iter 721/3000, Loss: 0.2407, Avg Adaptive LR: 3.343169\n",
      "INFO - Iter 731/3000, Loss: 0.2404, Avg Adaptive LR: 3.343152\n",
      "INFO - Iter 741/3000, Loss: 0.2402, Avg Adaptive LR: 3.343134\n",
      "INFO - Iter 751/3000, Loss: 0.2399, Avg Adaptive LR: 3.343117\n",
      "INFO - Iter 761/3000, Loss: 0.2397, Avg Adaptive LR: 3.343100\n",
      "INFO - Iter 771/3000, Loss: 0.2394, Avg Adaptive LR: 3.343084\n",
      "INFO - Iter 781/3000, Loss: 0.2392, Avg Adaptive LR: 3.343068\n",
      "INFO - Iter 791/3000, Loss: 0.2390, Avg Adaptive LR: 3.343052\n",
      "INFO - Iter 801/3000, Loss: 0.2388, Avg Adaptive LR: 3.343036\n",
      "INFO - Iter 811/3000, Loss: 0.2386, Avg Adaptive LR: 3.343021\n",
      "INFO - Iter 821/3000, Loss: 0.2383, Avg Adaptive LR: 3.343005\n",
      "INFO - Iter 831/3000, Loss: 0.2381, Avg Adaptive LR: 3.342990\n",
      "INFO - Iter 841/3000, Loss: 0.2379, Avg Adaptive LR: 3.342976\n",
      "INFO - Iter 851/3000, Loss: 0.2377, Avg Adaptive LR: 3.342961\n",
      "INFO - Iter 861/3000, Loss: 0.2375, Avg Adaptive LR: 3.342947\n",
      "INFO - Iter 871/3000, Loss: 0.2373, Avg Adaptive LR: 3.342933\n",
      "INFO - Iter 881/3000, Loss: 0.2371, Avg Adaptive LR: 3.342919\n",
      "INFO - Iter 891/3000, Loss: 0.2369, Avg Adaptive LR: 3.342905\n",
      "INFO - Iter 901/3000, Loss: 0.2367, Avg Adaptive LR: 3.342892\n",
      "INFO - Iter 911/3000, Loss: 0.2366, Avg Adaptive LR: 3.342879\n",
      "INFO - Iter 921/3000, Loss: 0.2364, Avg Adaptive LR: 3.342865\n",
      "INFO - Iter 931/3000, Loss: 0.2362, Avg Adaptive LR: 3.342853\n",
      "INFO - Iter 941/3000, Loss: 0.2360, Avg Adaptive LR: 3.342840\n",
      "INFO - Iter 951/3000, Loss: 0.2358, Avg Adaptive LR: 3.342827\n",
      "INFO - Iter 961/3000, Loss: 0.2357, Avg Adaptive LR: 3.342815\n",
      "INFO - Iter 971/3000, Loss: 0.2355, Avg Adaptive LR: 3.342803\n",
      "INFO - Iter 981/3000, Loss: 0.2353, Avg Adaptive LR: 3.342791\n",
      "INFO - Iter 991/3000, Loss: 0.2352, Avg Adaptive LR: 3.342779\n",
      "INFO - Iter 1001/3000, Loss: 0.2350, Avg Adaptive LR: 3.342767\n",
      "INFO - Iter 1011/3000, Loss: 0.2348, Avg Adaptive LR: 3.342755\n",
      "INFO - Iter 1021/3000, Loss: 0.2347, Avg Adaptive LR: 3.342744\n",
      "INFO - Iter 1031/3000, Loss: 0.2345, Avg Adaptive LR: 3.342733\n",
      "INFO - Iter 1041/3000, Loss: 0.2344, Avg Adaptive LR: 3.342722\n",
      "INFO - Iter 1051/3000, Loss: 0.2342, Avg Adaptive LR: 3.342711\n",
      "INFO - Iter 1061/3000, Loss: 0.2340, Avg Adaptive LR: 3.342700\n",
      "INFO - Iter 1071/3000, Loss: 0.2339, Avg Adaptive LR: 3.342689\n",
      "INFO - Iter 1081/3000, Loss: 0.2337, Avg Adaptive LR: 3.342678\n",
      "INFO - Iter 1091/3000, Loss: 0.2336, Avg Adaptive LR: 3.342668\n",
      "INFO - Iter 1101/3000, Loss: 0.2334, Avg Adaptive LR: 3.342657\n",
      "INFO - Iter 1111/3000, Loss: 0.2333, Avg Adaptive LR: 3.342647\n",
      "INFO - Iter 1121/3000, Loss: 0.2332, Avg Adaptive LR: 3.342637\n",
      "INFO - Iter 1131/3000, Loss: 0.2330, Avg Adaptive LR: 3.342627\n",
      "INFO - Iter 1141/3000, Loss: 0.2329, Avg Adaptive LR: 3.342617\n",
      "INFO - Iter 1151/3000, Loss: 0.2327, Avg Adaptive LR: 3.342607\n",
      "INFO - Iter 1161/3000, Loss: 0.2326, Avg Adaptive LR: 3.342598\n",
      "INFO - Iter 1171/3000, Loss: 0.2325, Avg Adaptive LR: 3.342588\n",
      "INFO - Iter 1181/3000, Loss: 0.2323, Avg Adaptive LR: 3.342578\n",
      "INFO - Iter 1191/3000, Loss: 0.2322, Avg Adaptive LR: 3.342569\n",
      "INFO - Iter 1201/3000, Loss: 0.2321, Avg Adaptive LR: 3.342560\n",
      "INFO - Iter 1211/3000, Loss: 0.2319, Avg Adaptive LR: 3.342551\n",
      "INFO - Iter 1221/3000, Loss: 0.2318, Avg Adaptive LR: 3.342542\n",
      "INFO - Iter 1231/3000, Loss: 0.2317, Avg Adaptive LR: 3.342533\n",
      "INFO - Iter 1241/3000, Loss: 0.2316, Avg Adaptive LR: 3.342524\n",
      "INFO - Iter 1251/3000, Loss: 0.2314, Avg Adaptive LR: 3.342515\n",
      "INFO - Iter 1261/3000, Loss: 0.2313, Avg Adaptive LR: 3.342506\n",
      "INFO - Iter 1271/3000, Loss: 0.2312, Avg Adaptive LR: 3.342498\n",
      "INFO - Iter 1281/3000, Loss: 0.2311, Avg Adaptive LR: 3.342489\n",
      "INFO - Iter 1291/3000, Loss: 0.2310, Avg Adaptive LR: 3.342481\n",
      "INFO - Iter 1301/3000, Loss: 0.2308, Avg Adaptive LR: 3.342472\n",
      "INFO - Iter 1311/3000, Loss: 0.2307, Avg Adaptive LR: 3.342464\n",
      "INFO - Iter 1321/3000, Loss: 0.2306, Avg Adaptive LR: 3.342456\n",
      "INFO - Iter 1331/3000, Loss: 0.2305, Avg Adaptive LR: 3.342448\n",
      "INFO - Iter 1341/3000, Loss: 0.2304, Avg Adaptive LR: 3.342440\n",
      "INFO - Iter 1351/3000, Loss: 0.2303, Avg Adaptive LR: 3.342432\n",
      "INFO - Iter 1361/3000, Loss: 0.2302, Avg Adaptive LR: 3.342424\n",
      "INFO - Iter 1371/3000, Loss: 0.2301, Avg Adaptive LR: 3.342416\n",
      "INFO - Iter 1381/3000, Loss: 0.2299, Avg Adaptive LR: 3.342408\n",
      "INFO - Iter 1391/3000, Loss: 0.2298, Avg Adaptive LR: 3.342400\n",
      "INFO - Iter 1401/3000, Loss: 0.2297, Avg Adaptive LR: 3.342393\n",
      "INFO - Iter 1411/3000, Loss: 0.2296, Avg Adaptive LR: 3.342385\n",
      "INFO - Iter 1421/3000, Loss: 0.2295, Avg Adaptive LR: 3.342378\n",
      "INFO - Iter 1431/3000, Loss: 0.2294, Avg Adaptive LR: 3.342370\n",
      "INFO - Iter 1441/3000, Loss: 0.2293, Avg Adaptive LR: 3.342363\n",
      "INFO - Iter 1451/3000, Loss: 0.2292, Avg Adaptive LR: 3.342356\n",
      "INFO - Iter 1461/3000, Loss: 0.2291, Avg Adaptive LR: 3.342349\n",
      "INFO - Iter 1471/3000, Loss: 0.2290, Avg Adaptive LR: 3.342341\n",
      "INFO - Iter 1481/3000, Loss: 0.2289, Avg Adaptive LR: 3.342334\n",
      "INFO - Iter 1491/3000, Loss: 0.2288, Avg Adaptive LR: 3.342327\n",
      "INFO - Iter 1501/3000, Loss: 0.2287, Avg Adaptive LR: 3.342320\n",
      "INFO - Iter 1511/3000, Loss: 0.2286, Avg Adaptive LR: 3.342314\n",
      "INFO - Iter 1521/3000, Loss: 0.2285, Avg Adaptive LR: 3.342307\n",
      "INFO - Iter 1531/3000, Loss: 0.2284, Avg Adaptive LR: 3.342300\n",
      "INFO - Iter 1541/3000, Loss: 0.2283, Avg Adaptive LR: 3.342293\n",
      "INFO - Iter 1551/3000, Loss: 0.2282, Avg Adaptive LR: 3.342287\n",
      "INFO - Iter 1561/3000, Loss: 0.2281, Avg Adaptive LR: 3.342280\n",
      "INFO - Iter 1571/3000, Loss: 0.2280, Avg Adaptive LR: 3.342273\n",
      "INFO - Iter 1581/3000, Loss: 0.2280, Avg Adaptive LR: 3.342267\n",
      "INFO - Iter 1591/3000, Loss: 0.2279, Avg Adaptive LR: 3.342260\n",
      "INFO - Iter 1601/3000, Loss: 0.2278, Avg Adaptive LR: 3.342254\n",
      "INFO - Iter 1611/3000, Loss: 0.2277, Avg Adaptive LR: 3.342248\n",
      "INFO - Iter 1621/3000, Loss: 0.2276, Avg Adaptive LR: 3.342241\n",
      "INFO - Iter 1631/3000, Loss: 0.2275, Avg Adaptive LR: 3.342235\n",
      "INFO - Iter 1641/3000, Loss: 0.2274, Avg Adaptive LR: 3.342229\n",
      "INFO - Iter 1651/3000, Loss: 0.2273, Avg Adaptive LR: 3.342223\n",
      "INFO - Iter 1661/3000, Loss: 0.2273, Avg Adaptive LR: 3.342217\n",
      "INFO - Iter 1671/3000, Loss: 0.2272, Avg Adaptive LR: 3.342211\n",
      "INFO - Iter 1681/3000, Loss: 0.2271, Avg Adaptive LR: 3.342205\n",
      "INFO - Iter 1691/3000, Loss: 0.2270, Avg Adaptive LR: 3.342199\n",
      "INFO - Iter 1701/3000, Loss: 0.2269, Avg Adaptive LR: 3.342193\n",
      "INFO - Iter 1711/3000, Loss: 0.2268, Avg Adaptive LR: 3.342187\n",
      "INFO - Iter 1721/3000, Loss: 0.2267, Avg Adaptive LR: 3.342181\n",
      "INFO - Iter 1731/3000, Loss: 0.2267, Avg Adaptive LR: 3.342175\n",
      "INFO - Iter 1741/3000, Loss: 0.2266, Avg Adaptive LR: 3.342170\n",
      "INFO - Iter 1751/3000, Loss: 0.2265, Avg Adaptive LR: 3.342164\n",
      "INFO - Iter 1761/3000, Loss: 0.2264, Avg Adaptive LR: 3.342158\n",
      "INFO - Iter 1771/3000, Loss: 0.2263, Avg Adaptive LR: 3.342153\n",
      "INFO - Iter 1781/3000, Loss: 0.2263, Avg Adaptive LR: 3.342147\n",
      "INFO - Iter 1791/3000, Loss: 0.2262, Avg Adaptive LR: 3.342141\n",
      "INFO - Iter 1801/3000, Loss: 0.2261, Avg Adaptive LR: 3.342136\n",
      "INFO - Iter 1811/3000, Loss: 0.2260, Avg Adaptive LR: 3.342131\n",
      "INFO - Iter 1821/3000, Loss: 0.2260, Avg Adaptive LR: 3.342125\n",
      "INFO - Iter 1831/3000, Loss: 0.2259, Avg Adaptive LR: 3.342120\n",
      "INFO - Iter 1841/3000, Loss: 0.2258, Avg Adaptive LR: 3.342114\n",
      "INFO - Iter 1851/3000, Loss: 0.2257, Avg Adaptive LR: 3.342109\n",
      "INFO - Iter 1861/3000, Loss: 0.2257, Avg Adaptive LR: 3.342104\n",
      "INFO - Iter 1871/3000, Loss: 0.2256, Avg Adaptive LR: 3.342099\n",
      "INFO - Iter 1881/3000, Loss: 0.2255, Avg Adaptive LR: 3.342093\n",
      "INFO - Iter 1891/3000, Loss: 0.2254, Avg Adaptive LR: 3.342088\n",
      "INFO - Iter 1901/3000, Loss: 0.2254, Avg Adaptive LR: 3.342083\n",
      "INFO - Iter 1911/3000, Loss: 0.2253, Avg Adaptive LR: 3.342078\n",
      "INFO - Iter 1921/3000, Loss: 0.2252, Avg Adaptive LR: 3.342073\n",
      "INFO - Iter 1931/3000, Loss: 0.2252, Avg Adaptive LR: 3.342068\n",
      "INFO - Iter 1941/3000, Loss: 0.2251, Avg Adaptive LR: 3.342063\n",
      "INFO - Iter 1951/3000, Loss: 0.2250, Avg Adaptive LR: 3.342058\n",
      "INFO - Iter 1961/3000, Loss: 0.2250, Avg Adaptive LR: 3.342053\n",
      "INFO - Iter 1971/3000, Loss: 0.2249, Avg Adaptive LR: 3.342048\n",
      "INFO - Iter 1981/3000, Loss: 0.2248, Avg Adaptive LR: 3.342043\n",
      "INFO - Iter 1991/3000, Loss: 0.2247, Avg Adaptive LR: 3.342039\n",
      "INFO - Iter 2001/3000, Loss: 0.2247, Avg Adaptive LR: 3.342034\n",
      "INFO - Iter 2011/3000, Loss: 0.2246, Avg Adaptive LR: 3.342029\n",
      "INFO - Iter 2021/3000, Loss: 0.2245, Avg Adaptive LR: 3.342024\n",
      "INFO - Iter 2031/3000, Loss: 0.2245, Avg Adaptive LR: 3.342020\n",
      "INFO - Iter 2041/3000, Loss: 0.2244, Avg Adaptive LR: 3.342015\n",
      "INFO - Iter 2051/3000, Loss: 0.2244, Avg Adaptive LR: 3.342010\n",
      "INFO - Iter 2061/3000, Loss: 0.2243, Avg Adaptive LR: 3.342006\n",
      "INFO - Iter 2071/3000, Loss: 0.2242, Avg Adaptive LR: 3.342001\n",
      "INFO - Iter 2081/3000, Loss: 0.2242, Avg Adaptive LR: 3.341997\n",
      "INFO - Iter 2091/3000, Loss: 0.2241, Avg Adaptive LR: 3.341992\n",
      "INFO - Iter 2101/3000, Loss: 0.2240, Avg Adaptive LR: 3.341988\n",
      "INFO - Iter 2111/3000, Loss: 0.2240, Avg Adaptive LR: 3.341983\n",
      "INFO - Iter 2121/3000, Loss: 0.2239, Avg Adaptive LR: 3.341979\n",
      "INFO - Iter 2131/3000, Loss: 0.2238, Avg Adaptive LR: 3.341974\n",
      "INFO - Iter 2141/3000, Loss: 0.2238, Avg Adaptive LR: 3.341970\n",
      "INFO - Iter 2151/3000, Loss: 0.2237, Avg Adaptive LR: 3.341966\n",
      "INFO - Iter 2161/3000, Loss: 0.2237, Avg Adaptive LR: 3.341961\n",
      "INFO - Iter 2171/3000, Loss: 0.2236, Avg Adaptive LR: 3.341957\n",
      "INFO - Iter 2181/3000, Loss: 0.2235, Avg Adaptive LR: 3.341953\n",
      "INFO - Iter 2191/3000, Loss: 0.2235, Avg Adaptive LR: 3.341949\n",
      "INFO - Iter 2201/3000, Loss: 0.2234, Avg Adaptive LR: 3.341944\n",
      "INFO - Iter 2211/3000, Loss: 0.2234, Avg Adaptive LR: 3.341940\n",
      "INFO - Iter 2221/3000, Loss: 0.2233, Avg Adaptive LR: 3.341936\n",
      "INFO - Iter 2231/3000, Loss: 0.2232, Avg Adaptive LR: 3.341932\n",
      "INFO - Iter 2241/3000, Loss: 0.2232, Avg Adaptive LR: 3.341928\n",
      "INFO - Iter 2251/3000, Loss: 0.2231, Avg Adaptive LR: 3.341924\n",
      "INFO - Iter 2261/3000, Loss: 0.2231, Avg Adaptive LR: 3.341920\n",
      "INFO - Iter 2271/3000, Loss: 0.2230, Avg Adaptive LR: 3.341916\n",
      "INFO - Iter 2281/3000, Loss: 0.2230, Avg Adaptive LR: 3.341912\n",
      "INFO - Iter 2291/3000, Loss: 0.2229, Avg Adaptive LR: 3.341908\n",
      "INFO - Iter 2301/3000, Loss: 0.2228, Avg Adaptive LR: 3.341904\n",
      "INFO - Iter 2311/3000, Loss: 0.2228, Avg Adaptive LR: 3.341900\n",
      "INFO - Iter 2321/3000, Loss: 0.2227, Avg Adaptive LR: 3.341896\n",
      "INFO - Iter 2331/3000, Loss: 0.2227, Avg Adaptive LR: 3.341892\n",
      "INFO - Iter 2341/3000, Loss: 0.2226, Avg Adaptive LR: 3.341888\n",
      "INFO - Iter 2351/3000, Loss: 0.2226, Avg Adaptive LR: 3.341884\n",
      "INFO - Iter 2361/3000, Loss: 0.2225, Avg Adaptive LR: 3.341880\n",
      "INFO - Iter 2371/3000, Loss: 0.2225, Avg Adaptive LR: 3.341876\n",
      "INFO - Iter 2381/3000, Loss: 0.2224, Avg Adaptive LR: 3.341873\n",
      "INFO - Iter 2391/3000, Loss: 0.2224, Avg Adaptive LR: 3.341869\n",
      "INFO - Iter 2401/3000, Loss: 0.2223, Avg Adaptive LR: 3.341865\n",
      "INFO - Iter 2411/3000, Loss: 0.2223, Avg Adaptive LR: 3.341861\n",
      "INFO - Iter 2421/3000, Loss: 0.2222, Avg Adaptive LR: 3.341858\n",
      "INFO - Iter 2431/3000, Loss: 0.2222, Avg Adaptive LR: 3.341854\n",
      "INFO - Iter 2441/3000, Loss: 0.2221, Avg Adaptive LR: 3.341850\n",
      "INFO - Iter 2451/3000, Loss: 0.2220, Avg Adaptive LR: 3.341847\n",
      "INFO - Iter 2461/3000, Loss: 0.2220, Avg Adaptive LR: 3.341843\n",
      "INFO - Iter 2471/3000, Loss: 0.2219, Avg Adaptive LR: 3.341839\n",
      "INFO - Iter 2481/3000, Loss: 0.2219, Avg Adaptive LR: 3.341836\n",
      "INFO - Iter 2491/3000, Loss: 0.2218, Avg Adaptive LR: 3.341832\n",
      "INFO - Iter 2501/3000, Loss: 0.2218, Avg Adaptive LR: 3.341829\n",
      "INFO - Iter 2511/3000, Loss: 0.2217, Avg Adaptive LR: 3.341825\n",
      "INFO - Iter 2521/3000, Loss: 0.2217, Avg Adaptive LR: 3.341822\n",
      "INFO - Iter 2531/3000, Loss: 0.2216, Avg Adaptive LR: 3.341818\n",
      "INFO - Iter 2541/3000, Loss: 0.2216, Avg Adaptive LR: 3.341815\n",
      "INFO - Iter 2551/3000, Loss: 0.2215, Avg Adaptive LR: 3.341811\n",
      "INFO - Iter 2561/3000, Loss: 0.2215, Avg Adaptive LR: 3.341808\n",
      "INFO - Iter 2571/3000, Loss: 0.2215, Avg Adaptive LR: 3.341804\n",
      "INFO - Iter 2581/3000, Loss: 0.2214, Avg Adaptive LR: 3.341801\n",
      "INFO - Iter 2591/3000, Loss: 0.2214, Avg Adaptive LR: 3.341798\n",
      "INFO - Iter 2601/3000, Loss: 0.2213, Avg Adaptive LR: 3.341794\n",
      "INFO - Iter 2611/3000, Loss: 0.2213, Avg Adaptive LR: 3.341791\n",
      "INFO - Iter 2621/3000, Loss: 0.2212, Avg Adaptive LR: 3.341788\n",
      "INFO - Iter 2631/3000, Loss: 0.2212, Avg Adaptive LR: 3.341784\n",
      "INFO - Iter 2641/3000, Loss: 0.2211, Avg Adaptive LR: 3.341781\n",
      "INFO - Iter 2651/3000, Loss: 0.2211, Avg Adaptive LR: 3.341778\n",
      "INFO - Iter 2661/3000, Loss: 0.2210, Avg Adaptive LR: 3.341774\n",
      "INFO - Iter 2671/3000, Loss: 0.2210, Avg Adaptive LR: 3.341771\n",
      "INFO - Iter 2681/3000, Loss: 0.2209, Avg Adaptive LR: 3.341768\n",
      "INFO - Iter 2691/3000, Loss: 0.2209, Avg Adaptive LR: 3.341765\n",
      "INFO - Iter 2701/3000, Loss: 0.2208, Avg Adaptive LR: 3.341761\n",
      "INFO - Iter 2711/3000, Loss: 0.2208, Avg Adaptive LR: 3.341758\n",
      "INFO - Iter 2721/3000, Loss: 0.2208, Avg Adaptive LR: 3.341755\n",
      "INFO - Iter 2731/3000, Loss: 0.2207, Avg Adaptive LR: 3.341752\n",
      "INFO - Iter 2741/3000, Loss: 0.2207, Avg Adaptive LR: 3.341749\n",
      "INFO - Iter 2751/3000, Loss: 0.2206, Avg Adaptive LR: 3.341746\n",
      "INFO - Iter 2761/3000, Loss: 0.2206, Avg Adaptive LR: 3.341743\n",
      "INFO - Iter 2771/3000, Loss: 0.2205, Avg Adaptive LR: 3.341740\n",
      "INFO - Iter 2781/3000, Loss: 0.2205, Avg Adaptive LR: 3.341736\n",
      "INFO - Iter 2791/3000, Loss: 0.2205, Avg Adaptive LR: 3.341733\n",
      "INFO - Iter 2801/3000, Loss: 0.2204, Avg Adaptive LR: 3.341730\n",
      "INFO - Iter 2811/3000, Loss: 0.2204, Avg Adaptive LR: 3.341727\n",
      "INFO - Iter 2821/3000, Loss: 0.2203, Avg Adaptive LR: 3.341724\n",
      "INFO - Iter 2831/3000, Loss: 0.2203, Avg Adaptive LR: 3.341721\n",
      "INFO - Iter 2841/3000, Loss: 0.2202, Avg Adaptive LR: 3.341718\n",
      "INFO - Iter 2851/3000, Loss: 0.2202, Avg Adaptive LR: 3.341715\n",
      "INFO - Iter 2861/3000, Loss: 0.2202, Avg Adaptive LR: 3.341712\n",
      "INFO - Iter 2871/3000, Loss: 0.2201, Avg Adaptive LR: 3.341709\n",
      "INFO - Iter 2881/3000, Loss: 0.2201, Avg Adaptive LR: 3.341706\n",
      "INFO - Iter 2891/3000, Loss: 0.2200, Avg Adaptive LR: 3.341704\n",
      "INFO - Iter 2901/3000, Loss: 0.2200, Avg Adaptive LR: 3.341701\n",
      "INFO - Iter 2911/3000, Loss: 0.2200, Avg Adaptive LR: 3.341698\n",
      "INFO - Iter 2921/3000, Loss: 0.2199, Avg Adaptive LR: 3.341695\n",
      "INFO - Iter 2931/3000, Loss: 0.2199, Avg Adaptive LR: 3.341692\n",
      "INFO - Iter 2941/3000, Loss: 0.2198, Avg Adaptive LR: 3.341689\n",
      "INFO - Iter 2951/3000, Loss: 0.2198, Avg Adaptive LR: 3.341686\n",
      "INFO - Iter 2961/3000, Loss: 0.2198, Avg Adaptive LR: 3.341684\n",
      "INFO - Iter 2971/3000, Loss: 0.2197, Avg Adaptive LR: 3.341681\n",
      "INFO - Iter 2981/3000, Loss: 0.2197, Avg Adaptive LR: 3.341678\n",
      "INFO - Iter 2991/3000, Loss: 0.2196, Avg Adaptive LR: 3.341675\n",
      "INFO - SoftmaxRegression training completed in 117.53 seconds.\n",
      "INFO - --- Linear Regression LR=0.1/Iter=3000 ---\n",
      "INFO - Iter 100/3000, Loss: 0.7045, Gradient Norm: 15.9743, Avg Adaptive LR: 1.3979463815447688\n",
      "INFO - Iter 200/3000, Loss: 0.3775, Gradient Norm: 11.3865, Avg Adaptive LR: 0.9918208472885416\n",
      "INFO - Iter 300/3000, Loss: 0.2592, Gradient Norm: 9.1784, Avg Adaptive LR: 0.8110873012350543\n",
      "INFO - Iter 400/3000, Loss: 0.1986, Gradient Norm: 7.8071, Avg Adaptive LR: 0.703104381147389\n",
      "INFO - Iter 500/3000, Loss: 0.1627, Gradient Norm: 6.8673, Avg Adaptive LR: 0.6292966829840383\n",
      "INFO - Iter 600/3000, Loss: 0.1385, Gradient Norm: 6.1567, Avg Adaptive LR: 0.5747538485278758\n",
      "INFO - Iter 700/3000, Loss: 0.1213, Gradient Norm: 5.5941, Avg Adaptive LR: 0.5323208681026976\n",
      "INFO - Iter 800/3000, Loss: 0.1090, Gradient Norm: 5.1562, Avg Adaptive LR: 0.49810426582336054\n",
      "INFO - Iter 900/3000, Loss: 0.0996, Gradient Norm: 4.7965, Avg Adaptive LR: 0.4697292357526956\n",
      "INFO - Iter 1000/3000, Loss: 0.0924, Gradient Norm: 4.4997, Avg Adaptive LR: 0.44573554216118855\n",
      "INFO - Iter 1100/3000, Loss: 0.0864, Gradient Norm: 4.2390, Avg Adaptive LR: 0.42507792087200574\n",
      "INFO - Iter 1200/3000, Loss: 0.0821, Gradient Norm: 4.0369, Avg Adaptive LR: 0.4070402974506587\n",
      "INFO - Iter 1300/3000, Loss: 0.0772, Gradient Norm: 3.8025, Avg Adaptive LR: 0.3911407209984137\n",
      "INFO - Iter 1400/3000, Loss: 0.0734, Gradient Norm: 3.6049, Avg Adaptive LR: 0.3769569667387786\n",
      "INFO - Iter 1500/3000, Loss: 0.0700, Gradient Norm: 3.4213, Avg Adaptive LR: 0.36421275399835285\n",
      "INFO - Iter 1600/3000, Loss: 0.0673, Gradient Norm: 3.2648, Avg Adaptive LR: 0.3526889621450347\n",
      "INFO - Iter 1700/3000, Loss: 0.0649, Gradient Norm: 3.1215, Avg Adaptive LR: 0.34218627650098443\n",
      "INFO - Iter 1800/3000, Loss: 0.0628, Gradient Norm: 2.9903, Avg Adaptive LR: 0.3325771051971342\n",
      "INFO - Iter 1900/3000, Loss: 0.0611, Gradient Norm: 2.8801, Avg Adaptive LR: 0.32373194838511077\n",
      "INFO - Iter 2000/3000, Loss: 0.0596, Gradient Norm: 2.7823, Avg Adaptive LR: 0.31555907201227723\n",
      "INFO - Iter 2100/3000, Loss: 0.0582, Gradient Norm: 2.6796, Avg Adaptive LR: 0.3079771645118265\n",
      "INFO - Iter 2200/3000, Loss: 0.0572, Gradient Norm: 2.6108, Avg Adaptive LR: 0.3009182380646709\n",
      "INFO - Iter 2300/3000, Loss: 0.0559, Gradient Norm: 2.5158, Avg Adaptive LR: 0.29432099415070795\n",
      "INFO - Iter 2400/3000, Loss: 0.0551, Gradient Norm: 2.4538, Avg Adaptive LR: 0.288142205816417\n",
      "INFO - Iter 2500/3000, Loss: 0.0541, Gradient Norm: 2.3810, Avg Adaptive LR: 0.2823368335314519\n",
      "INFO - Iter 2600/3000, Loss: 0.0534, Gradient Norm: 2.3190, Avg Adaptive LR: 0.276870984682765\n",
      "INFO - Iter 2700/3000, Loss: 0.0527, Gradient Norm: 2.2656, Avg Adaptive LR: 0.2717090318463083\n",
      "INFO - Iter 2800/3000, Loss: 0.0521, Gradient Norm: 2.2103, Avg Adaptive LR: 0.26682415233196355\n",
      "INFO - Iter 2900/3000, Loss: 0.0516, Gradient Norm: 2.1659, Avg Adaptive LR: 0.26219582309900336\n",
      "INFO - Iter 3000/3000, Loss: 0.0510, Gradient Norm: 2.1225, Avg Adaptive LR: 0.25780108446018046\n",
      "INFO - LinearRegressionClassifier training completed in 97.68 seconds.\n",
      "Train Regressions: 100%|██████████| 4/4 [06:08<00:00, 92.02s/it] \n",
      "INFO - Training complete for Softmax and Linear.\n",
      "INFO - === TRAINING PERCEPTRON MODELS (Clean & Pocket) ===\n",
      "Train Clean & Pocket:   0%|          | 0/4 [00:00<?, ?it/s]INFO - --- Clean PLA, max_iter=50 ---\n",
      "INFO - Training for digit 0...\n",
      "INFO - Training for digit 1...\n",
      "INFO - Training for digit 2...\n",
      "INFO - Training for digit 3...\n",
      "INFO - Training for digit 4...\n",
      "INFO - Training for digit 5...\n",
      "INFO - Training for digit 6...\n",
      "INFO - Training for digit 7...\n",
      "INFO - Training for digit 8...\n",
      "INFO - Training for digit 9...\n",
      "INFO - --- Pocket PLA, max_iter=50 ---\n",
      "INFO - Training for digit 0...\n",
      "INFO - Training for digit 1...\n",
      "INFO - Training for digit 2...\n",
      "INFO - Training for digit 3...\n",
      "INFO - Training for digit 4...\n",
      "INFO - Training for digit 5...\n",
      "INFO - Training for digit 6...\n",
      "INFO - Training for digit 7...\n",
      "INFO - Training for digit 8...\n",
      "INFO - Training for digit 9...\n",
      "Train Clean & Pocket:  25%|██▌       | 1/4 [00:47<02:22, 47.51s/it]INFO - --- Clean PLA, max_iter=100 ---\n",
      "INFO - Training for digit 0...\n",
      "INFO - Training for digit 1...\n",
      "INFO - Training for digit 2...\n",
      "INFO - Training for digit 3...\n",
      "INFO - Training for digit 4...\n",
      "INFO - Training for digit 5...\n",
      "INFO - Training for digit 6...\n",
      "INFO - Training for digit 7...\n",
      "INFO - Training for digit 8...\n",
      "INFO - Training for digit 9...\n",
      "INFO - --- Pocket PLA, max_iter=100 ---\n",
      "INFO - Training for digit 0...\n",
      "INFO - Training for digit 1...\n",
      "INFO - Training for digit 2...\n",
      "INFO - Training for digit 3...\n",
      "INFO - Training for digit 4...\n",
      "INFO - Training for digit 5...\n",
      "INFO - Training for digit 6...\n",
      "INFO - Training for digit 7...\n",
      "INFO - Training for digit 8...\n",
      "INFO - Training for digit 9...\n",
      "Train Clean & Pocket:  50%|█████     | 2/4 [02:10<02:17, 68.52s/it]INFO - --- Clean PLA, max_iter=2000 ---\n",
      "INFO - Training for digit 0...\n",
      "INFO - Training for digit 1...\n",
      "INFO - Training for digit 2...\n",
      "INFO - Training for digit 3...\n",
      "INFO - Training for digit 4...\n",
      "INFO - Training for digit 5...\n",
      "INFO - Training for digit 6...\n",
      "INFO - Training for digit 7...\n",
      "INFO - Training for digit 8...\n",
      "INFO - Training for digit 9...\n",
      "INFO - --- Pocket PLA, max_iter=2000 ---\n",
      "INFO - Training for digit 0...\n",
      "INFO - Training for digit 1...\n",
      "INFO - Training for digit 2...\n",
      "INFO - Training for digit 3...\n",
      "INFO - Training for digit 4...\n",
      "INFO - Training for digit 5...\n",
      "INFO - Training for digit 6...\n",
      "INFO - Training for digit 7...\n",
      "INFO - Training for digit 8...\n",
      "INFO - Training for digit 9...\n",
      "Train Clean & Pocket:  75%|███████▌  | 3/4 [24:05<10:37, 637.44s/it]INFO - --- Clean PLA, max_iter=3000 ---\n",
      "INFO - Training for digit 0...\n",
      "INFO - Training for digit 1...\n",
      "INFO - Training for digit 2...\n",
      "INFO - Training for digit 3...\n",
      "INFO - Training for digit 4...\n",
      "INFO - Training for digit 5...\n",
      "INFO - Training for digit 6...\n",
      "INFO - Training for digit 7...\n",
      "INFO - Training for digit 8...\n",
      "INFO - Training for digit 9...\n",
      "INFO - --- Pocket PLA, max_iter=3000 ---\n",
      "INFO - Training for digit 0...\n",
      "INFO - Training for digit 1...\n",
      "INFO - Training for digit 2...\n",
      "INFO - Training for digit 3...\n",
      "INFO - Training for digit 4...\n",
      "INFO - Training for digit 5...\n",
      "INFO - Training for digit 6...\n",
      "INFO - Training for digit 7...\n",
      "INFO - Training for digit 8...\n",
      "INFO - Training for digit 9...\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# TRAINING CELL\n",
    "# =========================================================\n",
    "\n",
    "# 1) Dictionaries to store trained models\n",
    "trained_models_clean   = {}\n",
    "trained_models_pocket  = {}\n",
    "trained_models_softmax = {}\n",
    "trained_models_linear  = {}\n",
    "\n",
    "# 2) Train Regression Models (Softmax & Linear)\n",
    "logger.info(\"=== TRAINING REGRESSION MODELS (Softmax & Linear) ===\")\n",
    "for cfg in tqdm(regression_run_configs, desc=\"Train Regressions\"):\n",
    "    lr_val = cfg[\"learning_rate\"]\n",
    "    max_iter_val = cfg[\"max_iter\"]\n",
    "    label = cfg[\"label\"]  # e.g. \"LR=0.001/Iter=1000\"\n",
    "\n",
    "    # --- Softmax ---\n",
    "    logger.info(f\"--- Softmax {label} ---\")\n",
    "    s_model = SoftmaxRegression(\n",
    "        num_classes=10,\n",
    "        max_iter=max_iter_val,\n",
    "        learning_rate=lr_val,\n",
    "        adaptive_lr=True\n",
    "    )\n",
    "    s_model.fit(X_train, y_train)\n",
    "    trained_models_softmax[(lr_val, max_iter_val)] = s_model\n",
    "\n",
    "    # --- Linear ---\n",
    "    logger.info(f\"--- Linear Regression {label} ---\")\n",
    "    lin_model = LinearRegression(\n",
    "        num_classes=10,\n",
    "        max_iter=max_iter_val,\n",
    "        learning_rate=lr_val,\n",
    "        adaptive_lr=True,\n",
    "        early_stopping=False\n",
    "    )\n",
    "    lin_model.fit(X_train, y_train)\n",
    "    trained_models_linear[(lr_val, max_iter_val)] = lin_model\n",
    "\n",
    "logger.info(\"Training complete for Softmax and Linear.\")\n",
    "\n",
    "# 3) Train Perceptron Models (Clean & Pocket)\n",
    "logger.info(\"=== TRAINING PERCEPTRON MODELS (Clean & Pocket) ===\")\n",
    "for max_iter in tqdm(perceptron_max_iter_values, desc=\"Train Clean & Pocket\"):\n",
    "    logger.info(f\"--- Clean PLA, max_iter={max_iter} ---\")\n",
    "    clean_perc = MultiClassPerceptron(num_classes=10, max_iter=max_iter, use_pocket=False)\n",
    "    clean_perc.fit(X_train, y_train)\n",
    "    trained_models_clean[max_iter] = clean_perc\n",
    "\n",
    "    logger.info(f\"--- Pocket PLA, max_iter={max_iter} ---\")\n",
    "    pocket_perc = MultiClassPerceptron(num_classes=10, max_iter=max_iter, use_pocket=True)\n",
    "    pocket_perc.fit(X_train, y_train)\n",
    "    trained_models_pocket[max_iter] = pocket_perc\n",
    "\n",
    "logger.info(\"Training complete for Clean PLA and Pocket PLA.\")\n",
    "logger.info(\"=== ALL TRAINING COMPLETE ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VepUzPkWrANq"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Hech6_lrANq"
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# EVALUATION CELL (with pandas DataFrame)\n",
    "##################################################\n",
    "\n",
    "\n",
    "# 1) Evaluate Perceptrons: Clean & Pocket\n",
    "accuracies_clean, accuracies_pocket = [], []\n",
    "runtimes_clean,   runtimes_pocket   = [], []\n",
    "sensitivities_clean, sensitivities_pocket = [], []\n",
    "selectivities_clean, selectivities_pocket = [], []\n",
    "\n",
    "conf_clean, conf_pocket = [], []\n",
    "meta_clean, meta_pocket = [], []\n",
    "\n",
    "for max_iter in tqdm(perceptron_max_iter_values, desc=\"Evaluate Clean & Pocket\"):\n",
    "    # === Evaluate Clean PLA ===\n",
    "    c_model = trained_models_clean[max_iter]\n",
    "    cm_c, acc_c, s_c, sp_c, rt_c, ex_c = evaluate_model(\n",
    "        c_model, X_test, y_test, classes=range(10), model_name=\"Clean PLA\"\n",
    "    )\n",
    "    accuracies_clean.append(acc_c)\n",
    "    runtimes_clean.append(rt_c)\n",
    "    sensitivities_clean.append(np.mean(s_c))\n",
    "    selectivities_clean.append(np.mean(sp_c))\n",
    "    conf_clean.append(cm_c)\n",
    "\n",
    "    cdict = {\n",
    "        \"max_iter\": max_iter,\n",
    "        \"accuracy\": acc_c,\n",
    "        \"runtime\": rt_c,\n",
    "        \"avg_sensitivity\": np.mean(s_c),\n",
    "        \"avg_selectivity\": np.mean(sp_c),\n",
    "        \"method\": \"Clean PLA\"\n",
    "    }\n",
    "    cdict.update(ex_c)\n",
    "    meta_clean.append(cdict)\n",
    "\n",
    "    # === Evaluate Pocket PLA ===\n",
    "    p_model = trained_models_pocket[max_iter]\n",
    "    cm_p, acc_p, s_p, sp_p, rt_p, ex_p = evaluate_model(\n",
    "        p_model, X_test, y_test, classes=range(10), model_name=\"Pocket PLA\"\n",
    "    )\n",
    "    accuracies_pocket.append(acc_p)\n",
    "    runtimes_pocket.append(rt_p)\n",
    "    sensitivities_pocket.append(np.mean(s_p))\n",
    "    selectivities_pocket.append(np.mean(sp_p))\n",
    "    conf_pocket.append(cm_p)\n",
    "\n",
    "    pdict = {\n",
    "        \"max_iter\": max_iter,\n",
    "        \"accuracy\": acc_p,\n",
    "        \"runtime\": rt_p,\n",
    "        \"avg_sensitivity\": np.mean(s_p),\n",
    "        \"avg_selectivity\": np.mean(sp_p),\n",
    "        \"method\": \"Pocket PLA\"\n",
    "    }\n",
    "    pdict.update(ex_p)\n",
    "    meta_pocket.append(pdict)\n",
    "\n",
    "# Aggregated iteration-level training curves for Perceptrons\n",
    "clean_train_curve = aggregate_iteration_losses(\n",
    "    [trained_models_clean[m] for m in perceptron_max_iter_values]\n",
    ")\n",
    "pocket_train_curve = aggregate_iteration_losses(\n",
    "    [trained_models_pocket[m] for m in perceptron_max_iter_values]\n",
    ")\n",
    "\n",
    "# 2) Evaluate Regression Models: Softmax & Linear\n",
    "accuracies_softmax = []\n",
    "runtimes_softmax   = []\n",
    "sensitivities_soft = []\n",
    "selectivities_soft = []\n",
    "conf_soft          = []\n",
    "meta_soft          = []\n",
    "\n",
    "accuracies_linear = []\n",
    "runtimes_linear   = []\n",
    "sensitivities_lin = []\n",
    "selectivities_lin = []\n",
    "conf_linear       = []\n",
    "meta_linear       = []\n",
    "\n",
    "for cfg in tqdm(regression_run_configs, desc=\"Evaluate Regressions\"):\n",
    "    lr_val = cfg[\"learning_rate\"]\n",
    "    max_iter_val = cfg[\"max_iter\"]\n",
    "    label = cfg[\"label\"]\n",
    "\n",
    "    # === Evaluate Softmax ===\n",
    "    s_model = trained_models_softmax[(lr_val, max_iter_val)]\n",
    "    cm_s, a_s, se_s, sp_s, r_s, ex_s = evaluate_model(\n",
    "        s_model, X_test, y_test, classes=range(10),\n",
    "        model_name=f\"Softmax ({label})\"\n",
    "    )\n",
    "    accuracies_softmax.append(a_s)\n",
    "    runtimes_softmax.append(r_s)\n",
    "    sensitivities_soft.append(np.mean(se_s))\n",
    "    selectivities_soft.append(np.mean(sp_s))\n",
    "    conf_soft.append(cm_s)\n",
    "\n",
    "    ms = {\n",
    "        \"label\": label,\n",
    "        \"learning_rate\": lr_val,\n",
    "        \"max_iter\": max_iter_val,\n",
    "        \"accuracy\": a_s,\n",
    "        \"runtime\": r_s,\n",
    "        \"avg_sensitivity\": np.mean(se_s),\n",
    "        \"avg_selectivity\": np.mean(sp_s),\n",
    "        \"method\": \"Softmax\"\n",
    "    }\n",
    "    ms.update(ex_s)\n",
    "    meta_soft.append(ms)\n",
    "\n",
    "    # === Evaluate Linear ===\n",
    "    lin_model = trained_models_linear[(lr_val, max_iter_val)]\n",
    "    cm_l, a_l, se_l, sp_l, r_l, ex_l = evaluate_model(\n",
    "        lin_model, X_test, y_test, classes=range(10),\n",
    "        model_name=f\"Linear ({label})\"\n",
    "    )\n",
    "    accuracies_linear.append(a_l)\n",
    "    runtimes_linear.append(r_l)\n",
    "    sensitivities_lin.append(np.mean(se_l))\n",
    "    selectivities_lin.append(np.mean(sp_l))\n",
    "    conf_linear.append(cm_l)\n",
    "\n",
    "    ml = {\n",
    "        \"label\": label,\n",
    "        \"learning_rate\": lr_val,\n",
    "        \"max_iter\": max_iter_val,\n",
    "        \"accuracy\": a_l,\n",
    "        \"runtime\": r_l,\n",
    "        \"avg_sensitivity\": np.mean(se_l),\n",
    "        \"avg_selectivity\": np.mean(sp_l),\n",
    "        \"method\": \"Linear Regression\"\n",
    "    }\n",
    "    ml.update(ex_l)\n",
    "    meta_linear.append(ml)\n",
    "\n",
    "\n",
    "logger.info(\"Evaluation complete for Perceptrons & Regressions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwCAybO5owmg"
   },
   "source": [
    "# Visualize (Generate Plots, Confusion Matricies, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rC4vaIjVowmg"
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 1) CREATE A SINGLE PANDAS DATAFRAME FOR ALL RESULTS\n",
    "##################################################\n",
    "all_rows = []\n",
    "\n",
    "# (A) Clean PLA\n",
    "for i, max_iter in tqdm(\n",
    "    enumerate(perceptron_max_iter_values),\n",
    "    desc=\"Collecting Clean PLA\",\n",
    "    total=len(perceptron_max_iter_values)\n",
    "):\n",
    "    all_rows.append({\n",
    "        'model': 'Clean PLA',\n",
    "        'max_iter': max_iter,\n",
    "        'runtime': runtimes_clean[i],\n",
    "        'accuracy': accuracies_clean[i],\n",
    "        'sensitivity': sensitivities_clean[i],\n",
    "        'selectivity': selectivities_clean[i]\n",
    "    })\n",
    "\n",
    "# (B) Pocket PLA\n",
    "for i, max_iter in tqdm(\n",
    "    enumerate(perceptron_max_iter_values),\n",
    "    desc=\"Collecting Pocket PLA\",\n",
    "    total=len(perceptron_max_iter_values)\n",
    "):\n",
    "    all_rows.append({\n",
    "        'model': 'Pocket PLA',\n",
    "        'max_iter': max_iter,\n",
    "        'runtime': runtimes_pocket[i],\n",
    "        'accuracy': accuracies_pocket[i],\n",
    "        'sensitivity': sensitivities_pocket[i],\n",
    "        'selectivity': selectivities_pocket[i]\n",
    "    })\n",
    "\n",
    "# (C) Softmax\n",
    "for i, row_meta in tqdm(\n",
    "    enumerate(meta_soft),\n",
    "    desc=\"Collecting Softmax\",\n",
    "    total=len(meta_soft)\n",
    "):\n",
    "    all_rows.append({\n",
    "        'model': 'Softmax',\n",
    "        'max_iter': row_meta['max_iter'],\n",
    "        'runtime': runtimes_softmax[i],\n",
    "        'accuracy': accuracies_softmax[i],\n",
    "        'sensitivity': sensitivities_soft[i],\n",
    "        'selectivity': selectivities_soft[i]\n",
    "    })\n",
    "\n",
    "# (D) Linear\n",
    "for i, row_meta in tqdm(\n",
    "    enumerate(meta_linear),\n",
    "    desc=\"Collecting Linear\",\n",
    "    total=len(meta_linear)\n",
    "):\n",
    "    all_rows.append({\n",
    "        'model': 'Linear',\n",
    "        'max_iter': row_meta['max_iter'],\n",
    "        'runtime': runtimes_linear[i],\n",
    "        'accuracy': accuracies_linear[i],\n",
    "        'sensitivity': sensitivities_lin[i],\n",
    "        'selectivity': selectivities_lin[i]\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(all_rows)\n",
    "logger.info(\"Combined Results DataFrame:\\n%s\", df_results)\n",
    "display(df_results.head(20))\n",
    "\n",
    "############################################################################\n",
    "# 2) CONFUSION MATRICES FOR ALL MODELS (GROUPED BY PLOT TYPE)\n",
    "############################################################################\n",
    "\n",
    "logger.info(\"=== Plotting ALL Confusion Matrices ===\")\n",
    "\n",
    "# 2A) Perceptron: Clean\n",
    "for idx, meta in tqdm(enumerate(meta_clean), total=len(meta_clean), desc=\"Confusions: Clean PLA\"):\n",
    "    title = f\"Clean PLA (max_iter={meta['max_iter']}, Acc={meta['accuracy']*100:.2f}%)\"\n",
    "    plot_confusion_matrix_annotated(\n",
    "        conf_clean[idx],\n",
    "        classes=range(10),\n",
    "        title=title,\n",
    "        method=meta[\"method\"],\n",
    "        max_iter=meta[\"max_iter\"]\n",
    "    )\n",
    "\n",
    "# 2B) Perceptron: Pocket\n",
    "for idx, meta in tqdm(enumerate(meta_pocket), total=len(meta_pocket), desc=\"Confusions: Pocket PLA\"):\n",
    "    title = f\"Pocket PLA (max_iter={meta['max_iter']}, Acc={meta['accuracy']*100:.2f}%)\"\n",
    "    plot_confusion_matrix_annotated(\n",
    "        conf_pocket[idx],\n",
    "        classes=range(10),\n",
    "        title=title,\n",
    "        method=meta[\"method\"],\n",
    "        max_iter=meta[\"max_iter\"]\n",
    "    )\n",
    "\n",
    "# 2C) Softmax\n",
    "for idx, meta in tqdm(enumerate(meta_soft), total=len(meta_soft), desc=\"Confusions: Softmax\"):\n",
    "    title = f\"Softmax ({meta['label']}, Acc={meta['accuracy']*100:.2f}%)\"\n",
    "    plot_confusion_matrix_annotated(\n",
    "        conf_soft[idx],\n",
    "        classes=range(10),\n",
    "        title=title,\n",
    "        method=meta[\"method\"],\n",
    "        max_iter=meta[\"max_iter\"]\n",
    "    )\n",
    "\n",
    "# 2D) Linear\n",
    "for idx, meta in tqdm(enumerate(meta_linear), total=len(meta_linear), desc=\"Confusions: Linear\"):\n",
    "    title = f\"Linear ({meta['label']}, Acc={meta['accuracy']*100:.2f}%)\"\n",
    "    plot_confusion_matrix_annotated(\n",
    "        conf_linear[idx],\n",
    "        classes=range(10),\n",
    "        title=title,\n",
    "        method=meta[\"method\"],\n",
    "        max_iter=meta[\"max_iter\"]\n",
    "    )\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# 3) ITERATION-LEVEL PLOTS (ALL MODELS)\n",
    "############################################################################\n",
    "\n",
    "logger.info(\"=== Iteration-Level Visualization (All Models) ===\")\n",
    "\n",
    "# 3A) Perceptron: Clean & Pocket\n",
    "for max_iter, c_model in trained_models_clean.items():\n",
    "    df_iter = c_model.get_iteration_df()\n",
    "    if not df_iter.empty and \"train_error\" in df_iter.columns:\n",
    "        title = f\"Clean PLA max_iter={max_iter}: Train Error vs. Iteration\"\n",
    "        df_iter.plot(x=\"iteration\", y=\"train_error\", marker='o', figsize=(8,5), title=title)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "for max_iter, p_model in trained_models_pocket.items():\n",
    "    df_iter = p_model.get_iteration_df()\n",
    "    if not df_iter.empty and \"train_error\" in df_iter.columns:\n",
    "        title = f\"Pocket PLA max_iter={max_iter}: Train Error vs. Iteration\"\n",
    "        df_iter.plot(x=\"iteration\", y=\"train_error\", marker='o', figsize=(8,5), title=title)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "# 3B) Softmax\n",
    "for (lr_val, max_iter_val), s_model in trained_models_softmax.items():\n",
    "    df_iter = s_model.get_iteration_df()  # Must be implemented in your SoftmaxRegression\n",
    "    if not df_iter.empty:\n",
    "        title = f\"Softmax LR={lr_val}, max_iter={max_iter_val}: Train Loss vs. Iteration\"\n",
    "        df_iter.plot(x=\"iteration\", y=\"train_loss\", marker='o', figsize=(8,5), title=title)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "        if \"test_loss\" in df_iter.columns:\n",
    "            title = f\"Softmax LR={lr_val}, max_iter={max_iter_val}: Train & Test Loss\"\n",
    "            df_iter.plot(x=\"iteration\", y=[\"train_loss\",\"test_loss\"], marker='o', figsize=(8,5), title=title)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.show()\n",
    "\n",
    "        if \"avg_adaptive_lr\" in df_iter.columns:\n",
    "            title = f\"Softmax LR={lr_val}, max_iter={max_iter_val}: Avg Adaptive LR vs. Iteration\"\n",
    "            df_iter.plot(x=\"iteration\", y=\"avg_adaptive_lr\", marker='x', figsize=(8,5), title=title)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.show()\n",
    "\n",
    "# 3C) Linear\n",
    "for (lr_val, max_iter_val), lin_model in trained_models_linear.items():\n",
    "    df_iter = lin_model.get_iteration_df()  # Must be implemented in your LinearRegression\n",
    "    if not df_iter.empty:\n",
    "        title = f\"Linear LR={lr_val}, max_iter={max_iter_val}: Train Loss vs. Iteration\"\n",
    "        df_iter.plot(x=\"iteration\", y=\"train_loss\", marker='o', figsize=(8,5), title=title)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "        if \"test_loss\" in df_iter.columns:\n",
    "            title = f\"Linear LR={lr_val}, max_iter={max_iter_val}: Train & Test Loss\"\n",
    "            df_iter.plot(x=\"iteration\", y=[\"train_loss\",\"test_loss\"], marker='o', figsize=(8,5), title=title)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.show()\n",
    "\n",
    "        if \"avg_adaptive_lr\" in df_iter.columns:\n",
    "            title = f\"Linear LR={lr_val}, max_iter={max_iter_val}: Avg Adaptive LR vs. Iteration\"\n",
    "            df_iter.plot(x=\"iteration\", y=\"avg_adaptive_lr\", marker='x', figsize=(8,5), title=title)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# 4) PANDAS + SEABORN PLOTS\n",
    "############################################################################\n",
    "\n",
    "logger.info(\"=== Pandas + Seaborn Plots ===\")\n",
    "\n",
    "# 4A) LINE PLOT: Accuracy vs. max_iter (Perceptrons Only)\n",
    "df_perc = df_results[df_results['model'].isin(['Clean PLA','Pocket PLA'])].copy()\n",
    "df_perc.sort_values(['model','max_iter'], inplace=True)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.lineplot(\n",
    "    data=df_perc,\n",
    "    x='max_iter', y='accuracy',\n",
    "    hue='model', marker='o'\n",
    ")\n",
    "plt.title(\"Perceptrons: Accuracy vs. max_iter (Pandas/Seaborn)\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# 4B) BAR CHART: Average Accuracy by Model\n",
    "df_mean = df_results.groupby('model', as_index=False)['accuracy'].mean()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(data=df_mean, x='model', y='accuracy')\n",
    "plt.title(\"Average Accuracy by Model (Pandas/Seaborn)\")\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# 4C) SCATTER PLOT: Accuracy vs. Runtime, colored by model\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(\n",
    "    data=df_results,\n",
    "    x='runtime', y='accuracy',\n",
    "    hue='model', style='model',\n",
    "    s=100\n",
    ")\n",
    "plt.title(\"Accuracy vs. Runtime (All Models) (Pandas/Seaborn)\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# 5) CUSTOM SUMMARY PLOTS (AGGREGATED CURVES, ETC.)\n",
    "############################################################################\n",
    "\n",
    "logger.info(\"=== Custom Summaries (Aggregated Curves, etc.) ===\")\n",
    "\n",
    "# 5A) Aggregated Perceptron Curves\n",
    "plot_train_curves_three_models(\n",
    "    clean_train_curve=clean_train_curve,\n",
    "    pocket_train_curve=pocket_train_curve,\n",
    "    softmax_train_curve=None,  # no Softmax aggregator\n",
    "    title=\"Aggregated Perceptron Train Curves (Clean vs. Pocket)\",\n",
    "    max_iter=perceptron_max_iter_values[-1]\n",
    ")\n",
    "\n",
    "# 5B) Summaries for Perceptron\n",
    "plot_accuracy_vs_max_iter(\n",
    "    max_iter_values=perceptron_max_iter_values,\n",
    "    accuracies_clean=accuracies_clean,\n",
    "    accuracies_pocket=accuracies_pocket,\n",
    "    accuracies_softmax=None\n",
    ")\n",
    "\n",
    "plot_runtime_vs_max_iter(\n",
    "    max_iter_values=perceptron_max_iter_values,\n",
    "    runtimes_clean=runtimes_clean,\n",
    "    runtimes_pocket=runtimes_pocket,\n",
    "    runtimes_softmax=None\n",
    ")\n",
    "\n",
    "plot_accuracy_vs_runtime(\n",
    "    runtimes_clean=runtimes_clean,\n",
    "    accuracies_clean=accuracies_clean,\n",
    "    runtimes_pocket=runtimes_pocket,\n",
    "    accuracies_pocket=accuracies_pocket,\n",
    "    title=\"Perceptrons: Accuracy vs. Runtime\"\n",
    ")\n",
    "\n",
    "plot_performance_summary_extended_by_runtime(\n",
    "    runtimes_clean=runtimes_clean,\n",
    "    accuracies_clean=accuracies_clean,\n",
    "    sensitivities_clean=sensitivities_clean,\n",
    "    selectivities_clean=selectivities_clean,\n",
    "    runtimes_pocket=runtimes_pocket,\n",
    "    accuracies_pocket=accuracies_pocket,\n",
    "    sensitivities_pocket=sensitivities_pocket,\n",
    "    selectivities_pocket=selectivities_pocket,\n",
    "    title=\"Perceptrons: Performance vs. Runtime\"\n",
    ")\n",
    "\n",
    "# 5C) Summaries for Softmax & Linear\n",
    "plot_accuracy_vs_runtime(\n",
    "    runtimes_clean=runtimes_softmax,\n",
    "    accuracies_clean=accuracies_softmax,\n",
    "    title=\"Softmax: Accuracy vs. Runtime\"\n",
    ")\n",
    "plot_accuracy_vs_runtime(\n",
    "    runtimes_clean=runtimes_linear,\n",
    "    accuracies_clean=accuracies_linear,\n",
    "    title=\"Linear: Accuracy vs. Runtime\"\n",
    ")\n",
    "plot_accuracy_vs_runtime(\n",
    "    runtimes_clean=runtimes_softmax,\n",
    "    accuracies_clean=accuracies_softmax,\n",
    "    runtimes_pocket=runtimes_linear,\n",
    "    accuracies_pocket=accuracies_linear,\n",
    "    title=\"Softmax vs. Linear: Accuracy vs. Runtime\"\n",
    ")\n",
    "plot_performance_summary_extended_by_runtime(\n",
    "    runtimes_clean=runtimes_softmax,\n",
    "    accuracies_clean=accuracies_softmax,\n",
    "    sensitivities_clean=sensitivities_soft,\n",
    "    selectivities_clean=selectivities_soft,\n",
    "    runtimes_pocket=runtimes_linear,\n",
    "    accuracies_pocket=accuracies_linear,\n",
    "    sensitivities_pocket=sensitivities_lin,\n",
    "    selectivities_pocket=selectivities_lin,\n",
    "    title=\"Softmax vs. Linear: TPR/TNR vs. Runtime\"\n",
    ")\n",
    "\n",
    "# 5D) 4-Model Comparison\n",
    "plot_performance_summary_4models_by_runtime(\n",
    "    runtimes_clean, accuracies_clean, sensitivities_clean, selectivities_clean,\n",
    "    runtimes_pocket, accuracies_pocket, sensitivities_pocket, selectivities_pocket,\n",
    "    runtimes_softmax, accuracies_softmax, sensitivities_soft, selectivities_soft,\n",
    "    runtimes_linear, accuracies_linear, sensitivities_lin, selectivities_lin,\n",
    "    title=\"Performance vs. Runtime (4-Model Comparison)\"\n",
    ")\n",
    "\n",
    "plot_accuracy_vs_runtime_4models(\n",
    "    rt_clean=runtimes_clean,\n",
    "    acc_clean=accuracies_clean,\n",
    "    rt_pocket=runtimes_pocket,\n",
    "    acc_pocket=accuracies_pocket,\n",
    "    rt_softmax=runtimes_softmax,\n",
    "    acc_softmax=accuracies_softmax,\n",
    "    rt_linear=runtimes_linear,\n",
    "    acc_linear=accuracies_linear,\n",
    "    title=\"Accuracy vs. Runtime (4 Models)\"\n",
    ")\n",
    "\n",
    "logger.info(\"=== All Visualizations Complete ===\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "cell_execution_strategy": "setup",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
