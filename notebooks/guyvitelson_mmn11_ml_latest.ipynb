{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v1t3ls0n/ml_intro_course_mmn11/blob/main/notebooks/guyvitelson_mmn11_ml_latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ztlf-kRcUjIz"
      },
      "source": [
        "# ממן 11 - מבוא ללמידה חישובית - סמסטר 2025ב - גיא ויטלזון 203379706"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXiZOQmO5Tak"
      },
      "source": [
        "##**If you run this within Google Collab, Dont Worry!**\n",
        "all the missing python files/directories/modules will be automatically feteched from my github repository\n",
        "\n",
        "**My GitHub Profile** : https://github.com/v1t3ls0n\n",
        "\n",
        "**The Repository:** https://github.com/v1t3ls0n/ml_intro_course_mmn11\n",
        "\n",
        "**Student ID:** 203379706"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqLIUat1dEr2"
      },
      "source": [
        "## Fetch Resources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmauUwgLR-mx"
      },
      "source": [
        "### External Code Imports (pip packages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUkaAHQFR-mx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import logging\n",
        "import numpy as np # type: ignore\n",
        "import matplotlib.pyplot as plt # type: ignore\n",
        "import seaborn as sns # type: ignore\n",
        "import time\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlMUAQraR-my"
      },
      "source": [
        "### Fetch Missing Files For Google Colab Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msKnbktXR-my"
      },
      "outputs": [],
      "source": [
        "\n",
        "# %%capture run_output\n",
        "# %matplotlib inline\n",
        "\n",
        "if sys.platform != 'win32': # check if we are running on google collab\n",
        "  repo_url = \"https://github.com/v1t3ls0n/ml_intro_course_mmn11\"\n",
        "  repo_name = \"ml_intro_course_mmn11\"\n",
        "  from tqdm.notebook import tqdm # type: ignore\n",
        "\n",
        "\n",
        "  # Clone the repository if it doesn't exist\n",
        "  if not os.path.exists(repo_name):\n",
        "    os.system(f\"git clone {repo_url}\")\n",
        "\n",
        "  # Construct the path to the repository directory\n",
        "  repo_path = os.path.join(os.getcwd(), repo_name)\n",
        "\n",
        "  # Add the repository directory to the Python path\n",
        "  if repo_path not in sys.path:\n",
        "    sys.path.insert(0, repo_path)\n",
        "\n",
        "  # --- Extract 'core' and 'notebooks' directories ---\n",
        "  def extract_directories(source_dir, destination_dir, dir_names):\n",
        "      for dir_name in dir_names:\n",
        "          source_path = os.path.join(source_dir, dir_name)\n",
        "          destination_path = os.path.join(destination_dir, dir_name)\n",
        "          if os.path.exists(source_path):\n",
        "              shutil.copytree(source_path, destination_path, dirs_exist_ok=True)\n",
        "\n",
        "  destination_path = \".\"\n",
        "  # Extract the directories\n",
        "  extract_directories(repo_path, destination_path, [\"core\"])\n",
        "  project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "  sys.path.insert(0, project_root)\n",
        "  if os.path.exists(\"ml_intro_course_mmn11\"):\n",
        "    shutil.rmtree(\"ml_intro_course_mmn11\")\n",
        "  if os.path.exists(\"sample_data\"):\n",
        "    shutil.rmtree(\"sample_data\")\n",
        "else:\n",
        "  from tqdm import tqdm  # type: ignore\n",
        "  current_dir = os.getcwd()  # Current working directory\n",
        "  project_root = os.path.abspath(os.path.join(current_dir, '..'))  # Root directory of the project\n",
        "  sys.path.insert(0, project_root)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYrRE0dcR-my"
      },
      "source": [
        "### Internal Code Imports (original code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTaL_MsqeE00"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========== Internal Code Imports ==========\n",
        "\n",
        "#Logger\n",
        "from core.logger.config import logger\n",
        "\n",
        "# Data Preprocessing\n",
        "from core.data.mnist_loader import load_mnist\n",
        "from core.data.data_preprocessing import preprocess_data\n",
        "\n",
        "# Models\n",
        "from core.models.perceptron.multi_class_perceptron import MultiClassPerceptron\n",
        "from core.models.logistic_regression.softmax_lregression import SoftmaxRegression\n",
        "from core.models.linear_regression.linear_regression import  LinearRegression\n",
        "\n",
        "# Performance & Plotting\n",
        "from core.analysis.evaluation_functions import (\n",
        "    evaluate_model,\n",
        "    aggregate_iteration_losses,\n",
        "    aggregate_iteration_losses_softmax\n",
        ")\n",
        "\n",
        "from core.analysis.plotting import (\n",
        "    plot_confusion_matrix_annotated,\n",
        "    plot_error_curves,\n",
        "    plot_accuracy_vs_max_iter,\n",
        "    plot_runtime_vs_max_iter,\n",
        "    plot_performance_summary_extended,\n",
        "    plot_train_curves_three_models,\n",
        "    plot_metric_vs_learning_rate,\n",
        "    plot_accuracy_vs_max_iter_4models,\n",
        "    plot_runtime_vs_max_iter_4models,\n",
        "    plot_accuracy_vs_runtime,\n",
        "    plot_performance_summary_extended_by_runtime,\n",
        "    plot_performance_summary_4models_by_runtime,\n",
        "    plot_accuracy_vs_runtime_4models\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(\"MyGlobalLogger\") # configured in core/logger/config.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKs0r5ROHxuY"
      },
      "source": [
        "# Overview\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWMyo4jnR-mx"
      },
      "source": [
        "## MNIST Digit Classification Using Perceptron Learning Algorithm (PLA)\n",
        "\n",
        "**Objective:**  \n",
        "This notebook compares the performance of two variants of the Perceptron Learning Algorithm (PLA) on the MNIST digit classification task:\n",
        "- **Clean PLA:** Standard perceptron without enhancements.\n",
        "- **Pocket PLA:** Enhanced perceptron that stores the best-performing weights during training (using the Pocket algorithm).\n",
        "\n",
        "**Dataset:**  \n",
        "- MNIST dataset consisting of 60,000 training samples and 10,000 test samples.\n",
        "- The images are normalized to the range [0, 1] and a bias term is added, resulting in input samples with 785 features.\n",
        "\n",
        "**Evaluation Metrics:**  \n",
        "- **Confusion Matrices:** Provides a detailed view of how well each digit is classified.\n",
        "- **Overall Accuracy (ACC):** Defined as \\(\\text{ACC} = \\frac{TP + TN}{TP + TN + FP + FN}\\).\n",
        "- **Sensitivity (True Positive Rate, TPR):** For each digit, calculated as \\(\\text{TPR} = \\frac{TP}{TP + FN}\\), showing the model’s ability to correctly identify the digit.\n",
        "- **Selectivity (Specificity, TNR):** For each digit, calculated as \\(\\text{TNR} = \\frac{TN}{TN + FP}\\), showing the model’s ability to correctly identify negatives.\n",
        "- **Training and Testing Error Curves:** Visualized as a function of iteration for detailed analysis of learning dynamics.\n",
        "- **Runtime:** The time taken to train the models.\n",
        "\n",
        "**Goals:**  \n",
        "- Evaluate and compare the model accuracy and robustness between Clean PLA and Pocket PLA.\n",
        "- Analyze and visualize the performance through confusion matrices, error curves, and summary plots (accuracy, sensitivity, selectivity, and runtime vs. the number of iterations).\n",
        "- Provide a comprehensive discussion on how training iterations affect the decision boundaries and the overall performance, particularly in the one-vs-all classification setup.\n",
        "\n",
        "This notebook integrates detailed quantitative evaluation with comprehensive visualizations to thoroughly analyze the multi-class Perceptron performance on the MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keDSGERzwvrB"
      },
      "source": [
        "# Choose Run Parameters **(Significant Effect On Model's Runtime!)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNJHO7mQrANq",
        "outputId": "6b4dd274-1d3f-4489-9ebd-ebb7c4299b04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-18 21:25:45,596 - INFO - === Perceptron Run Parameters ===\n",
            "2025-03-18 21:25:45,597 - INFO - max_iter_values = [20, 50, 100, 1000]\n",
            "2025-03-18 21:25:45,597 - INFO - === Regression Run Parameters ===\n",
            "2025-03-18 21:25:45,598 - INFO - LR=0.1/Iter=100 -> learning_rate=0.1, max_iter=100\n",
            "2025-03-18 21:25:45,598 - INFO - LR=0.1/Iter=1000 -> learning_rate=0.1, max_iter=1000\n",
            "2025-03-18 21:25:45,599 - INFO - LR=0.1/Iter=10000 -> learning_rate=0.1, max_iter=10000\n",
            "2025-03-18 21:25:45,597 - INFO - max_iter_values = [20, 50, 100, 1000]\n",
            "2025-03-18 21:25:45,597 - INFO - === Regression Run Parameters ===\n",
            "2025-03-18 21:25:45,598 - INFO - LR=0.1/Iter=100 -> learning_rate=0.1, max_iter=100\n",
            "2025-03-18 21:25:45,598 - INFO - LR=0.1/Iter=1000 -> learning_rate=0.1, max_iter=1000\n",
            "2025-03-18 21:25:45,599 - INFO - LR=0.1/Iter=10000 -> learning_rate=0.1, max_iter=10000\n"
          ]
        }
      ],
      "source": [
        "#######################################################################\n",
        "# SEPARATE RUN PARAMETERS FOR PERCEPTRONS vs. REGRESSIONS\n",
        "#######################################################################\n",
        "\n",
        "# Perceptrons (Clean & Pocket) iteration-based run\n",
        "perceptron_max_iter_values = [20,50,100,1000]  # for Clean PLA & Pocket PLA\n",
        "# Logging the run parameters\n",
        "logger.info(f\"=== Perceptron Run Parameters ===\")\n",
        "logger.info(f\"max_iter_values = {perceptron_max_iter_values}\")\n",
        "\n",
        "\n",
        "# Regression (Softmax & Linear) run parameters.\n",
        "learning_rates = [0.1]\n",
        "iteration_counts = [100,1000,10000]\n",
        "regression_run_configs = [\n",
        "    {\n",
        "        \"label\": f\"LR={lr}/Iter={it}\",\n",
        "        \"learning_rate\": lr,\n",
        "        \"max_iter\": it\n",
        "    }\n",
        "    for lr in learning_rates\n",
        "    for it in iteration_counts\n",
        "]\n",
        "\n",
        "logger.info(f\"=== Regression Run Parameters ===\")\n",
        "for cfg in regression_run_configs:\n",
        "    logger.info(f\"{cfg['label']} -> learning_rate={cfg['learning_rate']}, max_iter={cfg['max_iter']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e73BoKY7cmJU"
      },
      "source": [
        "# Load and Preprocess the MNIST Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osGLi3Hic5qW",
        "outputId": "1f51e239-f74c-418a-e3ed-801afd689fa7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-18 21:25:47,833 - INFO - Raw MNIST data shapes: X_raw: (70000, 784), y_raw: (70000,)\n",
            "2025-03-18 21:25:48,000 - INFO - Preprocessed shape: (70000, 785)\n",
            "2025-03-18 21:25:48,002 - INFO - Train set: X_train: (60000, 785), y_train: (60000,)\n",
            "2025-03-18 21:25:48,002 - INFO - Test set: X_test: (10000, 785), y_test: (10000,)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "We'll load the MNIST dataset using our custom loader (`mnist_loader`) and then apply preprocessing (`data_preprocessing`).\n",
        "The preprocessing step normalizes each image to the range [0, 1] and adds a bias term, resulting in input samples with 785 features.\n",
        "This setup ensures that the training set contains 60,000 samples and the test set 10,000 samples, preparing the data for the subsequent classification tasks.\n",
        "'''\n",
        "\n",
        "# New section\n",
        "# Load raw MNIST data (X: images, y: labels)\n",
        "X_raw, y_raw = load_mnist()\n",
        "\n",
        "\n",
        "logger.info(\"Raw MNIST data shapes: X_raw: %s, y_raw: %s\", X_raw.shape, y_raw.shape)\n",
        "\n",
        "# Preprocess (normalize & add bias = True)\n",
        "X = preprocess_data(X_raw, add_bias=True, normalize=True)\n",
        "logger.info(\"Preprocessed shape: %s\", X.shape)\n",
        "\n",
        "# Split into train/test manually or with 60k/10k as the task suggests\n",
        "X_train, y_train = X[:60000], y_raw[:60000]\n",
        "X_test,  y_test  = X[60000:], y_raw[60000:]\n",
        "\n",
        "logger.info(\"Train set: X_train: %s, y_train: %s\", X_train.shape, y_train.shape)\n",
        "logger.info(\"Test set: X_test: %s, y_test: %s\", X_test.shape, y_test.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O4hrMBCejtr"
      },
      "source": [
        "# Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ff75626fb22043ff8d354d4d81a99778"
          ]
        },
        "id": "Sik1JDX6Hxub",
        "outputId": "81e8ffe5-b04e-4d42-9f25-b4362f7c15da"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-18 18:55:50,068 - INFO - === TRAINING REGRESSION MODELS (Softmax & Linear) ===\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff75626fb22043ff8d354d4d81a99778",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Train Regressions:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-18 18:55:50,082 - INFO - --- Softmax LR=0.1/Iter=20 ---\n",
            "2025-03-18 18:55:57,046 - INFO - SoftmaxRegression training completed in 6.96 seconds.\n",
            "2025-03-18 18:55:57,051 - INFO - --- Linear Regression LR=0.1/Iter=20 ---\n",
            "2025-03-18 18:56:05,181 - INFO - LinearRegressionClassifier training completed in 8.13 seconds.\n",
            "2025-03-18 18:56:05,185 - INFO - --- Softmax LR=0.1/Iter=50 ---\n",
            "2025-03-18 18:56:26,495 - INFO - SoftmaxRegression training completed in 21.31 seconds.\n",
            "2025-03-18 18:56:26,502 - INFO - --- Linear Regression LR=0.1/Iter=50 ---\n",
            "2025-03-18 18:56:44,359 - INFO - LinearRegressionClassifier training completed in 17.86 seconds.\n",
            "2025-03-18 18:56:44,362 - INFO - --- Softmax LR=0.1/Iter=100 ---\n",
            "2025-03-18 18:57:24,208 - INFO - Iter 100/100, Loss: 0.2911, Avg Adaptive LR: 3.413107\n",
            "2025-03-18 18:57:24,211 - INFO - SoftmaxRegression training completed in 39.85 seconds.\n",
            "2025-03-18 18:57:24,214 - INFO - --- Linear Regression LR=0.1/Iter=100 ---\n",
            "2025-03-18 18:58:00,956 - INFO - Iter 100/100, Loss: 0.9284, Gradient Norm: 18.5031, Avg Adaptive LR: 1.3944604807474807\n",
            "2025-03-18 18:58:00,961 - INFO - LinearRegressionClassifier training completed in 36.74 seconds.\n",
            "2025-03-18 18:58:00,969 - INFO - --- Softmax LR=0.1/Iter=1000 ---\n",
            "2025-03-18 18:58:40,929 - INFO - Iter 100/1000, Loss: 0.2921, Avg Adaptive LR: 3.217279\n",
            "2025-03-18 18:59:20,987 - INFO - Iter 200/1000, Loss: 0.2696, Avg Adaptive LR: 3.215805\n",
            "2025-03-18 19:00:01,100 - INFO - Iter 300/1000, Loss: 0.2590, Avg Adaptive LR: 3.215108\n",
            "2025-03-18 19:00:41,269 - INFO - Iter 400/1000, Loss: 0.2523, Avg Adaptive LR: 3.214670\n",
            "2025-03-18 19:01:21,444 - INFO - Iter 500/1000, Loss: 0.2476, Avg Adaptive LR: 3.214360\n",
            "2025-03-18 19:02:01,781 - INFO - Iter 600/1000, Loss: 0.2439, Avg Adaptive LR: 3.214122\n",
            "2025-03-18 19:02:41,996 - INFO - Iter 700/1000, Loss: 0.2410, Avg Adaptive LR: 3.213931\n",
            "2025-03-18 19:03:23,588 - INFO - Iter 800/1000, Loss: 0.2386, Avg Adaptive LR: 3.213773\n",
            "2025-03-18 19:04:05,859 - INFO - Iter 900/1000, Loss: 0.2366, Avg Adaptive LR: 3.213639\n",
            "2025-03-18 19:04:45,777 - INFO - Iter 1000/1000, Loss: 0.2348, Avg Adaptive LR: 3.213523\n",
            "2025-03-18 19:04:45,780 - INFO - SoftmaxRegression training completed in 404.81 seconds.\n",
            "2025-03-18 19:04:45,782 - INFO - --- Linear Regression LR=0.1/Iter=1000 ---\n",
            "2025-03-18 19:05:22,634 - INFO - Iter 100/1000, Loss: 0.8366, Gradient Norm: 17.5033, Avg Adaptive LR: 1.3965721409335867\n",
            "2025-03-18 19:05:59,512 - INFO - Iter 200/1000, Loss: 0.4435, Gradient Norm: 12.4596, Avg Adaptive LR: 0.9915191209966415\n",
            "2025-03-18 19:06:34,245 - INFO - Iter 300/1000, Loss: 0.3018, Gradient Norm: 10.0383, Avg Adaptive LR: 0.8109561549340433\n",
            "2025-03-18 19:07:11,145 - INFO - Iter 400/1000, Loss: 0.2289, Gradient Norm: 8.5280, Avg Adaptive LR: 0.7030908053144924\n",
            "2025-03-18 19:07:48,252 - INFO - Iter 500/1000, Loss: 0.1859, Gradient Norm: 7.4986, Avg Adaptive LR: 0.6293190631755413\n",
            "2025-03-18 19:08:25,061 - INFO - Iter 600/1000, Loss: 0.1594, Gradient Norm: 6.7888, Avg Adaptive LR: 0.5747836924599233\n",
            "2025-03-18 19:08:59,785 - INFO - Iter 700/1000, Loss: 0.1422, Gradient Norm: 6.2832, Avg Adaptive LR: 0.5323216766634727\n",
            "2025-03-18 19:09:36,385 - INFO - Iter 800/1000, Loss: 0.1284, Gradient Norm: 5.8470, Avg Adaptive LR: 0.4981118989434924\n",
            "2025-03-18 19:10:13,828 - INFO - Iter 900/1000, Loss: 0.1171, Gradient Norm: 5.4639, Avg Adaptive LR: 0.46971189096352967\n",
            "2025-03-18 19:10:50,764 - INFO - Iter 1000/1000, Loss: 0.1050, Gradient Norm: 5.0181, Avg Adaptive LR: 0.44569819796079524\n",
            "2025-03-18 19:10:50,765 - INFO - LinearRegressionClassifier training completed in 364.98 seconds.\n",
            "2025-03-18 19:10:50,770 - INFO - --- Softmax LR=0.1/Iter=10000 ---\n",
            "2025-03-18 19:11:30,831 - INFO - Iter 100/10000, Loss: 0.2932, Avg Adaptive LR: 3.276818\n",
            "2025-03-18 19:12:11,285 - INFO - Iter 200/10000, Loss: 0.2707, Avg Adaptive LR: 3.275294\n",
            "2025-03-18 19:12:52,714 - INFO - Iter 300/10000, Loss: 0.2600, Avg Adaptive LR: 3.274567\n",
            "2025-03-18 19:13:33,300 - INFO - Iter 400/10000, Loss: 0.2533, Avg Adaptive LR: 3.274112\n",
            "2025-03-18 19:14:13,574 - INFO - Iter 500/10000, Loss: 0.2486, Avg Adaptive LR: 3.273789\n",
            "2025-03-18 19:14:53,609 - INFO - Iter 600/10000, Loss: 0.2450, Avg Adaptive LR: 3.273541\n",
            "2025-03-18 19:15:33,714 - INFO - Iter 700/10000, Loss: 0.2420, Avg Adaptive LR: 3.273343\n",
            "2025-03-18 19:16:13,934 - INFO - Iter 800/10000, Loss: 0.2396, Avg Adaptive LR: 3.273178\n",
            "2025-03-18 19:16:53,992 - INFO - Iter 900/10000, Loss: 0.2376, Avg Adaptive LR: 3.273037\n",
            "2025-03-18 19:17:33,920 - INFO - Iter 1000/10000, Loss: 0.2358, Avg Adaptive LR: 3.272916\n",
            "2025-03-18 19:18:13,870 - INFO - Iter 1100/10000, Loss: 0.2342, Avg Adaptive LR: 3.272809\n",
            "2025-03-18 19:18:54,206 - INFO - Iter 1200/10000, Loss: 0.2328, Avg Adaptive LR: 3.272714\n",
            "2025-03-18 19:19:34,154 - INFO - Iter 1300/10000, Loss: 0.2316, Avg Adaptive LR: 3.272629\n",
            "2025-03-18 19:20:15,383 - INFO - Iter 1400/10000, Loss: 0.2304, Avg Adaptive LR: 3.272552\n",
            "2025-03-18 19:20:55,857 - INFO - Iter 1500/10000, Loss: 0.2294, Avg Adaptive LR: 3.272482\n",
            "2025-03-18 19:21:36,157 - INFO - Iter 1600/10000, Loss: 0.2285, Avg Adaptive LR: 3.272417\n",
            "2025-03-18 19:22:16,608 - INFO - Iter 1700/10000, Loss: 0.2276, Avg Adaptive LR: 3.272358\n",
            "2025-03-18 19:22:56,949 - INFO - Iter 1800/10000, Loss: 0.2268, Avg Adaptive LR: 3.272303\n",
            "2025-03-18 19:23:37,029 - INFO - Iter 1900/10000, Loss: 0.2261, Avg Adaptive LR: 3.272253\n",
            "2025-03-18 19:24:18,186 - INFO - Iter 2000/10000, Loss: 0.2254, Avg Adaptive LR: 3.272205\n",
            "2025-03-18 19:24:58,517 - INFO - Iter 2100/10000, Loss: 0.2247, Avg Adaptive LR: 3.272161\n",
            "2025-03-18 19:25:39,263 - INFO - Iter 2200/10000, Loss: 0.2241, Avg Adaptive LR: 3.272119\n",
            "2025-03-18 19:26:19,800 - INFO - Iter 2300/10000, Loss: 0.2235, Avg Adaptive LR: 3.272080\n",
            "2025-03-18 19:26:59,923 - INFO - Iter 2400/10000, Loss: 0.2230, Avg Adaptive LR: 3.272043\n",
            "2025-03-18 19:27:40,075 - INFO - Iter 2500/10000, Loss: 0.2225, Avg Adaptive LR: 3.272008\n",
            "2025-03-18 19:28:19,832 - INFO - Iter 2600/10000, Loss: 0.2220, Avg Adaptive LR: 3.271975\n",
            "2025-03-18 19:29:00,072 - INFO - Iter 2700/10000, Loss: 0.2215, Avg Adaptive LR: 3.271944\n",
            "2025-03-18 19:29:40,358 - INFO - Iter 2800/10000, Loss: 0.2211, Avg Adaptive LR: 3.271914\n",
            "2025-03-18 19:30:20,845 - INFO - Iter 2900/10000, Loss: 0.2207, Avg Adaptive LR: 3.271886\n",
            "2025-03-18 19:31:00,916 - INFO - Iter 3000/10000, Loss: 0.2203, Avg Adaptive LR: 3.271859\n",
            "2025-03-18 19:31:41,596 - INFO - Iter 3100/10000, Loss: 0.2199, Avg Adaptive LR: 3.271833\n",
            "2025-03-18 19:32:21,898 - INFO - Iter 3200/10000, Loss: 0.2195, Avg Adaptive LR: 3.271809\n",
            "2025-03-18 19:33:02,141 - INFO - Iter 3300/10000, Loss: 0.2192, Avg Adaptive LR: 3.271785\n",
            "2025-03-18 19:33:42,531 - INFO - Iter 3400/10000, Loss: 0.2189, Avg Adaptive LR: 3.271763\n",
            "2025-03-18 19:34:23,390 - INFO - Iter 3500/10000, Loss: 0.2186, Avg Adaptive LR: 3.271741\n",
            "2025-03-18 19:35:04,015 - INFO - Iter 3600/10000, Loss: 0.2182, Avg Adaptive LR: 3.271720\n",
            "2025-03-18 19:35:44,264 - INFO - Iter 3700/10000, Loss: 0.2180, Avg Adaptive LR: 3.271700\n",
            "2025-03-18 19:36:24,568 - INFO - Iter 3800/10000, Loss: 0.2177, Avg Adaptive LR: 3.271681\n",
            "2025-03-18 19:37:04,916 - INFO - Iter 3900/10000, Loss: 0.2174, Avg Adaptive LR: 3.271662\n",
            "2025-03-18 19:37:45,198 - INFO - Iter 4000/10000, Loss: 0.2171, Avg Adaptive LR: 3.271644\n",
            "2025-03-18 19:38:25,446 - INFO - Iter 4100/10000, Loss: 0.2169, Avg Adaptive LR: 3.271627\n",
            "2025-03-18 19:39:05,620 - INFO - Iter 4200/10000, Loss: 0.2166, Avg Adaptive LR: 3.271611\n",
            "2025-03-18 19:39:45,896 - INFO - Iter 4300/10000, Loss: 0.2164, Avg Adaptive LR: 3.271594\n",
            "2025-03-18 19:40:26,245 - INFO - Iter 4400/10000, Loss: 0.2162, Avg Adaptive LR: 3.271579\n",
            "2025-03-18 19:41:06,888 - INFO - Iter 4500/10000, Loss: 0.2160, Avg Adaptive LR: 3.271564\n",
            "2025-03-18 19:41:47,363 - INFO - Iter 4600/10000, Loss: 0.2157, Avg Adaptive LR: 3.271549\n",
            "2025-03-18 19:42:26,421 - INFO - Iter 4700/10000, Loss: 0.2155, Avg Adaptive LR: 3.271535\n",
            "2025-03-18 19:43:06,204 - INFO - Iter 4800/10000, Loss: 0.2153, Avg Adaptive LR: 3.271521\n",
            "2025-03-18 19:43:45,508 - INFO - Iter 4900/10000, Loss: 0.2151, Avg Adaptive LR: 3.271508\n",
            "2025-03-18 19:44:25,047 - INFO - Iter 5000/10000, Loss: 0.2149, Avg Adaptive LR: 3.271495\n",
            "2025-03-18 19:45:04,520 - INFO - Iter 5100/10000, Loss: 0.2148, Avg Adaptive LR: 3.271482\n",
            "2025-03-18 19:45:44,043 - INFO - Iter 5200/10000, Loss: 0.2146, Avg Adaptive LR: 3.271470\n",
            "2025-03-18 19:46:23,623 - INFO - Iter 5300/10000, Loss: 0.2144, Avg Adaptive LR: 3.271458\n",
            "2025-03-18 19:47:03,112 - INFO - Iter 5400/10000, Loss: 0.2142, Avg Adaptive LR: 3.271446\n",
            "2025-03-18 19:47:42,676 - INFO - Iter 5500/10000, Loss: 0.2141, Avg Adaptive LR: 3.271435\n",
            "2025-03-18 19:48:22,255 - INFO - Iter 5600/10000, Loss: 0.2139, Avg Adaptive LR: 3.271424\n",
            "2025-03-18 19:49:01,860 - INFO - Iter 5700/10000, Loss: 0.2137, Avg Adaptive LR: 3.271413\n",
            "2025-03-18 19:49:41,230 - INFO - Iter 5800/10000, Loss: 0.2136, Avg Adaptive LR: 3.271403\n",
            "2025-03-18 19:50:20,748 - INFO - Iter 5900/10000, Loss: 0.2134, Avg Adaptive LR: 3.271393\n",
            "2025-03-18 19:51:00,200 - INFO - Iter 6000/10000, Loss: 0.2133, Avg Adaptive LR: 3.271383\n",
            "2025-03-18 19:51:39,587 - INFO - Iter 6100/10000, Loss: 0.2132, Avg Adaptive LR: 3.271373\n",
            "2025-03-18 19:52:19,130 - INFO - Iter 6200/10000, Loss: 0.2130, Avg Adaptive LR: 3.271364\n",
            "2025-03-18 19:52:58,976 - INFO - Iter 6300/10000, Loss: 0.2129, Avg Adaptive LR: 3.271354\n",
            "2025-03-18 19:53:39,312 - INFO - Iter 6400/10000, Loss: 0.2127, Avg Adaptive LR: 3.271345\n",
            "2025-03-18 19:54:19,071 - INFO - Iter 6500/10000, Loss: 0.2126, Avg Adaptive LR: 3.271337\n",
            "2025-03-18 19:54:58,812 - INFO - Iter 6600/10000, Loss: 0.2125, Avg Adaptive LR: 3.271328\n",
            "2025-03-18 19:55:38,341 - INFO - Iter 6700/10000, Loss: 0.2124, Avg Adaptive LR: 3.271319\n",
            "2025-03-18 19:56:17,578 - INFO - Iter 6800/10000, Loss: 0.2122, Avg Adaptive LR: 3.271311\n",
            "2025-03-18 19:56:56,934 - INFO - Iter 6900/10000, Loss: 0.2121, Avg Adaptive LR: 3.271303\n",
            "2025-03-18 19:57:36,315 - INFO - Iter 7000/10000, Loss: 0.2120, Avg Adaptive LR: 3.271295\n",
            "2025-03-18 19:58:15,752 - INFO - Iter 7100/10000, Loss: 0.2119, Avg Adaptive LR: 3.271287\n",
            "2025-03-18 19:58:55,094 - INFO - Iter 7200/10000, Loss: 0.2118, Avg Adaptive LR: 3.271280\n",
            "2025-03-18 19:59:34,246 - INFO - Iter 7300/10000, Loss: 0.2117, Avg Adaptive LR: 3.271272\n",
            "2025-03-18 20:00:14,137 - INFO - Iter 7400/10000, Loss: 0.2116, Avg Adaptive LR: 3.271265\n",
            "2025-03-18 20:00:53,668 - INFO - Iter 7500/10000, Loss: 0.2115, Avg Adaptive LR: 3.271258\n",
            "2025-03-18 20:01:33,142 - INFO - Iter 7600/10000, Loss: 0.2114, Avg Adaptive LR: 3.271251\n",
            "2025-03-18 20:02:12,503 - INFO - Iter 7700/10000, Loss: 0.2113, Avg Adaptive LR: 3.271244\n",
            "2025-03-18 20:02:25,783 - INFO - Early stopping triggered at iteration 7734 with training loss 0.211221\n",
            "2025-03-18 20:02:25,788 - INFO - SoftmaxRegression training completed in 3095.02 seconds.\n",
            "2025-03-18 20:02:25,791 - INFO - --- Linear Regression LR=0.1/Iter=10000 ---\n",
            "2025-03-18 20:03:01,715 - INFO - Iter 100/10000, Loss: 0.7485, Gradient Norm: 16.4996, Avg Adaptive LR: 1.39671688675772\n",
            "2025-03-18 20:03:37,749 - INFO - Iter 200/10000, Loss: 0.3701, Gradient Norm: 11.2600, Avg Adaptive LR: 0.991591655186752\n",
            "2025-03-18 20:04:13,937 - INFO - Iter 300/10000, Loss: 0.2395, Gradient Norm: 8.7501, Avg Adaptive LR: 0.8110932891797903\n",
            "2025-03-18 20:04:48,117 - INFO - Iter 400/10000, Loss: 0.1766, Gradient Norm: 7.2394, Avg Adaptive LR: 0.7031988945476274\n",
            "2025-03-18 20:05:23,812 - INFO - Iter 500/10000, Loss: 0.1407, Gradient Norm: 6.2137, Avg Adaptive LR: 0.6294135050868462\n",
            "2025-03-18 20:06:00,369 - INFO - Iter 600/10000, Loss: 0.1180, Gradient Norm: 5.4671, Avg Adaptive LR: 0.5748826480844975\n",
            "2025-03-18 20:06:36,225 - INFO - Iter 700/10000, Loss: 0.1026, Gradient Norm: 4.8977, Avg Adaptive LR: 0.5324555452364487\n",
            "2025-03-18 20:07:10,610 - INFO - Iter 800/10000, Loss: 0.0921, Gradient Norm: 4.4694, Avg Adaptive LR: 0.49822550524601167\n",
            "2025-03-18 20:07:46,764 - INFO - Iter 900/10000, Loss: 0.0839, Gradient Norm: 4.1077, Avg Adaptive LR: 0.46986502642743255\n",
            "2025-03-18 20:08:23,065 - INFO - Iter 1000/10000, Loss: 0.0779, Gradient Norm: 3.8166, Avg Adaptive LR: 0.4458512891885307\n",
            "2025-03-18 20:08:58,917 - INFO - Iter 1100/10000, Loss: 0.0732, Gradient Norm: 3.5728, Avg Adaptive LR: 0.4251849860570999\n",
            "2025-03-18 20:09:33,135 - INFO - Iter 1200/10000, Loss: 0.0695, Gradient Norm: 3.3734, Avg Adaptive LR: 0.4071559480807529\n",
            "2025-03-18 20:10:09,298 - INFO - Iter 1300/10000, Loss: 0.0663, Gradient Norm: 3.1904, Avg Adaptive LR: 0.3912403433557772\n",
            "2025-03-18 20:10:45,457 - INFO - Iter 1400/10000, Loss: 0.0641, Gradient Norm: 3.0529, Avg Adaptive LR: 0.37705667787241914\n",
            "2025-03-18 20:11:20,708 - INFO - Iter 1500/10000, Loss: 0.0617, Gradient Norm: 2.9070, Avg Adaptive LR: 0.3643131844052585\n",
            "2025-03-18 20:11:55,789 - INFO - Iter 1600/10000, Loss: 0.0601, Gradient Norm: 2.7967, Avg Adaptive LR: 0.352782252017618\n",
            "2025-03-18 20:12:31,726 - INFO - Iter 1700/10000, Loss: 0.0585, Gradient Norm: 2.6887, Avg Adaptive LR: 0.3422823261539601\n",
            "2025-03-18 20:13:07,762 - INFO - Iter 1800/10000, Loss: 0.0572, Gradient Norm: 2.5970, Avg Adaptive LR: 0.3326680659208349\n",
            "2025-03-18 20:13:42,341 - INFO - Iter 1900/10000, Loss: 0.0561, Gradient Norm: 2.5144, Avg Adaptive LR: 0.3238215874917681\n",
            "2025-03-18 20:14:18,296 - INFO - Iter 2000/10000, Loss: 0.0551, Gradient Norm: 2.4406, Avg Adaptive LR: 0.3156457365035975\n",
            "2025-03-18 20:14:54,484 - INFO - Iter 2100/10000, Loss: 0.0542, Gradient Norm: 2.3702, Avg Adaptive LR: 0.30806000393458866\n",
            "2025-03-18 20:15:30,610 - INFO - Iter 2200/10000, Loss: 0.0535, Gradient Norm: 2.3160, Avg Adaptive LR: 0.30099693456982396\n",
            "2025-03-18 20:16:05,969 - INFO - Iter 2300/10000, Loss: 0.0527, Gradient Norm: 2.2481, Avg Adaptive LR: 0.2943984544819122\n",
            "2025-03-18 20:16:40,749 - INFO - Iter 2400/10000, Loss: 0.0522, Gradient Norm: 2.2107, Avg Adaptive LR: 0.28821683504711393\n",
            "2025-03-18 20:17:16,864 - INFO - Iter 2500/10000, Loss: 0.0514, Gradient Norm: 2.1434, Avg Adaptive LR: 0.2824086238093318\n",
            "2025-03-18 20:17:53,195 - INFO - Iter 2600/10000, Loss: 0.0511, Gradient Norm: 2.1171, Avg Adaptive LR: 0.2769378091372457\n",
            "2025-03-18 20:18:29,010 - INFO - Iter 2700/10000, Loss: 0.0505, Gradient Norm: 2.0585, Avg Adaptive LR: 0.27177357271201014\n",
            "2025-03-18 20:19:03,482 - INFO - Iter 2800/10000, Loss: 0.0501, Gradient Norm: 2.0308, Avg Adaptive LR: 0.2668879171094532\n",
            "2025-03-18 20:19:39,410 - INFO - Iter 2900/10000, Loss: 0.0498, Gradient Norm: 1.9963, Avg Adaptive LR: 0.262257547843987\n",
            "2025-03-18 20:20:15,479 - INFO - Iter 3000/10000, Loss: 0.0493, Gradient Norm: 1.9517, Avg Adaptive LR: 0.2578598914625985\n",
            "2025-03-18 20:20:49,713 - INFO - Iter 3100/10000, Loss: 0.0491, Gradient Norm: 1.9335, Avg Adaptive LR: 0.2536762371890872\n",
            "2025-03-18 20:21:25,379 - INFO - Iter 3200/10000, Loss: 0.0486, Gradient Norm: 1.8896, Avg Adaptive LR: 0.24968996477451286\n",
            "2025-03-18 20:22:01,478 - INFO - Iter 3300/10000, Loss: 0.0484, Gradient Norm: 1.8692, Avg Adaptive LR: 0.24588629349244603\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3aa722347cf3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     )\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mlin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mtrained_models_linear\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlin_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/core/models/linear_regression/linear_regression.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_test, y_test)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;31m# gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgrad_norm\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mdW\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mmax_grad_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/linalg/_linalg.py\u001b[0m in \u001b[0;36m_norm_dispatcher\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_norm_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2563\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# TRAINING CELL\n",
        "# =========================================================\n",
        "\n",
        "# 1) Dictionaries to store trained models\n",
        "trained_models_clean   = {}\n",
        "trained_models_pocket  = {}\n",
        "trained_models_softmax = {}\n",
        "trained_models_linear  = {}\n",
        "\n",
        "# 2) Train Regression Models (Softmax & Linear)\n",
        "logger.info(\"=== TRAINING REGRESSION MODELS (Softmax & Linear) ===\")\n",
        "for cfg in tqdm(regression_run_configs, desc=\"Train Regressions\"):\n",
        "    lr_val = cfg[\"learning_rate\"]\n",
        "    max_iter_val = cfg[\"max_iter\"]\n",
        "    label = cfg[\"label\"]  # e.g. \"LR=0.001/Iter=1000\"\n",
        "\n",
        "    # --- Softmax ---\n",
        "    logger.info(f\"--- Softmax {label} ---\")\n",
        "    s_model = SoftmaxRegression(\n",
        "        num_classes=10,\n",
        "        max_iter=max_iter_val,\n",
        "        learning_rate=lr_val,\n",
        "        adaptive_lr=True\n",
        "    )\n",
        "    s_model.fit(X_train, y_train)\n",
        "    trained_models_softmax[(lr_val, max_iter_val)] = s_model\n",
        "\n",
        "    # --- Linear ---\n",
        "    logger.info(f\"--- Linear Regression {label} ---\")\n",
        "    lin_model = LinearRegression(\n",
        "        num_classes=10,\n",
        "        max_iter=max_iter_val,\n",
        "        learning_rate=lr_val,\n",
        "        adaptive_lr=True,\n",
        "        early_stopping=False\n",
        "    )\n",
        "    lin_model.fit(X_train, y_train)\n",
        "    trained_models_linear[(lr_val, max_iter_val)] = lin_model\n",
        "\n",
        "logger.info(\"Training complete for Softmax and Linear.\")\n",
        "\n",
        "# 3) Train Perceptron Models (Clean & Pocket)\n",
        "logger.info(\"=== TRAINING PERCEPTRON MODELS (Clean & Pocket) ===\")\n",
        "for max_iter in tqdm(perceptron_max_iter_values, desc=\"Train Clean & Pocket\"):\n",
        "    logger.info(f\"--- Clean PLA, max_iter={max_iter} ---\")\n",
        "    clean_perc = MultiClassPerceptron(num_classes=10, max_iter=max_iter, use_pocket=False)\n",
        "    clean_perc.fit(X_train, y_train)\n",
        "    trained_models_clean[max_iter] = clean_perc\n",
        "\n",
        "    logger.info(f\"--- Pocket PLA, max_iter={max_iter} ---\")\n",
        "    pocket_perc = MultiClassPerceptron(num_classes=10, max_iter=max_iter, use_pocket=True)\n",
        "    pocket_perc.fit(X_train, y_train)\n",
        "    trained_models_pocket[max_iter] = pocket_perc\n",
        "\n",
        "logger.info(\"Training complete for Clean PLA and Pocket PLA.\")\n",
        "logger.info(\"=== ALL TRAINING COMPLETE ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VepUzPkWrANq"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Hech6_lrANq"
      },
      "outputs": [],
      "source": [
        "##################################################\n",
        "# EVALUATION CELL (with pandas DataFrame)\n",
        "##################################################\n",
        "\n",
        "\n",
        "# 1) Evaluate Perceptrons: Clean & Pocket\n",
        "accuracies_clean, accuracies_pocket = [], []\n",
        "runtimes_clean,   runtimes_pocket   = [], []\n",
        "sensitivities_clean, sensitivities_pocket = [], []\n",
        "selectivities_clean, selectivities_pocket = [], []\n",
        "\n",
        "conf_clean, conf_pocket = [], []\n",
        "meta_clean, meta_pocket = [], []\n",
        "\n",
        "for max_iter in tqdm(perceptron_max_iter_values, desc=\"Evaluate Clean & Pocket\"):\n",
        "    # === Evaluate Clean PLA ===\n",
        "    c_model = trained_models_clean[max_iter]\n",
        "    cm_c, acc_c, s_c, sp_c, rt_c, ex_c = evaluate_model(\n",
        "        c_model, X_test, y_test, classes=range(10), model_name=\"Clean PLA\"\n",
        "    )\n",
        "    accuracies_clean.append(acc_c)\n",
        "    runtimes_clean.append(rt_c)\n",
        "    sensitivities_clean.append(np.mean(s_c))\n",
        "    selectivities_clean.append(np.mean(sp_c))\n",
        "    conf_clean.append(cm_c)\n",
        "\n",
        "    cdict = {\n",
        "        \"max_iter\": max_iter,\n",
        "        \"accuracy\": acc_c,\n",
        "        \"runtime\": rt_c,\n",
        "        \"avg_sensitivity\": np.mean(s_c),\n",
        "        \"avg_selectivity\": np.mean(sp_c),\n",
        "        \"method\": \"Clean PLA\"\n",
        "    }\n",
        "    cdict.update(ex_c)\n",
        "    meta_clean.append(cdict)\n",
        "\n",
        "    # === Evaluate Pocket PLA ===\n",
        "    p_model = trained_models_pocket[max_iter]\n",
        "    cm_p, acc_p, s_p, sp_p, rt_p, ex_p = evaluate_model(\n",
        "        p_model, X_test, y_test, classes=range(10), model_name=\"Pocket PLA\"\n",
        "    )\n",
        "    accuracies_pocket.append(acc_p)\n",
        "    runtimes_pocket.append(rt_p)\n",
        "    sensitivities_pocket.append(np.mean(s_p))\n",
        "    selectivities_pocket.append(np.mean(sp_p))\n",
        "    conf_pocket.append(cm_p)\n",
        "\n",
        "    pdict = {\n",
        "        \"max_iter\": max_iter,\n",
        "        \"accuracy\": acc_p,\n",
        "        \"runtime\": rt_p,\n",
        "        \"avg_sensitivity\": np.mean(s_p),\n",
        "        \"avg_selectivity\": np.mean(sp_p),\n",
        "        \"method\": \"Pocket PLA\"\n",
        "    }\n",
        "    pdict.update(ex_p)\n",
        "    meta_pocket.append(pdict)\n",
        "\n",
        "# Aggregated iteration-level training curves for Perceptrons\n",
        "clean_train_curve = aggregate_iteration_losses(\n",
        "    [trained_models_clean[m] for m in perceptron_max_iter_values]\n",
        ")\n",
        "pocket_train_curve = aggregate_iteration_losses(\n",
        "    [trained_models_pocket[m] for m in perceptron_max_iter_values]\n",
        ")\n",
        "\n",
        "# 2) Evaluate Regression Models: Softmax & Linear\n",
        "accuracies_softmax = []\n",
        "runtimes_softmax   = []\n",
        "sensitivities_soft = []\n",
        "selectivities_soft = []\n",
        "conf_soft          = []\n",
        "meta_soft          = []\n",
        "\n",
        "accuracies_linear = []\n",
        "runtimes_linear   = []\n",
        "sensitivities_lin = []\n",
        "selectivities_lin = []\n",
        "conf_linear       = []\n",
        "meta_linear       = []\n",
        "\n",
        "for cfg in tqdm(regression_run_configs, desc=\"Evaluate Regressions\"):\n",
        "    lr_val = cfg[\"learning_rate\"]\n",
        "    max_iter_val = cfg[\"max_iter\"]\n",
        "    label = cfg[\"label\"]\n",
        "\n",
        "    # === Evaluate Softmax ===\n",
        "    s_model = trained_models_softmax[(lr_val, max_iter_val)]\n",
        "    cm_s, a_s, se_s, sp_s, r_s, ex_s = evaluate_model(\n",
        "        s_model, X_test, y_test, classes=range(10),\n",
        "        model_name=f\"Softmax ({label})\"\n",
        "    )\n",
        "    accuracies_softmax.append(a_s)\n",
        "    runtimes_softmax.append(r_s)\n",
        "    sensitivities_soft.append(np.mean(se_s))\n",
        "    selectivities_soft.append(np.mean(sp_s))\n",
        "    conf_soft.append(cm_s)\n",
        "\n",
        "    ms = {\n",
        "        \"label\": label,\n",
        "        \"learning_rate\": lr_val,\n",
        "        \"max_iter\": max_iter_val,\n",
        "        \"accuracy\": a_s,\n",
        "        \"runtime\": r_s,\n",
        "        \"avg_sensitivity\": np.mean(se_s),\n",
        "        \"avg_selectivity\": np.mean(sp_s),\n",
        "        \"method\": \"Softmax\"\n",
        "    }\n",
        "    ms.update(ex_s)\n",
        "    meta_soft.append(ms)\n",
        "\n",
        "    # === Evaluate Linear ===\n",
        "    lin_model = trained_models_linear[(lr_val, max_iter_val)]\n",
        "    cm_l, a_l, se_l, sp_l, r_l, ex_l = evaluate_model(\n",
        "        lin_model, X_test, y_test, classes=range(10),\n",
        "        model_name=f\"Linear ({label})\"\n",
        "    )\n",
        "    accuracies_linear.append(a_l)\n",
        "    runtimes_linear.append(r_l)\n",
        "    sensitivities_lin.append(np.mean(se_l))\n",
        "    selectivities_lin.append(np.mean(sp_l))\n",
        "    conf_linear.append(cm_l)\n",
        "\n",
        "    ml = {\n",
        "        \"label\": label,\n",
        "        \"learning_rate\": lr_val,\n",
        "        \"max_iter\": max_iter_val,\n",
        "        \"accuracy\": a_l,\n",
        "        \"runtime\": r_l,\n",
        "        \"avg_sensitivity\": np.mean(se_l),\n",
        "        \"avg_selectivity\": np.mean(sp_l),\n",
        "        \"method\": \"Linear Regression\"\n",
        "    }\n",
        "    ml.update(ex_l)\n",
        "    meta_linear.append(ml)\n",
        "\n",
        "\n",
        "logger.info(\"Evaluation complete for Perceptrons & Regressions.\")\n",
        "\n",
        "\n",
        "# # 1) Build the DataFrame of all model results\n",
        "# all_rows = []\n",
        "\n",
        "# # A) Clean PLA\n",
        "# for i, max_iter in tqdm(\n",
        "#     enumerate(perceptron_max_iter_values),\n",
        "#     desc=\"Collecting Clean PLA\",\n",
        "#     total=len(perceptron_max_iter_values)\n",
        "# ):\n",
        "#     all_rows.append({\n",
        "#         'model': 'Clean PLA',\n",
        "#         'max_iter': max_iter,\n",
        "#         'runtime': runtimes_clean[i],\n",
        "#         'accuracy': accuracies_clean[i],\n",
        "#         'sensitivity': sensitivities_clean[i],\n",
        "#         'selectivity': selectivities_clean[i]\n",
        "#     })\n",
        "\n",
        "# # B) Pocket PLA\n",
        "# for i, max_iter in tqdm(\n",
        "#     enumerate(perceptron_max_iter_values),\n",
        "#     desc=\"Collecting Pocket PLA\",\n",
        "#     total=len(perceptron_max_iter_values)\n",
        "# ):\n",
        "#     all_rows.append({\n",
        "#         'model': 'Pocket PLA',\n",
        "#         'max_iter': max_iter,\n",
        "#         'runtime': runtimes_pocket[i],\n",
        "#         'accuracy': accuracies_pocket[i],\n",
        "#         'sensitivity': sensitivities_pocket[i],\n",
        "#         'selectivity': selectivities_pocket[i]\n",
        "#     })\n",
        "\n",
        "# # C) Softmax\n",
        "# for i, row_meta in tqdm(\n",
        "#     enumerate(meta_soft),\n",
        "#     desc=\"Collecting Softmax\",\n",
        "#     total=len(meta_soft)\n",
        "# ):\n",
        "#     all_rows.append({\n",
        "#         'model': 'Softmax',\n",
        "#         'max_iter': row_meta['max_iter'],\n",
        "#         'runtime': runtimes_softmax[i],\n",
        "#         'accuracy': accuracies_softmax[i],\n",
        "#         'sensitivity': sensitivities_soft[i],\n",
        "#         'selectivity': selectivities_soft[i]\n",
        "#     })\n",
        "\n",
        "# # D) Linear\n",
        "# for i, row_meta in tqdm(\n",
        "#     enumerate(meta_linear),\n",
        "#     desc=\"Collecting Linear\",\n",
        "#     total=len(meta_linear)\n",
        "# ):\n",
        "#     all_rows.append({\n",
        "#         'model': 'Linear',\n",
        "#         'max_iter': row_meta['max_iter'],\n",
        "#         'runtime': runtimes_linear[i],\n",
        "#         'accuracy': accuracies_linear[i],\n",
        "#         'sensitivity': sensitivities_lin[i],\n",
        "#         'selectivity': selectivities_lin[i]\n",
        "#     })\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwCAybO5owmg"
      },
      "source": [
        "# Visualize (Generate Plots, Confusion Matricies, etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC4vaIjVowmg"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "# from tqdm import tqdm\n",
        "\n",
        "##################################################\n",
        "# 1) CREATE A SINGLE PANDAS DATAFRAME FOR ALL RESULTS\n",
        "##################################################\n",
        "all_rows = []\n",
        "\n",
        "# (A) Clean PLA\n",
        "for i, max_iter in tqdm(\n",
        "    enumerate(perceptron_max_iter_values),\n",
        "    desc=\"Collecting Clean PLA\",\n",
        "    total=len(perceptron_max_iter_values)\n",
        "):\n",
        "    all_rows.append({\n",
        "        'model': 'Clean PLA',\n",
        "        'max_iter': max_iter,\n",
        "        'runtime': runtimes_clean[i],\n",
        "        'accuracy': accuracies_clean[i],\n",
        "        'sensitivity': sensitivities_clean[i],\n",
        "        'selectivity': selectivities_clean[i]\n",
        "    })\n",
        "\n",
        "# (B) Pocket PLA\n",
        "for i, max_iter in tqdm(\n",
        "    enumerate(perceptron_max_iter_values),\n",
        "    desc=\"Collecting Pocket PLA\",\n",
        "    total=len(perceptron_max_iter_values)\n",
        "):\n",
        "    all_rows.append({\n",
        "        'model': 'Pocket PLA',\n",
        "        'max_iter': max_iter,\n",
        "        'runtime': runtimes_pocket[i],\n",
        "        'accuracy': accuracies_pocket[i],\n",
        "        'sensitivity': sensitivities_pocket[i],\n",
        "        'selectivity': selectivities_pocket[i]\n",
        "    })\n",
        "\n",
        "# (C) Softmax\n",
        "for i, row_meta in tqdm(\n",
        "    enumerate(meta_soft),\n",
        "    desc=\"Collecting Softmax\",\n",
        "    total=len(meta_soft)\n",
        "):\n",
        "    all_rows.append({\n",
        "        'model': 'Softmax',\n",
        "        'max_iter': row_meta['max_iter'],\n",
        "        'runtime': runtimes_softmax[i],\n",
        "        'accuracy': accuracies_softmax[i],\n",
        "        'sensitivity': sensitivities_soft[i],\n",
        "        'selectivity': selectivities_soft[i]\n",
        "    })\n",
        "\n",
        "# (D) Linear\n",
        "for i, row_meta in tqdm(\n",
        "    enumerate(meta_linear),\n",
        "    desc=\"Collecting Linear\",\n",
        "    total=len(meta_linear)\n",
        "):\n",
        "    all_rows.append({\n",
        "        'model': 'Linear',\n",
        "        'max_iter': row_meta['max_iter'],\n",
        "        'runtime': runtimes_linear[i],\n",
        "        'accuracy': accuracies_linear[i],\n",
        "        'sensitivity': sensitivities_lin[i],\n",
        "        'selectivity': selectivities_lin[i]\n",
        "    })\n",
        "\n",
        "df_results = pd.DataFrame(all_rows)\n",
        "logger.info(\"Combined Results DataFrame:\\n%s\", df_results)\n",
        "display(df_results.head(20))\n",
        "\n",
        "############################################################################\n",
        "# 2) CONFUSION MATRICES FOR ALL MODELS (GROUPED BY PLOT TYPE)\n",
        "############################################################################\n",
        "\n",
        "logger.info(\"=== Plotting ALL Confusion Matrices ===\")\n",
        "\n",
        "# 2A) Perceptron: Clean\n",
        "for idx, meta in tqdm(enumerate(meta_clean), total=len(meta_clean), desc=\"Confusions: Clean PLA\"):\n",
        "    title = f\"Clean PLA (max_iter={meta['max_iter']}, Acc={meta['accuracy']*100:.2f}%)\"\n",
        "    plot_confusion_matrix_annotated(\n",
        "        conf_clean[idx],\n",
        "        classes=range(10),\n",
        "        title=title,\n",
        "        method=meta[\"method\"],\n",
        "        max_iter=meta[\"max_iter\"]\n",
        "    )\n",
        "\n",
        "# 2B) Perceptron: Pocket\n",
        "for idx, meta in tqdm(enumerate(meta_pocket), total=len(meta_pocket), desc=\"Confusions: Pocket PLA\"):\n",
        "    title = f\"Pocket PLA (max_iter={meta['max_iter']}, Acc={meta['accuracy']*100:.2f}%)\"\n",
        "    plot_confusion_matrix_annotated(\n",
        "        conf_pocket[idx],\n",
        "        classes=range(10),\n",
        "        title=title,\n",
        "        method=meta[\"method\"],\n",
        "        max_iter=meta[\"max_iter\"]\n",
        "    )\n",
        "\n",
        "# 2C) Softmax\n",
        "for idx, meta in tqdm(enumerate(meta_soft), total=len(meta_soft), desc=\"Confusions: Softmax\"):\n",
        "    title = f\"Softmax ({meta['label']}, Acc={meta['accuracy']*100:.2f}%)\"\n",
        "    plot_confusion_matrix_annotated(\n",
        "        conf_soft[idx],\n",
        "        classes=range(10),\n",
        "        title=title,\n",
        "        method=meta[\"method\"],\n",
        "        max_iter=meta[\"max_iter\"]\n",
        "    )\n",
        "\n",
        "# 2D) Linear\n",
        "for idx, meta in tqdm(enumerate(meta_linear), total=len(meta_linear), desc=\"Confusions: Linear\"):\n",
        "    title = f\"Linear ({meta['label']}, Acc={meta['accuracy']*100:.2f}%)\"\n",
        "    plot_confusion_matrix_annotated(\n",
        "        conf_linear[idx],\n",
        "        classes=range(10),\n",
        "        title=title,\n",
        "        method=meta[\"method\"],\n",
        "        max_iter=meta[\"max_iter\"]\n",
        "    )\n",
        "\n",
        "\n",
        "############################################################################\n",
        "# 3) ITERATION-LEVEL PLOTS (ALL MODELS)\n",
        "############################################################################\n",
        "\n",
        "logger.info(\"=== Iteration-Level Visualization (All Models) ===\")\n",
        "\n",
        "# 3A) Perceptron: Clean & Pocket\n",
        "for max_iter, c_model in trained_models_clean.items():\n",
        "    df_iter = c_model.get_iteration_df()\n",
        "    if not df_iter.empty and \"train_error\" in df_iter.columns:\n",
        "        title = f\"Clean PLA max_iter={max_iter}: Train Error vs. Iteration\"\n",
        "        df_iter.plot(x=\"iteration\", y=\"train_error\", marker='o', figsize=(8,5), title=title)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.show()\n",
        "\n",
        "for max_iter, p_model in trained_models_pocket.items():\n",
        "    df_iter = p_model.get_iteration_df()\n",
        "    if not df_iter.empty and \"train_error\" in df_iter.columns:\n",
        "        title = f\"Pocket PLA max_iter={max_iter}: Train Error vs. Iteration\"\n",
        "        df_iter.plot(x=\"iteration\", y=\"train_error\", marker='o', figsize=(8,5), title=title)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.show()\n",
        "\n",
        "# 3B) Softmax\n",
        "for (lr_val, max_iter_val), s_model in trained_models_softmax.items():\n",
        "    df_iter = s_model.get_iteration_df()  # Must be implemented in your SoftmaxRegression\n",
        "    if not df_iter.empty:\n",
        "        title = f\"Softmax LR={lr_val}, max_iter={max_iter_val}: Train Loss vs. Iteration\"\n",
        "        df_iter.plot(x=\"iteration\", y=\"train_loss\", marker='o', figsize=(8,5), title=title)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.show()\n",
        "\n",
        "        if \"test_loss\" in df_iter.columns:\n",
        "            title = f\"Softmax LR={lr_val}, max_iter={max_iter_val}: Train & Test Loss\"\n",
        "            df_iter.plot(x=\"iteration\", y=[\"train_loss\",\"test_loss\"], marker='o', figsize=(8,5), title=title)\n",
        "            plt.grid(True, linestyle='--', alpha=0.7)\n",
        "            plt.show()\n",
        "\n",
        "        if \"avg_adaptive_lr\" in df_iter.columns:\n",
        "            title = f\"Softmax LR={lr_val}, max_iter={max_iter_val}: Avg Adaptive LR vs. Iteration\"\n",
        "            df_iter.plot(x=\"iteration\", y=\"avg_adaptive_lr\", marker='x', figsize=(8,5), title=title)\n",
        "            plt.grid(True, linestyle='--', alpha=0.7)\n",
        "            plt.show()\n",
        "\n",
        "# 3C) Linear\n",
        "for (lr_val, max_iter_val), lin_model in trained_models_linear.items():\n",
        "    df_iter = lin_model.get_iteration_df()  # Must be implemented in your LinearRegression\n",
        "    if not df_iter.empty:\n",
        "        title = f\"Linear LR={lr_val}, max_iter={max_iter_val}: Train Loss vs. Iteration\"\n",
        "        df_iter.plot(x=\"iteration\", y=\"train_loss\", marker='o', figsize=(8,5), title=title)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.show()\n",
        "\n",
        "        if \"test_loss\" in df_iter.columns:\n",
        "            title = f\"Linear LR={lr_val}, max_iter={max_iter_val}: Train & Test Loss\"\n",
        "            df_iter.plot(x=\"iteration\", y=[\"train_loss\",\"test_loss\"], marker='o', figsize=(8,5), title=title)\n",
        "            plt.grid(True, linestyle='--', alpha=0.7)\n",
        "            plt.show()\n",
        "\n",
        "        if \"avg_adaptive_lr\" in df_iter.columns:\n",
        "            title = f\"Linear LR={lr_val}, max_iter={max_iter_val}: Avg Adaptive LR vs. Iteration\"\n",
        "            df_iter.plot(x=\"iteration\", y=\"avg_adaptive_lr\", marker='x', figsize=(8,5), title=title)\n",
        "            plt.grid(True, linestyle='--', alpha=0.7)\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "############################################################################\n",
        "# 4) PANDAS + SEABORN PLOTS\n",
        "############################################################################\n",
        "\n",
        "logger.info(\"=== Pandas + Seaborn Plots ===\")\n",
        "\n",
        "# 4A) LINE PLOT: Accuracy vs. max_iter (Perceptrons Only)\n",
        "df_perc = df_results[df_results['model'].isin(['Clean PLA','Pocket PLA'])].copy()\n",
        "df_perc.sort_values(['model','max_iter'], inplace=True)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.lineplot(\n",
        "    data=df_perc,\n",
        "    x='max_iter', y='accuracy',\n",
        "    hue='model', marker='o'\n",
        ")\n",
        "plt.title(\"Perceptrons: Accuracy vs. max_iter (Pandas/Seaborn)\")\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# 4B) BAR CHART: Average Accuracy by Model\n",
        "df_mean = df_results.groupby('model', as_index=False)['accuracy'].mean()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(data=df_mean, x='model', y='accuracy')\n",
        "plt.title(\"Average Accuracy by Model (Pandas/Seaborn)\")\n",
        "plt.ylim(0.7, 1.0)\n",
        "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# 4C) SCATTER PLOT: Accuracy vs. Runtime, colored by model\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.scatterplot(\n",
        "    data=df_results,\n",
        "    x='runtime', y='accuracy',\n",
        "    hue='model', style='model',\n",
        "    s=100\n",
        ")\n",
        "plt.title(\"Accuracy vs. Runtime (All Models) (Pandas/Seaborn)\")\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "############################################################################\n",
        "# 5) CUSTOM SUMMARY PLOTS (AGGREGATED CURVES, ETC.)\n",
        "############################################################################\n",
        "\n",
        "logger.info(\"=== Custom Summaries (Aggregated Curves, etc.) ===\")\n",
        "\n",
        "# 5A) Aggregated Perceptron Curves\n",
        "plot_train_curves_three_models(\n",
        "    clean_train_curve=clean_train_curve,\n",
        "    pocket_train_curve=pocket_train_curve,\n",
        "    softmax_train_curve=None,  # no Softmax aggregator\n",
        "    title=\"Aggregated Perceptron Train Curves (Clean vs. Pocket)\",\n",
        "    max_iter=perceptron_max_iter_values[-1]\n",
        ")\n",
        "\n",
        "# 5B) Summaries for Perceptron\n",
        "plot_accuracy_vs_max_iter(\n",
        "    max_iter_values=perceptron_max_iter_values,\n",
        "    accuracies_clean=accuracies_clean,\n",
        "    accuracies_pocket=accuracies_pocket,\n",
        "    accuracies_softmax=None\n",
        ")\n",
        "\n",
        "plot_runtime_vs_max_iter(\n",
        "    max_iter_values=perceptron_max_iter_values,\n",
        "    runtimes_clean=runtimes_clean,\n",
        "    runtimes_pocket=runtimes_pocket,\n",
        "    runtimes_softmax=None\n",
        ")\n",
        "\n",
        "plot_accuracy_vs_runtime(\n",
        "    runtimes_clean=runtimes_clean,\n",
        "    accuracies_clean=accuracies_clean,\n",
        "    runtimes_pocket=runtimes_pocket,\n",
        "    accuracies_pocket=accuracies_pocket,\n",
        "    title=\"Perceptrons: Accuracy vs. Runtime\"\n",
        ")\n",
        "\n",
        "plot_performance_summary_extended_by_runtime(\n",
        "    runtimes_clean=runtimes_clean,\n",
        "    accuracies_clean=accuracies_clean,\n",
        "    sensitivities_clean=sensitivities_clean,\n",
        "    selectivities_clean=selectivities_clean,\n",
        "    runtimes_pocket=runtimes_pocket,\n",
        "    accuracies_pocket=accuracies_pocket,\n",
        "    sensitivities_pocket=sensitivities_pocket,\n",
        "    selectivities_pocket=selectivities_pocket,\n",
        "    title=\"Perceptrons: Performance vs. Runtime\"\n",
        ")\n",
        "\n",
        "# 5C) Summaries for Softmax & Linear\n",
        "plot_accuracy_vs_runtime(\n",
        "    runtimes_clean=runtimes_softmax,\n",
        "    accuracies_clean=accuracies_softmax,\n",
        "    title=\"Softmax: Accuracy vs. Runtime\"\n",
        ")\n",
        "plot_accuracy_vs_runtime(\n",
        "    runtimes_clean=runtimes_linear,\n",
        "    accuracies_clean=accuracies_linear,\n",
        "    title=\"Linear: Accuracy vs. Runtime\"\n",
        ")\n",
        "plot_accuracy_vs_runtime(\n",
        "    runtimes_clean=runtimes_softmax,\n",
        "    accuracies_clean=accuracies_softmax,\n",
        "    runtimes_pocket=runtimes_linear,\n",
        "    accuracies_pocket=accuracies_linear,\n",
        "    title=\"Softmax vs. Linear: Accuracy vs. Runtime\"\n",
        ")\n",
        "plot_performance_summary_extended_by_runtime(\n",
        "    runtimes_clean=runtimes_softmax,\n",
        "    accuracies_clean=accuracies_softmax,\n",
        "    sensitivities_clean=sensitivities_soft,\n",
        "    selectivities_clean=selectivities_soft,\n",
        "    runtimes_pocket=runtimes_linear,\n",
        "    accuracies_pocket=accuracies_linear,\n",
        "    sensitivities_pocket=sensitivities_lin,\n",
        "    selectivities_pocket=selectivities_lin,\n",
        "    title=\"Softmax vs. Linear: TPR/TNR vs. Runtime\"\n",
        ")\n",
        "\n",
        "# 5D) 4-Model Comparison\n",
        "plot_performance_summary_4models_by_runtime(\n",
        "    runtimes_clean, accuracies_clean, sensitivities_clean, selectivities_clean,\n",
        "    runtimes_pocket, accuracies_pocket, sensitivities_pocket, selectivities_pocket,\n",
        "    runtimes_softmax, accuracies_softmax, sensitivities_soft, selectivities_soft,\n",
        "    runtimes_linear, accuracies_linear, sensitivities_lin, selectivities_lin,\n",
        "    title=\"Performance vs. Runtime (4-Model Comparison)\"\n",
        ")\n",
        "\n",
        "plot_accuracy_vs_runtime_4models(\n",
        "    rt_clean=runtimes_clean,\n",
        "    acc_clean=accuracies_clean,\n",
        "    rt_pocket=runtimes_pocket,\n",
        "    acc_pocket=accuracies_pocket,\n",
        "    rt_softmax=runtimes_softmax,\n",
        "    acc_softmax=accuracies_softmax,\n",
        "    rt_linear=runtimes_linear,\n",
        "    acc_linear=accuracies_linear,\n",
        "    title=\"Accuracy vs. Runtime (4 Models)\"\n",
        ")\n",
        "\n",
        "logger.info(\"=== All Visualizations Complete ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mRmCfTiHxuc"
      },
      "source": [
        "# Final Results Summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSf_XT9J3Km1"
      },
      "source": [
        "\n",
        "**Observations:**\n",
        "- **Pocket PLA** consistently outperforms Clean PLA in both accuracy and sensitivity (TPR) across all tested iteration counts.\n",
        "- Increasing `max_iter` improves performance, though gains tend to plateau beyond roughly 50–100 iterations.\n",
        "- **Runtime** increases nearly linearly with `max_iter` for both methods, highlighting a clear trade-off between higher accuracy and computational cost.\n",
        "- Perfect linear separation is not achieved—even at higher iteration counts, neither method reaches 100% accuracy, indicating that the dataset is not strictly linearly separable.\n",
        "\n",
        "**Trade-off Analysis:**\n",
        "- **Low Iterations (max_iter = 10–30):**  \n",
        "  Fast training with modest accuracy and TPR, suitable for rapid prototyping or time-sensitive applications.\n",
        "- **Medium Iterations (max_iter = 50–100):**  \n",
        "  Balanced performance and runtime, capturing most achievable gains without excessive overhead.\n",
        "- **High Iterations (max_iter > 100):**  \n",
        "  Marginal performance improvements with significant runtime increase; diminishing returns for practical applications.\n",
        "\n",
        "**Recommendations for Future Work:**\n",
        "- Experiment with alternative update rules (e.g., adaptive learning rates) to accelerate convergence.\n",
        "- Compare against more sophisticated models (e.g., Logistic Regression, SVMs, neural networks) for broader insights.\n",
        "- Evaluate model robustness under noisy or adversarial conditions.\n",
        "\n",
        "This comprehensive analysis—including confusion matrices, error curves, and summary plots—provides detailed insights into the performance of the multi-class Perceptron on MNIST and informs the optimal balance between training efficiency and classification performance.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}