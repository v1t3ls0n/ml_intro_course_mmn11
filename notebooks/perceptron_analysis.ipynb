{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Classification Using Perceptron Learning Algorithm (PLA)\n",
    "\n",
    "**Objective**:  \n",
    "This notebook compares the performance of two variants of the Perceptron Learning Algorithm (PLA) on the MNIST digit classification task:\n",
    "- **Clean PLA**: Standard perceptron without enhancements.\n",
    "- **Pocket PLA**: Enhanced perceptron that stores the best-performing weights.\n",
    "\n",
    "**Dataset**:  \n",
    "- MNIST dataset (60,000 training samples and 10,000 test samples).\n",
    "- Images normalized to range [0, 1] and bias term added.\n",
    "\n",
    "**Evaluation Metrics**:  \n",
    "- Confusion matrices\n",
    "- Overall accuracy (ACC)\n",
    "- Sensitivity (True Positive Rate - TPR) for each digit class\n",
    "- Training and testing error curves for detailed iteration analysis\n",
    "\n",
    "**Goals**:  \n",
    "- Evaluate and compare model accuracy and robustness between Clean and Pocket PLA.\n",
    "- Visualize and analyze model performance in depth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture run_output\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Assuming 'notebooks/' is one folder below your project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from core.logger.config import logger\n",
    "from core.data.mnist_loader import load_mnist\n",
    "from core.data.data_preprocessing import preprocess_data\n",
    "from core.models.perceptron.multi_class_perceptron import MultiClassPerceptron\n",
    "from core.analysis.evaluation_functions import evaluate_model\n",
    "from core.analysis.plotting import plot_error_curves\n",
    "\n",
    "# Define different max_iter values for testing\n",
    "max_iter_values = [10, 20, 30, 50, 100]\n",
    "\n",
    "# Ensure results directories exist\n",
    "os.makedirs(\"results/perceptron_results/clean\", exist_ok=True)\n",
    "os.makedirs(\"results/perceptron_results/pocket\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess the MNIST Dataset\n",
    "\n",
    "We'll load the MNIST dataset using our custom loader (`mnist_loader`) and then apply preprocessing (`data_preprocessing`), which normalizes each image to [0,1] and adds a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data.mnist_loader import load_mnist\n",
    "from core.data.data_preprocessing import preprocess_data\n",
    "# Load raw MNIST data (X: images, y: labels)\n",
    "X_raw, y_raw = load_mnist()\n",
    "\n",
    "print(\"Raw MNIST data shapes:\")\n",
    "print(\"X_raw:\", X_raw.shape, \"y_raw:\", y_raw.shape)\n",
    "\n",
    "# Preprocess (normalize & add bias = True)\n",
    "X = preprocess_data(X_raw, add_bias=True, normalize=True)\n",
    "print(\"Preprocessed shape:\", X.shape)\n",
    "\n",
    "# Split into train/test manually or with 60k/10k as the task suggests\n",
    "X_train, y_train = X[:60000], y_raw[:60000]\n",
    "X_test,  y_test  = X[60000:], y_raw[60000:]\n",
    "print(\"Train set:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set: \", X_test.shape,  y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train, Evaluate, and Plot Training Results\n",
    "\n",
    "This section **trains, evaluates, and visualizes** the performance of **Clean PLA** and **Pocket PLA** across multiple values of `max_iter`.\n",
    "\n",
    "### **Training and Evaluation Steps:**\n",
    "1. **Train Models for Different Iterations (`max_iter`)**  \n",
    "   - Train **Clean PLA** (standard Perceptron) and **Pocket PLA** (best-weight tracking variant).  \n",
    "   - Store trained models for later analysis.  \n",
    "\n",
    "2. **Assess Model Performance:**  \n",
    "   - Compute **confusion matrices** to analyze per-class predictions.  \n",
    "   - Calculate **overall accuracy (ACC)** and **average sensitivity (TPR)** for each model.  \n",
    "   - Compare the effects of different `max_iter` values on classification results.  \n",
    "\n",
    "3. **Analyze Training Behavior:**  \n",
    "   - Plot **Accuracy vs. Max Iterations** to observe how training time affects accuracy.  \n",
    "   - Plot **Runtime vs. Max Iterations** to understand computational efficiency tradeoffs.  \n",
    "\n",
    "4. **Visualize Training Error Progression:**  \n",
    "   - Aggregate **training error curves** from all digit classifiers.  \n",
    "   - Compare how Clean vs. Pocket PLA models evolve across iterations.  \n",
    "   - Identify potential **overfitting or plateau effects** in training.  \n",
    "\n",
    "ðŸ“Œ **Goal:** Understand how iteration count (`max_iter`) impacts accuracy, runtime, and convergence speed while balancing training efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Train, Evaluate, and Plot Training Results\n",
    "import os\n",
    "import numpy as np\n",
    "from core.models.perceptron.multi_class_perceptron import MultiClassPerceptron\n",
    "from core.analysis.evaluation_functions import evaluate_model\n",
    "from core.analysis.plotting import (\n",
    "    plot_accuracy_vs_max_iter, \n",
    "    plot_runtime_vs_max_iter\n",
    ")\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "# Dictionaries to store trained models\n",
    "trained_models_clean = {}\n",
    "trained_models_pocket = {}\n",
    "\n",
    "# Lists to store accuracy, runtime, and sensitivity results\n",
    "accuracies_clean = []\n",
    "accuracies_pocket = []\n",
    "runtimes_clean = []\n",
    "runtimes_pocket = []\n",
    "sensitivities_clean = []\n",
    "sensitivities_pocket = []\n",
    "\n",
    "# Ensure results directory exists\n",
    "os.makedirs(\"results/perceptron_results\", exist_ok=True)\n",
    "\n",
    "# ========== Train Clean and Pocket PLA for different max_iter values ==========\n",
    "for max_iter in max_iter_values:\n",
    "    print(f\"=== Training PLA with max_iter={max_iter} ===\")\n",
    "\n",
    "    # Train Clean PLA\n",
    "    clean_perceptron = MultiClassPerceptron(num_classes=10, max_iter=max_iter, use_pocket=False)\n",
    "    clean_perceptron.fit(X_train, y_train)\n",
    "    trained_models_clean[max_iter] = clean_perceptron\n",
    "\n",
    "    # Train Pocket PLA\n",
    "    pocket_perceptron = MultiClassPerceptron(num_classes=10, max_iter=max_iter, use_pocket=True)\n",
    "    pocket_perceptron.fit(X_train, y_train)\n",
    "    trained_models_pocket[max_iter] = pocket_perceptron\n",
    "\n",
    "    print(f\"Training complete for max_iter={max_iter}\")\n",
    "\n",
    "# ========== Evaluate Models ==========\n",
    "for max_iter in max_iter_values:\n",
    "    print(f\"=== Evaluating PLA with max_iter={max_iter} ===\")\n",
    "\n",
    "    # Ensure directories exist\n",
    "    plot_dir_clean = f\"results/clean_{max_iter}\"\n",
    "    plot_dir_pocket = f\"results/pocket_{max_iter}\"\n",
    "    os.makedirs(plot_dir_clean, exist_ok=True)\n",
    "    os.makedirs(plot_dir_pocket, exist_ok=True)\n",
    "\n",
    "    # Retrieve trained models\n",
    "    clean_perceptron = trained_models_clean[max_iter]\n",
    "    pocket_perceptron = trained_models_pocket[max_iter]\n",
    "\n",
    "    # Evaluate Clean PLA\n",
    "    _, acc_clean, sens_clean, _ = evaluate_model(clean_perceptron, X_test, y_test, classes=list(range(10)), plot_dir=plot_dir_clean)\n",
    "    accuracies_clean.append(acc_clean)\n",
    "    sensitivities_clean.append(np.mean(sens_clean))  # Store mean sensitivity\n",
    "    runtimes_clean.append(clean_perceptron.training_runtime)\n",
    "\n",
    "    # Evaluate Pocket PLA\n",
    "    _, acc_pocket, sens_pocket, _ = evaluate_model(pocket_perceptron, X_test, y_test, classes=list(range(10)), plot_dir=plot_dir_pocket)\n",
    "    accuracies_pocket.append(acc_pocket)\n",
    "    sensitivities_pocket.append(np.mean(sens_pocket))  # Store mean sensitivity\n",
    "    runtimes_pocket.append(pocket_perceptron.training_runtime)\n",
    "\n",
    "    print(f\"Evaluation complete for max_iter={max_iter}\")\n",
    "\n",
    "# ========== Plot Accuracy and Runtime vs. Max Iterations ==========\n",
    "plot_accuracy_vs_max_iter(\n",
    "    max_iter_values,\n",
    "    accuracies_clean,\n",
    "    accuracies_pocket,\n",
    "    save_path=\"results/accuracy_vs_max_iter.png\"\n",
    ")\n",
    "\n",
    "plot_runtime_vs_max_iter(\n",
    "    max_iter_values,\n",
    "    runtimes_clean,\n",
    "    runtimes_pocket,\n",
    "    save_path=\"results/runtime_vs_max_iter.png\"\n",
    ")\n",
    "\n",
    "print(\"Plotted accuracy and runtime vs max_iter.\")\n",
    "\n",
    "# ================== Display Accuracy and Runtime Plots in Notebook ==================\n",
    "display(Image.open(\"results/accuracy_vs_max_iter.png\"))\n",
    "display(Image.open(\"results/runtime_vs_max_iter.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Training Error Curves\n",
    "\n",
    "Each digit-specific classifier within `MultiClassPerceptron` stores iteration-level training errors. We'll **aggregate** them across all digits to create an average training curve. This provides a high-level overview of how the algorithm's error evolves over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Visualize Training Error Curves\n",
    "\n",
    "import numpy as np\n",
    "from core.logger.config import logger\n",
    "from core.analysis.plotting import plot_error_curves\n",
    "from IPython.display import display\n",
    "\n",
    "# Function to aggregate loss curves across iterations\n",
    "def aggregate_iteration_losses(mcp_list):\n",
    "    \"\"\"\n",
    "    Aggregates iteration-level train/test losses across all digits\n",
    "    into an overall 'train_curve' by averaging across tested models.\n",
    "    \"\"\"\n",
    "    num_classes = mcp_list[0].num_classes  # Assume all models have the same num_classes\n",
    "\n",
    "    # Determine the maximum number of iterations across all models\n",
    "    max_len = max(max(len(mcp.loss_history[cls_idx][\"train\"]) for cls_idx in range(num_classes)) for mcp in mcp_list)\n",
    "\n",
    "    all_train_curves = []\n",
    "\n",
    "    for mcp in mcp_list:\n",
    "        all_train = []\n",
    "        for cls_idx in range(num_classes):\n",
    "            t_arr = mcp.loss_history[cls_idx][\"train\"][:]\n",
    "\n",
    "            # If classifier converged early, pad with last value\n",
    "            if len(t_arr) < max_len:\n",
    "                t_arr += [t_arr[-1]] * (max_len - len(t_arr))\n",
    "\n",
    "            all_train.append(t_arr)\n",
    "\n",
    "        # Convert to NumPy array and compute mean curve\n",
    "        all_train = np.array(all_train)\n",
    "        train_curve = np.mean(all_train, axis=0)\n",
    "\n",
    "        all_train_curves.append(train_curve)\n",
    "\n",
    "    # Convert all train curves into a uniform NumPy array\n",
    "    all_train_curves = np.array(all_train_curves)\n",
    "\n",
    "    return np.mean(all_train_curves, axis=0)  # Final averaged curve\n",
    "\n",
    "\n",
    "print(\"=== Plotting Average Training Curves for Clean vs Pocket PLA ===\")\n",
    "\n",
    "# Aggregate training curves across all `max_iter` runs\n",
    "clean_train_curve = aggregate_iteration_losses(list(trained_models_clean.values()))\n",
    "pocket_train_curve = aggregate_iteration_losses(list(trained_models_pocket.values()))\n",
    "\n",
    "plot_error_curves(\n",
    "    train_curve=clean_train_curve, \n",
    "    test_curve=pocket_train_curve,\n",
    "    title=\"Clean PLA vs. Pocket PLA (Avg. Train Error)\",\n",
    "    save_path=\"results/perceptron_results/train_curve_comparison.png\"\n",
    ")\n",
    "\n",
    "display(Image.open(\"results/perceptron_results/train_curve_comparison.png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary of Performance Across Iterations\n",
    "\n",
    "This section provides a comprehensive comparison of **PLA Clean** and **PLA Pocket** models across multiple iteration settings (`max_iter`). The table below summarizes the key performance metrics, including:\n",
    "\n",
    "- **Overall Accuracy (%)**: Measures the classification success rate.\n",
    "- **Sensitivity (TPR, %) **: Reflects the model's ability to correctly identify positive instances.\n",
    "- **Training Runtime (seconds)**: Evaluates computational efficiency.\n",
    "\n",
    "By analyzing these results, we can assess the tradeoff between **accuracy improvements** and **increased training time** as `max_iter` increases. The insights gained will guide optimal hyperparameter selection for real-world applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Dynamically generate table header\n",
    "table_header = \"| Max Iterations (`max_iter`) | \" + \" | \".join(map(str, max_iter_values)) + \" |\\n\"\n",
    "table_separator = \"|----------------------------|\" + \" | \".join([\"-\" * 7] * len(max_iter_values)) + \" |\\n\"\n",
    "\n",
    "# Accuracy row\n",
    "acc_clean_row = \"| **PLA Clean Accuracy (%)** | \" + \" | \".join([f\"{acc * 100:.2f}\" for acc in accuracies_clean]) + \" |\\n\"\n",
    "acc_pocket_row = \"| **PLA Pocket Accuracy (%)** | \" + \" | \".join([f\"{acc * 100:.2f}\" for acc in accuracies_pocket]) + \" |\\n\"\n",
    "\n",
    "# Sensitivity (TPR) row\n",
    "sens_clean_row = \"| **PLA Clean TPR (%)** | \" + \" | \".join([f\"{sens * 100:.2f}\" for sens in sensitivities_clean]) + \" |\\n\"\n",
    "sens_pocket_row = \"| **PLA Pocket TPR (%)** | \" + \" | \".join([f\"{sens * 100:.2f}\" for sens in sensitivities_pocket]) + \" |\\n\"\n",
    "\n",
    "# Runtime row\n",
    "runtime_clean_row = \"| **PLA Clean Runtime (s)** | \" + \" | \".join([f\"{runtime:.2f}\" for runtime in runtimes_clean]) + \" |\\n\"\n",
    "runtime_pocket_row = \"| **PLA Pocket Runtime (s)** | \" + \" | \".join([f\"{runtime:.2f}\" for runtime in runtimes_pocket]) + \" |\\n\"\n",
    "\n",
    "# Construct final summary table\n",
    "summary_table = f\"\"\"\n",
    "## Final Results Summary\n",
    "\n",
    "### Performance Comparison Across Iterations\n",
    "\n",
    "{table_header}{table_separator}{acc_clean_row}{acc_pocket_row}{sens_clean_row}{sens_pocket_row}{runtime_clean_row}{runtime_pocket_row}\n",
    "\n",
    "### **Observations:**\n",
    "- **Pocket PLA consistently outperforms Clean PLA** in accuracy and sensitivity.\n",
    "- **Increasing `max_iter` improves accuracy**, but with **diminishing returns beyond 50-100 iterations**.\n",
    "- **Runtime increases significantly** with more iterations, requiring a tradeoff between accuracy and efficiency.\n",
    "\n",
    "### **Tradeoff Analysis:**\n",
    "- **Low Iterations (`max_iter = 10-30`)**: **Fast training, moderate accuracy**, best for time-constrained tasks.\n",
    "- **Medium Iterations (`max_iter = 50-100`)**: **Balanced tradeoff**, preferred setting for stable performance.\n",
    "- **High Iterations (`max_iter > 100`)**: **Marginal accuracy gain**, but significant computational overhead.\n",
    "\n",
    "### **Recommendations for Future Work:**\n",
    "- **Test alternative update rules** to accelerate convergence.\n",
    "- **Compare PLA models with Logistic Regression or SVMs** for a broader perspective.\n",
    "- **Evaluate under noisy data or adversarial attacks** to assess robustness.\n",
    "\"\"\"\n",
    "\n",
    "# Display dynamically formatted markdown\n",
    "display(Markdown(summary_table))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
