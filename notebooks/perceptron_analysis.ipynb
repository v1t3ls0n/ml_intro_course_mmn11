{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Classification Using Perceptron Learning Algorithm (PLA)\n",
    "\n",
    "**Objective:**  \n",
    "This notebook compares the performance of two variants of the Perceptron Learning Algorithm (PLA) on the MNIST digit classification task:\n",
    "- **Clean PLA:** Standard perceptron without enhancements.\n",
    "- **Pocket PLA:** Enhanced perceptron that stores the best-performing weights during training (using the Pocket algorithm).\n",
    "\n",
    "**Dataset:**  \n",
    "- MNIST dataset consisting of 60,000 training samples and 10,000 test samples.\n",
    "- The images are normalized to the range [0, 1] and a bias term is added, resulting in input samples with 785 features.\n",
    "\n",
    "**Evaluation Metrics:**  \n",
    "- **Confusion Matrices:** Provides a detailed view of how well each digit is classified.\n",
    "- **Overall Accuracy (ACC):** Defined as \\(\\text{ACC} = \\frac{TP + TN}{TP + TN + FP + FN}\\).\n",
    "- **Sensitivity (True Positive Rate, TPR):** For each digit, calculated as \\(\\text{TPR} = \\frac{TP}{TP + FN}\\), showing the modelâ€™s ability to correctly identify the digit.\n",
    "- **Selectivity (Specificity, TNR):** For each digit, calculated as \\(\\text{TNR} = \\frac{TN}{TN + FP}\\), showing the modelâ€™s ability to correctly identify negatives.\n",
    "- **Training and Testing Error Curves:** Visualized as a function of iteration for detailed analysis of learning dynamics.\n",
    "- **Runtime:** The time taken to train the models.\n",
    "\n",
    "**Goals:**  \n",
    "- Evaluate and compare the model accuracy and robustness between Clean PLA and Pocket PLA.\n",
    "- Analyze and visualize the performance through confusion matrices, error curves, and summary plots (accuracy, sensitivity, selectivity, and runtime vs. the number of iterations).\n",
    "- Provide a comprehensive discussion on how training iterations affect the decision boundaries and the overall performance, particularly in the one-vs-all classification setup.\n",
    "\n",
    "This notebook integrates detailed quantitative evaluation with comprehensive visualizations to thoroughly analyze the multi-class Perceptron performance on the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture run_output\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# Assuming 'notebooks/' is one folder below your project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from core.data.mnist_loader import load_mnist\n",
    "from core.data.data_preprocessing import preprocess_data\n",
    "from core.models.perceptron.multi_class_perceptron import MultiClassPerceptron\n",
    "from core.analysis.evaluation_functions import evaluate_model\n",
    "from core.analysis.plotting import plot_error_curves\n",
    "\n",
    "# Define different max_iter values for testing\n",
    "# max_iter_values = [10, 20, 30, 50, 100, 500, 1000]\n",
    "max_iter_values = [i for i in range(1,21)]\n",
    "# max_iter_values = [10, 20]\n",
    "\n",
    "# # Ensure results directories exist\n",
    "# os.makedirs(\"results/perceptron_results/clean\", exist_ok=True)\n",
    "# os.makedirs(\"results/perceptron_results/pocket\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess the MNIST Dataset\n",
    "\n",
    "We'll load the MNIST dataset using our custom loader (`mnist_loader`) and then apply preprocessing (`data_preprocessing`). The preprocessing step normalizes each image to the range [0, 1] and adds a bias term, resulting in input samples with 785 features. This setup ensures that the training set contains 60,000 samples and the test set 10,000 samples, preparing the data for the subsequent classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data.mnist_loader import load_mnist\n",
    "from core.data.data_preprocessing import preprocess_data\n",
    "import logging\n",
    "\n",
    "# Load raw MNIST data (X: images, y: labels)\n",
    "X_raw, y_raw = load_mnist()\n",
    "\n",
    "logger = logging.getLogger(\"MyGlobalLogger\")\n",
    "\n",
    "logger.info(\"Raw MNIST data shapes: X_raw: %s, y_raw: %s\", X_raw.shape, y_raw.shape)\n",
    "\n",
    "# Preprocess (normalize & add bias = True)\n",
    "X = preprocess_data(X_raw, add_bias=True, normalize=True)\n",
    "logger.info(\"Preprocessed shape: %s\", X.shape)\n",
    "\n",
    "# Split into train/test manually or with 60k/10k as the task suggests\n",
    "X_train, y_train = X[:60000], y_raw[:60000]\n",
    "X_test,  y_test  = X[60000:], y_raw[60000:]\n",
    "\n",
    "logger.info(\"Train set: X_train: %s, y_train: %s\", X_train.shape, y_train.shape)\n",
    "logger.info(\"Test set: X_test: %s, y_test: %s\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train, Evaluate, and Visualize Training Results\n",
    "\n",
    "This section trains, evaluates, and visualizes the performance of **Clean PLA** and **Pocket PLA** across multiple values of `max_iter`.\n",
    "\n",
    "### **Training and Evaluation Steps:**\n",
    "\n",
    "1. **Train Models for Different Iterations (`max_iter`):**  \n",
    "   - Train **Clean PLA** (standard Perceptron) and **Pocket PLA** (best-weight tracking variant) in a one-vs-all setup.\n",
    "   - Each digit is handled by a separate binary classifier with batch updates on all misclassified samples.\n",
    "   - Trained models are stored for later analysis.\n",
    "\n",
    "2. **Assess Model Performance:**  \n",
    "   - **Confusion Matrices:** Generate annotated confusion matrices to inspect per-class predictions.\n",
    "   - **Overall Accuracy (ACC):** Calculate accuracy as \\(\\text{ACC} = \\frac{TP + TN}{TP + TN + FP + FN}\\).\n",
    "   - **Sensitivity (TPR):** For each digit, compute \\(\\text{TPR} = \\frac{TP}{TP + FN}\\).\n",
    "   - **Selectivity (TNR):** For each digit, compute \\(\\text{TNR} = \\frac{TN}{TN + FP}\\).\n",
    "   - **Runtime:** Record the training time.\n",
    "\n",
    "3. **Performance Visualization:**  \n",
    "   - Plot **Accuracy vs. Max Iterations** to observe the effect of iteration count on accuracy.\n",
    "   - Plot **Runtime vs. Max Iterations** to examine computational efficiency.\n",
    "   - Generate an extended summary plot combining Accuracy, Sensitivity (TPR), Selectivity (TNR), and Runtime vs. `max_iter`.\n",
    "\n",
    "ðŸ“Œ **Goal:**  \n",
    "Understand how the iteration count (`max_iter`) impacts accuracy, sensitivity, selectivity, and runtime. This analysis helps in selecting an optimal trade-off between training efficiency and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Train, Evaluate, and Visualize Training Results\n",
    "import os\n",
    "import numpy as np\n",
    "from core.models.perceptron.multi_class_perceptron import MultiClassPerceptron\n",
    "from core.analysis.evaluation_functions import evaluate_model\n",
    "from core.analysis.plotting import (\n",
    "    plot_accuracy_vs_max_iter, \n",
    "    plot_runtime_vs_max_iter,\n",
    "    plot_performance_summary_extended\n",
    ")\n",
    "from core.logger.config import logger\n",
    "\n",
    "# Dictionaries to store trained models\n",
    "trained_models_clean = {}\n",
    "trained_models_pocket = {}\n",
    "\n",
    "# Lists to store accuracy, runtime, sensitivity, and selectivity results\n",
    "accuracies_clean = []\n",
    "accuracies_pocket = []\n",
    "runtimes_clean = []\n",
    "runtimes_pocket = []\n",
    "sensitivities_clean = []\n",
    "sensitivities_pocket = []\n",
    "selectivities_clean = []\n",
    "selectivities_pocket = []\n",
    "\n",
    "# ========== Train Clean and Pocket PLA for different max_iter values ==========\n",
    "for max_iter in max_iter_values:\n",
    "    logger.info(f\"=== Training PLA with max_iter={max_iter} ===\")\n",
    "\n",
    "    # Train Clean PLA\n",
    "    clean_perceptron = MultiClassPerceptron(num_classes=10, max_iter=max_iter, use_pocket=False)\n",
    "    clean_perceptron.fit(X_train, y_train)\n",
    "    trained_models_clean[max_iter] = clean_perceptron\n",
    "\n",
    "    # Train Pocket PLA\n",
    "    pocket_perceptron = MultiClassPerceptron(num_classes=10, max_iter=max_iter, use_pocket=True)\n",
    "    pocket_perceptron.fit(X_train, y_train)\n",
    "    trained_models_pocket[max_iter] = pocket_perceptron\n",
    "\n",
    "    logger.info(f\"Training complete for max_iter={max_iter}\")\n",
    "\n",
    "# ========== Evaluate Models ==========\n",
    "for max_iter in max_iter_values:\n",
    "    logger.info(f\"=== Evaluating PLA with max_iter={max_iter} ===\")\n",
    "\n",
    "    # Retrieve trained models\n",
    "    clean_perceptron = trained_models_clean[max_iter]\n",
    "    pocket_perceptron = trained_models_pocket[max_iter]\n",
    "\n",
    "    # Evaluate Clean PLA\n",
    "    cm_clean, acc_clean, sens_clean, spec_clean, runtime_clean = evaluate_model(\n",
    "        clean_perceptron, X_test, y_test, classes=list(range(10)), plot_dir=plot_dir_clean\n",
    "    )\n",
    "    accuracies_clean.append(acc_clean)\n",
    "    sensitivities_clean.append(np.mean(sens_clean))   # Mean sensitivity for reporting\n",
    "    selectivities_clean.append(np.mean(spec_clean))     # Mean selectivity for reporting\n",
    "    runtimes_clean.append(runtime_clean)\n",
    "\n",
    "    # Evaluate Pocket PLA\n",
    "    cm_pocket, acc_pocket, sens_pocket, spec_pocket, runtime_pocket = evaluate_model(\n",
    "        pocket_perceptron, X_test, y_test, classes=list(range(10)), plot_dir=plot_dir_pocket\n",
    "    )\n",
    "    accuracies_pocket.append(acc_pocket)\n",
    "    sensitivities_pocket.append(np.mean(sens_pocket))   # Mean sensitivity for reporting\n",
    "    selectivities_pocket.append(np.mean(spec_pocket))     # Mean selectivity for reporting\n",
    "    runtimes_pocket.append(runtime_pocket)\n",
    "\n",
    "    logger.info(f\"Evaluation complete for max_iter={max_iter}\")\n",
    "\n",
    "# ========== Summary Plots ==========\n",
    "# Plot accuracy vs. max_iter\n",
    "plot_accuracy_vs_max_iter(\n",
    "    max_iter_values,\n",
    "    accuracies_clean,\n",
    "    accuracies_pocket,\n",
    ")\n",
    "\n",
    "# Plot runtime vs. max_iter\n",
    "plot_runtime_vs_max_iter(\n",
    "    max_iter_values,\n",
    "    runtimes_clean,\n",
    "    runtimes_pocket,\n",
    ")\n",
    "\n",
    "# Plot comprehensive summary: Accuracy, Sensitivity (TPR), Selectivity (TNR), and Runtime vs. max_iter\n",
    "plot_performance_summary_extended(\n",
    "    max_iter_values,\n",
    "    accuracies_clean, accuracies_pocket,\n",
    "    sensitivities_clean, sensitivities_pocket,\n",
    "    selectivities_clean, selectivities_pocket,\n",
    "    runtimes_clean, runtimes_pocket,\n",
    ")\n",
    "\n",
    "logger.info(\"Plotted accuracy, sensitivity, selectivity, and runtime vs. max_iter.\")\n",
    "\n",
    "# Optionally, print out a summary of the metrics:\n",
    "print(\"Mean Sensitivity (TPR) for Clean PLA:\", sensitivities_clean)\n",
    "print(\"Mean Sensitivity (TPR) for Pocket PLA:\", sensitivities_pocket)\n",
    "print(\"Mean Selectivity (TNR) for Clean PLA:\", selectivities_clean)\n",
    "print(\"Mean Selectivity (TNR) for Pocket PLA:\", selectivities_pocket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Training Error Curves\n",
    "\n",
    "Each digit-specific classifier within the `MultiClassPerceptron` stores iteration-level training errors. We aggregate these losses across all digit classifiers to create an average training error curve. This provides a high-level overview of how the algorithm's error evolves over time for both Clean and Pocket PLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Visualize Training Error Curves\n",
    "\n",
    "import numpy as np\n",
    "from core.logger.config import logger\n",
    "from core.analysis.plotting import plot_error_curves\n",
    "\n",
    "# Function to aggregate loss curves across iterations\n",
    "def aggregate_iteration_losses(mcp_list):\n",
    "    \"\"\"\n",
    "    Aggregates iteration-level train/test losses across all digits\n",
    "    into an overall 'train_curve' by averaging across tested models.\n",
    "    \"\"\"\n",
    "    num_classes = mcp_list[0].num_classes  # Assume all models have the same num_classes\n",
    "\n",
    "    # Determine the maximum number of iterations across all models\n",
    "    max_len = max(max(len(mcp.loss_history[cls_idx][\"train\"]) for cls_idx in range(num_classes)) for mcp in mcp_list)\n",
    "\n",
    "    all_train_curves = []\n",
    "\n",
    "    for mcp in mcp_list:\n",
    "        all_train = []\n",
    "        for cls_idx in range(num_classes):\n",
    "            t_arr = mcp.loss_history[cls_idx][\"train\"][:]\n",
    "\n",
    "            # If classifier converged early, pad with last value\n",
    "            if len(t_arr) < max_len:\n",
    "                t_arr += [t_arr[-1]] * (max_len - len(t_arr))\n",
    "\n",
    "            all_train.append(t_arr)\n",
    "\n",
    "        # Convert to NumPy array and compute mean curve\n",
    "        all_train = np.array(all_train)\n",
    "        train_curve = np.mean(all_train, axis=0)\n",
    "\n",
    "        all_train_curves.append(train_curve)\n",
    "\n",
    "    # Convert all train curves into a uniform NumPy array\n",
    "    all_train_curves = np.array(all_train_curves)\n",
    "\n",
    "    return np.mean(all_train_curves, axis=0)  # Final averaged curve\n",
    "\n",
    "\n",
    "logger.info(\"=== Plotting Average Training Curves for Clean vs Pocket PLA ===\")\n",
    "\n",
    "# Aggregate training curves across all `max_iter` runs\n",
    "clean_train_curve = aggregate_iteration_losses(list(trained_models_clean.values()))\n",
    "pocket_train_curve = aggregate_iteration_losses(list(trained_models_pocket.values()))\n",
    "\n",
    "plot_error_curves(\n",
    "    train_curve=clean_train_curve, \n",
    "    test_curve=pocket_train_curve,\n",
    "    title=\"Clean PLA vs. Pocket PLA (Avg. Train Error)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary of Performance Across Iterations\n",
    "\n",
    "This section provides a comprehensive comparison of Clean PLA and Pocket PLA across multiple iteration settings (`max_iter`). The performance summary includes:\n",
    "- **Overall Accuracy (%):** Measures the classification success rate.\n",
    "- **Sensitivity (TPR, %):** Reflects the model's ability to correctly identify positive instances.\n",
    "- **Selectivity (TNR, %):** Assesses how well the model identifies negatives.\n",
    "- **Training Runtime (seconds):** Evaluates computational efficiency.\n",
    "\n",
    "By analyzing these results, we can assess the trade-off between accuracy improvements and increased training time as `max_iter` increases, guiding optimal hyperparameter selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.analysis.plotting import plot_performance_summary\n",
    "# Generate performance plots\n",
    "plot_performance_summary(max_iter_values, accuracies_clean, accuracies_pocket,\n",
    "                         sensitivities_clean, sensitivities_pocket,\n",
    "                         runtimes_clean, runtimes_pocket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Results Summary\n",
    "\n",
    "**Observations:**\n",
    "- **Pocket PLA** consistently outperforms Clean PLA in both accuracy and sensitivity (TPR) across all tested iteration counts.\n",
    "- Increasing `max_iter` improves performance, though gains tend to plateau beyond roughly 50â€“100 iterations.\n",
    "- **Runtime** increases nearly linearly with `max_iter` for both methods, highlighting a clear trade-off between higher accuracy and computational cost.\n",
    "- Perfect linear separation is not achievedâ€”even at higher iteration counts, neither method reaches 100% accuracy, indicating that the dataset is not strictly linearly separable.\n",
    "\n",
    "**Trade-off Analysis:**\n",
    "- **Low Iterations (max_iter = 10â€“30):**  \n",
    "  Fast training with modest accuracy and TPR, suitable for rapid prototyping or time-sensitive applications.\n",
    "- **Medium Iterations (max_iter = 50â€“100):**  \n",
    "  Balanced performance and runtime, capturing most achievable gains without excessive overhead.\n",
    "- **High Iterations (max_iter > 100):**  \n",
    "  Marginal performance improvements with significant runtime increase; diminishing returns for practical applications.\n",
    "\n",
    "**Recommendations for Future Work:**\n",
    "- Experiment with alternative update rules (e.g., adaptive learning rates) to accelerate convergence.\n",
    "- Compare against more sophisticated models (e.g., Logistic Regression, SVMs, neural networks) for broader insights.\n",
    "- Evaluate model robustness under noisy or adversarial conditions.\n",
    "\n",
    "This comprehensive analysisâ€”including confusion matrices, error curves, and summary plotsâ€”provides detailed insights into the performance of the multi-class Perceptron on MNIST and informs the optimal balance between training efficiency and classification performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
