{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Classification Using Perceptron Learning Algorithm (PLA)\n",
    "\n",
    "**Objective**:  \n",
    "This notebook compares the performance of two variants of the Perceptron Learning Algorithm (PLA) on the MNIST digit classification task:\n",
    "- **Clean PLA**: Standard perceptron without enhancements.\n",
    "- **Pocket PLA**: Enhanced perceptron that stores the best-performing weights.\n",
    "\n",
    "**Dataset**:  \n",
    "- MNIST dataset (60,000 training samples and 10,000 test samples).\n",
    "- Images normalized to range [0, 1] and bias term added.\n",
    "\n",
    "**Evaluation Metrics**:  \n",
    "- Confusion matrices\n",
    "- Overall accuracy (ACC)\n",
    "- Sensitivity (True Positive Rate - TPR) for each digit class\n",
    "- Training and testing error curves for detailed iteration analysis\n",
    "\n",
    "**Goals**:  \n",
    "- Evaluate and compare model accuracy and robustness between Clean and Pocket PLA.\n",
    "- Visualize and analyze model performance in depth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture run_output\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# Assuming 'notebooks/' is one folder below your project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from core.data.mnist_loader import load_mnist\n",
    "from core.data.data_preprocessing import preprocess_data\n",
    "from core.models.perceptron.multi_class_perceptron import MultiClassPerceptron\n",
    "from core.analysis.evaluation_functions import evaluate_model\n",
    "from core.analysis.plotting import plot_error_curves\n",
    "\n",
    "# Define different max_iter values for testing\n",
    "# max_iter_values = [10, 20, 30, 50, 100, 500, 1000]\n",
    "max_iter_values = [i for i in range(1,21)]\n",
    "# max_iter_values = [10, 20]\n",
    "\n",
    "# Ensure results directories exist\n",
    "os.makedirs(\"results/perceptron_results/clean\", exist_ok=True)\n",
    "os.makedirs(\"results/perceptron_results/pocket\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess the MNIST Dataset\n",
    "\n",
    "We'll load the MNIST dataset using our custom loader (`mnist_loader`) and then apply preprocessing (`data_preprocessing`), which normalizes each image to [0,1] and adds a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data.mnist_loader import load_mnist\n",
    "from core.data.data_preprocessing import preprocess_data\n",
    "import logging\n",
    "\n",
    "# Load raw MNIST data (X: images, y: labels)\n",
    "X_raw, y_raw = load_mnist()\n",
    "\n",
    "logger = logging.getLogger(\"MyGlobalLogger\")\n",
    "\n",
    "logger.info(\"Raw MNIST data shapes: X_raw: %s, y_raw: %s\", X_raw.shape, y_raw.shape)\n",
    "\n",
    "# Preprocess (normalize & add bias = True)\n",
    "X = preprocess_data(X_raw, add_bias=True, normalize=True)\n",
    "logger.info(\"Preprocessed shape: %s\", X.shape)\n",
    "\n",
    "# Split into train/test manually or with 60k/10k as the task suggests\n",
    "X_train, y_train = X[:60000], y_raw[:60000]\n",
    "X_test,  y_test  = X[60000:], y_raw[60000:]\n",
    "\n",
    "logger.info(\"Train set: X_train: %s, y_train: %s\", X_train.shape, y_train.shape)\n",
    "logger.info(\"Test set: X_test: %s, y_test: %s\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train, Evaluate, and Plot Training Results\n",
    "\n",
    "This section **trains, evaluates, and visualizes** the performance of **Clean PLA** and **Pocket PLA** across multiple values of `max_iter`.\n",
    "\n",
    "### **Training and Evaluation Steps:**\n",
    "1. **Train Models for Different Iterations (`max_iter`)**  \n",
    "   - Train **Clean PLA** (standard Perceptron) and **Pocket PLA** (best-weight tracking variant).  \n",
    "   - Store trained models for later analysis.  \n",
    "\n",
    "2. **Assess Model Performance:**  \n",
    "   - Compute **confusion matrices** to analyze per-class predictions.  \n",
    "   - Calculate **overall accuracy (ACC)** and **average sensitivity (TPR)** for each model.  \n",
    "   - Compare the effects of different `max_iter` values on classification results.  \n",
    "\n",
    "3. **Analyze Training Behavior:**  \n",
    "   - Plot **Accuracy vs. Max Iterations** to observe how training time affects accuracy.  \n",
    "   - Plot **Runtime vs. Max Iterations** to understand computational efficiency tradeoffs.  \n",
    "\n",
    "4. **Visualize Training Error Progression:**  \n",
    "   - Aggregate **training error curves** from all digit classifiers.  \n",
    "   - Compare how Clean vs. Pocket PLA models evolve across iterations.  \n",
    "   - Identify potential **overfitting or plateau effects** in training.  \n",
    "\n",
    "ðŸ“Œ **Goal:** Understand how iteration count (`max_iter`) impacts accuracy, runtime, and convergence speed while balancing training efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Train, Evaluate, and Visualize Training Results\n",
    "import os\n",
    "import numpy as np\n",
    "from core.models.perceptron.multi_class_perceptron import MultiClassPerceptron\n",
    "from core.analysis.evaluation_functions import evaluate_model\n",
    "from core.analysis.plotting import (\n",
    "    plot_accuracy_vs_max_iter, \n",
    "    plot_runtime_vs_max_iter,\n",
    "    plot_performance_summary_extended\n",
    ")\n",
    "from core.logger.config import logger\n",
    "\n",
    "# Dictionaries to store trained models\n",
    "trained_models_clean = {}\n",
    "trained_models_pocket = {}\n",
    "\n",
    "# Lists to store accuracy, runtime, sensitivity, and selectivity results\n",
    "accuracies_clean = []\n",
    "accuracies_pocket = []\n",
    "runtimes_clean = []\n",
    "runtimes_pocket = []\n",
    "sensitivities_clean = []\n",
    "sensitivities_pocket = []\n",
    "selectivities_clean = []\n",
    "selectivities_pocket = []\n",
    "\n",
    "# Ensure results directory exists\n",
    "results_dir = \"results/perceptron_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# ========== Train Clean and Pocket PLA for different max_iter values ==========\n",
    "for max_iter in max_iter_values:\n",
    "    logger.info(f\"=== Training PLA with max_iter={max_iter} ===\")\n",
    "\n",
    "    # Train Clean PLA\n",
    "    clean_perceptron = MultiClassPerceptron(num_classes=10, max_iter=max_iter, use_pocket=False)\n",
    "    clean_perceptron.fit(X_train, y_train)\n",
    "    trained_models_clean[max_iter] = clean_perceptron\n",
    "\n",
    "    # Train Pocket PLA\n",
    "    pocket_perceptron = MultiClassPerceptron(num_classes=10, max_iter=max_iter, use_pocket=True)\n",
    "    pocket_perceptron.fit(X_train, y_train)\n",
    "    trained_models_pocket[max_iter] = pocket_perceptron\n",
    "\n",
    "    logger.info(f\"Training complete for max_iter={max_iter}\")\n",
    "\n",
    "# ========== Evaluate Models ==========\n",
    "for max_iter in max_iter_values:\n",
    "    logger.info(f\"=== Evaluating PLA with max_iter={max_iter} ===\")\n",
    "\n",
    "    # Define plot directories for each model version\n",
    "    plot_dir_clean = f\"{results_dir}/clean_{max_iter}\"\n",
    "    plot_dir_pocket = f\"{results_dir}/pocket_{max_iter}\"\n",
    "    os.makedirs(plot_dir_clean, exist_ok=True)\n",
    "    os.makedirs(plot_dir_pocket, exist_ok=True)\n",
    "\n",
    "    # Retrieve trained models\n",
    "    clean_perceptron = trained_models_clean[max_iter]\n",
    "    pocket_perceptron = trained_models_pocket[max_iter]\n",
    "\n",
    "    # Evaluate Clean PLA\n",
    "    cm_clean, acc_clean, sens_clean, spec_clean, runtime_clean = evaluate_model(\n",
    "        clean_perceptron, X_test, y_test, classes=list(range(10)), plot_dir=plot_dir_clean\n",
    "    )\n",
    "    accuracies_clean.append(acc_clean)\n",
    "    sensitivities_clean.append(np.mean(sens_clean))   # Mean sensitivity for reporting\n",
    "    selectivities_clean.append(np.mean(spec_clean))     # Mean selectivity for reporting\n",
    "    runtimes_clean.append(runtime_clean)\n",
    "\n",
    "    # Evaluate Pocket PLA\n",
    "    cm_pocket, acc_pocket, sens_pocket, spec_pocket, runtime_pocket = evaluate_model(\n",
    "        pocket_perceptron, X_test, y_test, classes=list(range(10)), plot_dir=plot_dir_pocket\n",
    "    )\n",
    "    accuracies_pocket.append(acc_pocket)\n",
    "    sensitivities_pocket.append(np.mean(sens_pocket))   # Mean sensitivity for reporting\n",
    "    selectivities_pocket.append(np.mean(spec_pocket))     # Mean selectivity for reporting\n",
    "    runtimes_pocket.append(runtime_pocket)\n",
    "\n",
    "    logger.info(f\"Evaluation complete for max_iter={max_iter}\")\n",
    "\n",
    "# ========== Summary Plots ==========\n",
    "# Plot accuracy vs. max_iter\n",
    "plot_accuracy_vs_max_iter(\n",
    "    max_iter_values,\n",
    "    accuracies_clean,\n",
    "    accuracies_pocket,\n",
    ")\n",
    "\n",
    "# Plot runtime vs. max_iter\n",
    "plot_runtime_vs_max_iter(\n",
    "    max_iter_values,\n",
    "    runtimes_clean,\n",
    "    runtimes_pocket,\n",
    ")\n",
    "\n",
    "# Plot comprehensive summary: Accuracy, Sensitivity (TPR), Selectivity (TNR), and Runtime vs. max_iter\n",
    "plot_performance_summary_extended(\n",
    "    max_iter_values,\n",
    "    accuracies_clean, accuracies_pocket,\n",
    "    sensitivities_clean, sensitivities_pocket,\n",
    "    selectivities_clean, selectivities_pocket,\n",
    "    runtimes_clean, runtimes_pocket,\n",
    ")\n",
    "\n",
    "logger.info(\"Plotted accuracy, sensitivity, selectivity, and runtime vs. max_iter.\")\n",
    "\n",
    "# Optionally, print out a summary of the metrics:\n",
    "print(\"Mean Sensitivity (TPR) for Clean PLA:\", sensitivities_clean)\n",
    "print(\"Mean Sensitivity (TPR) for Pocket PLA:\", sensitivities_pocket)\n",
    "print(\"Mean Selectivity (TNR) for Clean PLA:\", selectivities_clean)\n",
    "print(\"Mean Selectivity (TNR) for Pocket PLA:\", selectivities_pocket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Training Error Curves\n",
    "\n",
    "Each digit-specific classifier within `MultiClassPerceptron` stores iteration-level training errors. We'll **aggregate** them across all digits to create an average training curve. This provides a high-level overview of how the algorithm's error evolves over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Visualize Training Error Curves\n",
    "\n",
    "import numpy as np\n",
    "from core.logger.config import logger\n",
    "from core.analysis.plotting import plot_error_curves\n",
    "\n",
    "# Function to aggregate loss curves across iterations\n",
    "def aggregate_iteration_losses(mcp_list):\n",
    "    \"\"\"\n",
    "    Aggregates iteration-level train/test losses across all digits\n",
    "    into an overall 'train_curve' by averaging across tested models.\n",
    "    \"\"\"\n",
    "    num_classes = mcp_list[0].num_classes  # Assume all models have the same num_classes\n",
    "\n",
    "    # Determine the maximum number of iterations across all models\n",
    "    max_len = max(max(len(mcp.loss_history[cls_idx][\"train\"]) for cls_idx in range(num_classes)) for mcp in mcp_list)\n",
    "\n",
    "    all_train_curves = []\n",
    "\n",
    "    for mcp in mcp_list:\n",
    "        all_train = []\n",
    "        for cls_idx in range(num_classes):\n",
    "            t_arr = mcp.loss_history[cls_idx][\"train\"][:]\n",
    "\n",
    "            # If classifier converged early, pad with last value\n",
    "            if len(t_arr) < max_len:\n",
    "                t_arr += [t_arr[-1]] * (max_len - len(t_arr))\n",
    "\n",
    "            all_train.append(t_arr)\n",
    "\n",
    "        # Convert to NumPy array and compute mean curve\n",
    "        all_train = np.array(all_train)\n",
    "        train_curve = np.mean(all_train, axis=0)\n",
    "\n",
    "        all_train_curves.append(train_curve)\n",
    "\n",
    "    # Convert all train curves into a uniform NumPy array\n",
    "    all_train_curves = np.array(all_train_curves)\n",
    "\n",
    "    return np.mean(all_train_curves, axis=0)  # Final averaged curve\n",
    "\n",
    "\n",
    "logger.info(\"=== Plotting Average Training Curves for Clean vs Pocket PLA ===\")\n",
    "\n",
    "# Aggregate training curves across all `max_iter` runs\n",
    "clean_train_curve = aggregate_iteration_losses(list(trained_models_clean.values()))\n",
    "pocket_train_curve = aggregate_iteration_losses(list(trained_models_pocket.values()))\n",
    "\n",
    "plot_error_curves(\n",
    "    train_curve=clean_train_curve, \n",
    "    test_curve=pocket_train_curve,\n",
    "    title=\"Clean PLA vs. Pocket PLA (Avg. Train Error)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary of Performance Across Iterations\n",
    "\n",
    "This section provides a comprehensive comparison of **PLA Clean** and **PLA Pocket** models across multiple iteration settings (`max_iter`). The table below summarizes the key performance metrics, including:\n",
    "\n",
    "- **Overall Accuracy (%)**: Measures the classification success rate.\n",
    "- **Sensitivity (TPR, %) **: Reflects the model's ability to correctly identify positive instances.\n",
    "- **Training Runtime (seconds)**: Evaluates computational efficiency.\n",
    "\n",
    "By analyzing these results, we can assess the tradeoff between **accuracy improvements** and **increased training time** as `max_iter` increases. The insights gained will guide optimal hyperparameter selection for real-world applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.analysis.plotting import plot_performance_summary\n",
    "# Generate performance plots\n",
    "plot_performance_summary(max_iter_values, accuracies_clean, accuracies_pocket,\n",
    "                         sensitivities_clean, sensitivities_pocket,\n",
    "                         runtimes_clean, runtimes_pocket)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Results Summary\n",
    "\n",
    "### **Observations:**\n",
    "- **Pocket PLA consistently outperforms Clean PLA** in both accuracy and sensitivity (TPR) across all tested iteration counts.\n",
    "- **Increasing `max_iter` steadily improves performance**, but **the gains taper off** once you move beyond roughly **50â€“100 iterations**.\n",
    "- **Runtime grows nearly linearly** with `max_iter` for both algorithms, creating a clear **tradeoff** between higher accuracy/sensitivity and computational cost.\n",
    "- **Perfect linear separation is not achieved**, as even at higher iteration counts neither method reaches **100%** accuracy, indicating that the dataset is not strictly linearly separable.\n",
    "\n",
    "### **Tradeoff Analysis:**\n",
    "- **Low Iterations (`max_iter = 10â€“30`)**  \n",
    "  - **Very fast training** with minimal computational overhead.  \n",
    "  - **Accuracy and TPR are modest**, making this range best for rapid prototyping or extremely time-sensitive tasks.\n",
    "- **Medium Iterations (`max_iter = 50â€“100`)**  \n",
    "  - **Balanced tradeoff** between accuracy and runtime.  \n",
    "  - **Performance stabilizes** here, showing most of the achievable gain without excessive overhead.\n",
    "- **High Iterations (`max_iter > 100`)**  \n",
    "  - **Marginal performance improvements** beyond this point.  \n",
    "  - **Significant increase in runtime**, offering diminishing returns for most practical applications.\n",
    "\n",
    "### **Recommendations for Future Work:**\n",
    "- **Experiment with alternative update rules** (e.g., adaptive learning rates) to accelerate convergence or improve final performance.\n",
    "- **Compare against more sophisticated models** such as Logistic Regression, SVMs, or neural networks for a broader perspective on linear vs. nonlinear decision boundaries.\n",
    "- **Evaluate under noisy or adversarial conditions** to assess model robustness and generalizability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
